% !TeX root = RJwrapper.tex
\title{Gaussian Mixture Models in R}
\author{by Bastien Chassagnol, Antoine Bichat, Cheïma Boudjeniba, Pierre-Henri Wuillemin, Mickaël Guedj, Gregory Nuel, and Etienne Becht}

\maketitle

\abstract{%
Gaussian mixture models (GMMs) are widely used for modelling stochastic problems. Indeed, a wide diversity of packages have been developed in R. However, no recent review describing the main features offered by these packages and comparing their performances has been performed. In this article, we first introduce GMMs and the EM algorithm used to retrieve the parameters of the model and analyse the main features implemented among seven of the most widely used R packages. We then empirically compare their statistical and computational performances in relation with the choice of the initialisation algorithm and the complexity of the mixture. We demonstrate that the best estimation with well-separated components or with a small number of components with distinguishable modes is obtained with REBMIX initialisation, implemented in the \CRANpkg{rebmix} package, while the best estimation with highly overlapping components is obtained with \emph{k}-means or random initialisation. Importantly, we show that implementation details in the EM algorithm yield differences in the parameters' estimation. Especially, packages \CRANpkg{mixtools} (Young et al. 2020) and \CRANpkg{Rmixmod} (Langrognet et al. 2021) estimate the parameters of the mixture with smaller bias, while the RMSE and variability of the estimates is smaller with packages \CRANpkg{bgmm} (Ewa Szczurek 2021) , \CRANpkg{EMCluster} (W.-C. Chen and Maitra 2022) , \CRANpkg{GMKMcharlie} (Liu 2021), \CRANpkg{flexmix} (Gruen and Leisch 2022) and \CRANpkg{mclust} (Fraley, Raftery, and Scrucca 2022). The comparison of these packages provides R users with useful recommendations for improving the computational and statistical performance of their clustering and for identifying common deficiencies. Additionally, we propose several improvements in the development of a future, unified mixture model package.
}

\hypertarget{introduction-to-mixture-modelling}{%
\section{Introduction to Mixture modelling}\label{introduction-to-mixture-modelling}}

Formally, let's consider a pair of random variables \((X,S)\) with \(S \in \{1, \ldots, k\}\) a discrete variable and designing
the component identity of each observation. When observed, \(S\) is
generally denoted as the labels of the individual observations. \(k\) is
the number of mixture \emph{components}. Then, the density distribution of
\(X\) is given in Equation \eqref{eq:1}:

\begin{equation}
\begin{split}
f_\theta(X) &= \sum_S f_\theta (X, S) \\
&= \sum_{j=1}^k p_j f_{\zeta j}(X), \quad X\in\mathbb{R}
\end{split}
\label{eq:1}
\end{equation}

where \(\theta = (p, \zeta) = (p_1, \ldots, p_k, \zeta_1, \ldots, \zeta_k)\)
denotes the parameters of the model: \(p_j\) is the proportion of
component \(j\) and \(\zeta_j\) represents the parameters of the density
distribution followed by component \(j\). In addition, since \(S\) is a categorical variable parametrized by \(p\), the prior weights must enforce the unit simplex constraint (Equation \eqref{eq:2}):

\begin{equation}
\begin{cases}
p_j \ge 0 \quad \forall j \in \{1, \ldots, k \}\\
\sum_{j=1}^k p_j =1
\end{cases}
\label{eq:2}
\end{equation}

In terms of applications, mixture models can be used to achieve the
following goals:

\begin{itemize}
\tightlist
\item
  \emph{Clustering}: hard clustering consists in determining a complete
  partition of the \(n\) observations \(x_{1:n}\) into \(k\) disjoint
  non-empty subsets. In the context of \emph{mixture model-based
  clustering}, this is done by assigning each observation \(i\) to the
  cluster \(\hat{s_i}=\arg \max_j \eta_{i} (j)\) that maximises the
  posterior distribution (MAP) (see Equation
  \eqref{eq:posteriori}):
\end{itemize}

\begin{equation}
        \eta_{i} (j) := \mathbb{P}_{\theta} (S_i=j |X_i=x_i)
    \label{eq:posteriori}
\end{equation}

\begin{itemize}
\tightlist
\item
  \emph{Prediction}: the purpose is to predict a response variable \(Y\) from
  an explanatory variable \(X\). The dependent variable \(Y\) can either
  be discrete, taking values in classes \(\{1, \ldots, G\}\)
  (\emph{classification} task) or continuous (\emph{regression} task). In this
  paper, we do not extensively discuss application of mixture models to regression purposes but refer the reader
  to Bouveyron and Girard (2009) for mixture classification and
  Shimizu and Kaneko (2020) for mixtures of regression models.
\end{itemize}

In section \protect\hyperlink{univariate-and-multivariate-gaussian-distributions-in-the-context-of-mixture-models}{Univariate and multivariate Gaussian distributions in the context of mixture models}, we describe the most
commonly used family, the Gaussian Mixture Model (GMM). Then, we present
the MLE estimation of the parameters of a GMM, introducing the classic
EM algorithm in section \protect\hyperlink{parameter-estimation-in-finite-mixtures-models}{Parameter estimation in finite mixtures models}. Finally, we introduce bootstrap methods used to evaluate the
quality of the estimation and metrics used for the selection of the best
model in respectively appendices \protect\hyperlink{derivation-of-confidence-intervals-in-gmms}{Derivation of confidence intervals in GMMs} and \protect\hyperlink{model-selection}{Model selection}.

\hypertarget{univariate-and-multivariate-gaussian-distributions-in-the-context-of-mixture-models}{%
\subsection{Univariate and multivariate Gaussian distributions in the context of mixture models}\label{univariate-and-multivariate-gaussian-distributions-in-the-context-of-mixture-models}}

We focus our study on the finite Gaussian mixture models (GMM) in which we
suppose that each of the \(k\) components follows a Gaussian
distribution.

We recall below the definition of the Gaussian
distribution in both univariate and multivariate context. In the finite univariate Gaussian mixture model, the distribution of
each component \(f_{\zeta j}(X)\) is given by the following univariate
Gaussian p.d.f. (probability density function) (Equation
\eqref{eq:gaussian-dist}):

\begin{equation}
f_{\zeta j}(X=x)=\varphi_{\zeta_j}(x | \mu_j, \sigma_j):=\frac{1}{\sqrt{2\pi} \sigma_j} \exp^{- \frac{(x - \mu_j)^2}{2 \sigma_j^2}}
\label{eq:gaussian-dist}
\end{equation} which we note: \(X \sim \mathcal{N}(\mu_j, \sigma_j)\).

In the univariate case, the parameters to be inferred from each
component, \(\zeta_j\), are: \(\mu_j\), the \emph{location} parameter (equal to
the mean of the distribution) and \(\sigma_j\), the \emph{scale} parameter
(equal to the standard deviation of the distribution with a Gaussian distribution).

Following parsimonious parametrisations with respect to univariate GMMs
are often considered:

\begin{itemize}
\item
  \emph{homoscedascity}: variance is considered equal for all components,
  \(\sigma_j = \sigma, \forall j \in \{1, \ldots, k \}\), as opposed to
  heteroscedascity where each sub-population has its unique
  variability.
\item
  \emph{equi-proportion} among all mixtures:
  \(p_j = \frac{1}{k} \quad j \in \{ 1, \ldots, k\}\) \footnote{A rarer constraint considered implies to enforce a linear
    constraint over the clusters' means, of the following general form:
    \(\sum_{j=1}^k a_j \mu_j=0\), with \(\{a_1, \ldots, a_k\}\). For
    instance, the R package \pkg{epigenomix} considers a \(k=3\) component
    mixture in the context of transcriptomic (differential analyses) and
    epigenetics (histone modification) to automatically identify
    undifferentiated, over and under-expressed genes between case and
    control samples. A common constraint then is to enforce the
    distribution of fold changes corresponding to the undifferentiated
    expressed genes to have a distribution centred on 0. Combining
    equality of means and equality of variances is irrelevant, as the
    model is then degenerate. Additionally, setting constraints on the
    means makes the estimation of the parameters challenging, as
    detailed in section \protect\hyperlink{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{Appendix B: Extensions of the EM algorithm to overcome its limitations}.}
\end{itemize}

In the finite multivariate Gaussian mixture model, the distribution \(f_{\zeta j}(\boldsymbol{X})\) of each component \(j\), where
\(\boldsymbol{X} \in \mathbb{R}^D =(X_1, \ldots, X_D)^\top\) is a multivariate random variable
of dimension \(D\), is given by the
following multivariate Gaussian p.d.f. (probability density function)
(Equation \eqref{eq:multivariate-distribution}):

\begin{equation}
    f_{\zeta j}(\boldsymbol{X}=\boldsymbol{x})=\operatorname{det}(2\pi\boldsymbol{\Sigma}_j)^{-\frac{1}{2}} \exp\left( -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu}_j) \boldsymbol{\Sigma}_j^{-1} (\boldsymbol{x} - \boldsymbol{\mu}_j)^\top\right)
\label{eq:multivariate-distribution}
\end{equation}

which we note
\(\boldsymbol{X} \sim \mathcal{N}_D(\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)\). The parameters to be estimated for each component can be decomposed into:

\begin{itemize}
\item
  \(\boldsymbol{\mu}_j=\begin{pmatrix} \mu_{1j} \\ \vdots \\ \mu_{Dj} \end{pmatrix} \in \mathbb{R}^D\), the \(D\)-dimensional mean vector.
\item
  \(\boldsymbol{\Sigma}_j\), the \(\mathcal{M}_D(\mathbb{R})\) positive-definite \footnote{The positive-definiteness constraint can be interpreted from a probabilistic point of view as a necessary condition such that the generalised integral of the multivariate distribution is defined and sum-to-one over \(\mathbb{R}\) or from the statistical definition of the covariance. A symmetric real matrix \(\boldsymbol{X}\) of rank \(D\) is said to be \textit{positive-definite} if for any non-zero vector
    \(\mathbf{v}, \in \mathbb{R}^D\), the following constraint
    \(\mathbf{v}^\top \boldsymbol{X} \mathbf{v} > 0\) is enforced.}covariance matrix, whose diagonal terms are the individual variance of each feature and the off-diagonal terms the pairwise covariance terms.
\end{itemize}

Three families of multivariate GMMs are often considered:

\begin{itemize}
\tightlist
\item
  the \emph{spherical} family, \(\boldsymbol{\Sigma}_j=\sigma_j^2 \boldsymbol{I}_D\), with \(\sigma_j \in \mathbb{R}_{+}^*\), refers to GMMs whose covariance matrix is diagonal with an unique standard deviation term. The corresponding volume representation is a \(D-\)\emph{hypersphere} of radius \(\sigma_j\).
\item
  the \emph{diagonal} family, \(\boldsymbol{\Sigma}_j=\operatorname{diag} \left(\sigma_{1j}^2, \ldots, \sigma_{1D}^2\right)\), with \(\sigma_j \in \mathbb{R}_{+}^D\), refers to GMMs whose covariance matrix is diagonal. Its associated volume representation is an ellipsoid whose main axes are aligned with the \(D\) canonical basis of \(\mathbb{R}^D\). Of note, the null constraint imposed over the off-diagonal terms in the spherical and diagonal families imply that the multivariate distribution can be further decomposed and analysed as the product of univariate independent Gaussian realisations.
\item
  the \emph{ellipsoidal} family, also named the \emph{general} family, refer to GMMs whose covariance matrix, \(\boldsymbol{\Sigma}_j\), can be any arbitrary positive-definite \(D \times D\) matrix. Thus, the corresponding clusters for each component \(J\) are ellipsoidal, centred at the mean vector \(\boldsymbol{\mu}_j\), and volume and orientation respectively determined by the eigenvalues and the eigenvectors of the covariance matrix \(\boldsymbol{\Sigma}_j\).
\end{itemize}

In the multivariate setting, the volume, shape, and orientation of the covariances can be constrained to be equal or variable across clusters, generating 14 possible parametrisations with different geometric characteristics (Banfield and Raftery 1993; Celeux and Govaert 1992). We review them in Appendix (\protect\hyperlink{parameters-estimation-in-a-high-dimensional-context}{Parameters estimation in a high-dimensional context}) and Table \ref{tab:parameter-configuration-bivariate}. Of note, the
correlation matrix can be easily derived from the covariance
matrix with the following normalisation:
\(\operatorname{cor}(\boldsymbol{X})=\left(\frac{\operatorname{cov}(x_l, x_m)}{\sqrt{\operatorname{var}(x_l)} \times \sqrt{\operatorname{var}(x_m)}}\right)_{(l,m) \in D \times D}\). Correlation if strictly included between -1 and 1, the strength of the
correlation is given by its absolute value while the type of the
interaction is returned by its sign. A correlation of 1 or -1 between two features indicates a strictly linear relationship.

For the sake of simplicity and tractability, we will only consider the
fully unconstrained model in both the univariate (heteroscedastic and
unbalanced classes) and multivariate dimension (unbalanced and complete
covariance matrices for each cluster) in the remainder of our paper.

\hypertarget{parameter-estimation-in-finite-mixtures-models}{%
\subsection{Parameter estimation in finite mixtures models}\label{parameter-estimation-in-finite-mixtures-models}}

A common way for estimating the parameters of a parametric distribution is
the \emph{maximum likelihood estimation} (MLE) method. It consists in
estimating the distribution parameters by maximising the likelihood, or
equivalently the log-likelihood of a sample. In what follows,
\(\ell(\theta|x_{1:n})=\log (f(x_{1:n}|\theta))\) is the log-likelihood of
a \emph{n}-sample. When all observations are independent, it simplifies to
\(\ell(\theta|x_{1:n}) = \sum_{i=1}^n \log (f(x_i|\theta))\). The MLE
consists in finding the parameter estimate \(\hat{\theta}\) maximising the
log-likelihood \(\hat{\theta} = \arg \max \ell (\theta | x_{1:n})\).

Recovering the maximum of a function is generally performed by
determining from which values its derivative cancels. The MLE in GMMs
has interesting properties, as opposed to the \emph{moment estimation}
method: it is a consistent, asymptotically efficient and unbiased
estimator (Jiahua Chen 2016; G. McLachlan and Peel 2000).

When \(S\) is completely observed, for pairs of observations
\((x_{1:n}, s_{1:n})\), the log-likelihood of a finite mixture model is
simply given by Equation \eqref{eq:6}:

\begin{equation}
\ell(\theta|X_{1:n}=x_{1:n}, S_{1:n}=s_{1:n})=\sum_{i=1}^n \sum_{j=1}^k \left[\log\left(f_{\zeta_j} (x_i, s_i=j)\right) + \log(p_j) \right]_{\mathbf{1}_{s_i=j}}
\label{eq:6}
\end{equation}

where an analytical solution can be computed provided that a closed-form estimate exists to retrieve the parameters \(\zeta_j\) for each components' parametric distribution. The MLE maximisation, in this context, amounts to estimate clusterwise each components' parameter, \(\zeta_j\) while the corresponding proportions, \(p_j\), is simply the ratio of the observations assigned to cluster \(j\) over the total number of observations \(n\).

However, when \(S\) is unobserved, the log-likelihood, qualified as
incomplete with respect to the previous case, is given by Equation
\eqref{eq:7}:

\begin{equation}
\ell (\theta \vert x_{1:n}) = \sum_{i=1}^n  \log \left( \underbrace{\sum_{j=1}^k  p_j f_{\zeta_j}(x_i)}_{\text{sum of of logs}} \right)
\label{eq:7}
\end{equation}

The sum of terms embed in the log function (see underbrace section in Equation \eqref{eq:7}) makes it intractable in practice to derive the null values of its corresponding derivative. Thus, no closed form of the MLE is available,
including for the basic univariate GMM model. This is why most
parameter estimation methods derive instead from the \emph{EM algorithm},
first described in Dempster, Laird, and Rubin (1977). We detail in the next section its
main theoretical properties, the reasons of its popularity as well as
its main limitations.

\hypertarget{the-em-algorithm}{%
\subsection{The EM algorithm}\label{the-em-algorithm}}

When both \(S\) and the parameters of the model are unknown, no closed-form solution exists to jointly optimise the log-likelihood (Equation \eqref{eq:7}) parametrised by \(({\theta}, S)\).
However, when either \(S\) or \(\theta\) are known, the estimation of the
other parameters is straightforward. Hence, the general principle of
EM-like algorithms is splitting this complex non-closed joint MLE
estimation of \((S, \theta)\) into the iterative estimation of \(S_q\) from
\((\hat{\theta}_{q-1}, X)\) (expectation phase, or \emph{E-step} of the algorithm)
and the estimation of \(\hat{\theta}_{q}\) from \((S_q, X)\) (maximisation phase, or
\emph{M-step}), with \(\hat{\theta}_{q-1}\) being the estimated parameters at the
previous step \(q-1\), until we reach the convergence.

The EM algorithm sets itself apart from other commonly used methods by taking
into account all possible values taken by the latent variable \(S\). To do
so, it computes the expected value of the log likelihood of \(\theta\),
conditioned by the posterior distribution
\(\mathbb{P}_{\hat{\theta}_{q-1}} (S|X)\), also named as the \emph{auxiliary
function}. Making profit of the independence assumption between the
observations of a mixture model, the general formula of this proxy
function of the incomplete log-likelihood is given in finite mixture
models by Equation \eqref{eq:8}.

\begin{equation}
\begin{split}
Q(\theta|\hat{\theta}_{q-1}) & := \mathbb{E}_{S_{1:n}| X_{1:n}, \hat{\theta}_{q-1}} \left[\ell(\theta | X_{1:n}, S_{1:n})\right] \\
&=\sum_{i=1}^n \sum_{j=1}^k \eta_{i}(j) \left( \log (p_j) +  \log (\mathbb{P}(X_i|S_i=j, \theta)) \right)
\end{split}
\label{eq:8}
\end{equation}

with \(\hat{\theta}_{q-1}=\hat{\theta}\) the current estimated parameter
value.

In practice, the EM algorithm consists in performing alternatively E-step and M-step until convergence. We supplied below a pseudocode version:

\begin{blackbox}{\textbf{The EM algorithm}}

\begin{center}

\begin{itemize}
\tightlist
\item
  \emph{step E}: determine the posterior probability function \(\eta_i(j)\)
  for each observation of \(X\) for each possible discrete latent class,
  using the initial estimates \(\hat{\theta}_0\) at step \(q=0\), or the
  previously computed estimates \(\hat{\theta}_{q-1}\). The general formula is given by Equation \eqref{eq:10}:
\end{itemize}

\begin{equation}
    \eta_i(j) = \frac{p_j f_{\zeta_j} (x_i)}{\sum_{j=1}^k p_j f_{\zeta_j} (x_i)}
\label{eq:10}
\end{equation}

\begin{itemize}
\tightlist
\item
  \emph{step M}: compute the mapping function
  \(\hat{\theta}_q:=M(\theta | \hat{\theta}_{q-1})=\arg \max Q(\theta| \hat{\theta}_{q-1})\) which maximises the auxiliary function. One way of retrieving the MLE associated to the auxiliary function is to determine the roots of its derivative, namely solving Equation \eqref{eq:mapping-function-derivative}\footnote{To ensure
    that we reach a maximum, we should assert that the Hessian matrix evaluated at the MLE is indeed negative definite.}:
\end{itemize}

\begin{equation}
    \frac{\partial Q(\theta| \hat{\theta}_{q-1})}{\partial \theta}=0
\label{eq:mapping-function-derivative}
\end{equation}

\end{center}

\end{blackbox}

Interestingly, the decomposition of the incomplete log-likelihood
associated to a mixture model \(\ell(\theta|X)\) reveals an entropy term
and the so-called auxiliary function (Dempster, Laird, and Rubin 1977). It can be used to prove that
maximising the auxiliary function at each step induces a bounded
increase of the incomplete log-likelihood. Namely, the convergence of
the EM algorithm, defined by comparisons of consecutive log-likelihood,
is guaranteed, provided the mapping function returns the maximum of the
auxiliary function. Yet, the convergence of the series of estimated
parameters
\((\theta_q)_{q \ge 0} \underset{i\to +\infty}{\longrightarrow} \hat{\theta}\)
is harder to prove but is considered asserted for the probability family
of \emph{exponential laws} (a superset of the Gaussian family), as stated in
Dempster, Laird, and Rubin (1977).

Additionally, the EM algorithm is \emph{deterministic}, meaning that for a
given initial estimate \(\theta_0\) the parameters returned by the
algorithm at a given step \(q\) are fixed. Yet, it requires the user to
provide an initial guess \(\theta_0\) on the estimate parameters and to
set the number of components of the mixture. We review some classic
initialisation methods in \protect\hyperlink{initialisation-of-the-em-algorithm}{Initialisation of the EM algorithm} and some
algorithms used to overcome the main limitations of the EM
algorithm in the Appendix \protect\hyperlink{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{Appendix B: Extensions of the EM algorithm to overcome its limitations}.

Finally, the prevalent choice of Gaussian distributions to model the distribution of random observations proceeds from a strong set of interesting properties and from their strong tractability. In particular, Geary (1936) has shown that the Normal distribution is the only distribution for which the Cochran's theorem (Cochran 1934) is guaranteed, namely for which the the mean and variance of the sample are independent of each other. Additionally, similar to any distribution proceeding from the exponential family, the MLE statistic is \emph{sufficient}\footnote{The Pitman--Koopman--Darmois theorem (Koopman 1936) states that only the exponential family provides distributions whose statistic can summarize arbitrary amounts of iid draws using a finite number of values}.

\hypertarget{initialisation-of-the-em-algorithm}{%
\subsection{Initialisation of the EM algorithm}\label{initialisation-of-the-em-algorithm}}

EM-like algorithms require an initial parameter estimate \(\theta_0\) to
optimise the maximum likelihood. \emph{Initialisation} is a crucial step, as
a bad initialisation would possibly lead to a local sub-optimal solution
or trap the algorithm in the boundary of the parameter space. The
simplest initialisation methods do not require any other initialisation
algorithm, while meta-methods include as well an initialisation step.
The commonly-used initialisation methods are:

\begin{itemize}
\item
  The \emph{Model-based Hierarchical Agglomerative Clustering} (MBHC) is
  an agglomerative hierarchical clustering based on MLE criteria applied to
  GMMs (Scrucca and Raftery 2015). First, the MBHC is
  initialised by assigning each observation to its own cluster. Next,
  the pair of clusters that maximises the likelihood of the underlying
  statistical model among all possible pairs is merged. This procedure
  is repeated until all clusters are merged. The final resulting
  clusters are then simply the last \(k\) cuts of the resulting
  dendrogram. In the homoscedastic case in the univariate setting, and the diagonal family with shared variance across all components, the underlying
  merging criteria reveals similar to the \emph{Ward's criterion}, with the
  merged pair of clusters being the one minimising the sum of squares.
  As opposed to other initialisation methods described after, MBHC is a
  deterministic method that hence does not require careful calibration of
  hyperparameters. However, as acknowledged by the creator of the method itself of the method (Fraley 1998), the resulting partitions are generally
  suboptimal compared to other initialisation methods.
\item
  The standard \emph{random} initialisation, used classically for the
  initialisation step of the \emph{k}-means algorithm, consists in randomly
  selecting \(k\) distinct observations, referred to as \emph{centroids} and
  then assigns each observation to the closest centroid
  (Biernacki, Celeux, and Govaert 2003). Doing so is close from the C-step of the CEM algorithm.
  This is the method used in this paper, unless otherwise stated. Alternative versions of this method have been developed: for instance, the package \pkg{mixtools} draws the proportions of the components from a Dirichlet distribution, whose main advantage lies on respecting the unit simplex constraint (Equation \eqref{eq:2})\footnote{Without prior knowledge favouring one component over another, the Dirichlet distribution is generally parametrised by \(\alpha=\frac{1}{k}\), implicitly stating that any observation has equal chance to proceed from a given cluster. In that case, the corresponding distribution is parametrised by a single scalar value \(\alpha\), called the \emph{concentration parameter}.}, but uses binning methods to guess the means and standard deviations of the components. In paper Kwedlo (2013), the means of the components are randomly chosen but with additional constraint of maximising the Mahalanobis distance between the selected centroids. This enables to cover a larger portion of the parameters' space.
\item
  \emph{k}-means is a CEM algorithm, in which the additional assumption of
  balanced classes and homoscedascity implies that each observation in
  the E-step is assigned to the cluster with the nearest mean (the one
  with the shortest Euclidean distance). \emph{K}-means is initialised by
  randomly selecting \(k\) points, the \emph{centroids}. It is often chosen
  for its fast convergence and memory-saving consumption.
\item
  The \emph{quantile} method sorts each observation \(x_i\) by increasing
  order and splits them into equi-balanced quantiles of size \(1/k\). Then, all
  observations for a given quantile are assumed to belong to the same
  component. \footnote{This method is only available in the univariate framework,
  since it is not possible to define a unique partition of the observable space into $k$-splits. For example, in bivariate setting, a binning with $k=2$ components on each axis leads to a total of $2 \times 2=4$ binned regions, which raises the selection issue of the best $k$ hyper-squared volumes for the initial parameters estimation. More generally, $\binom{D}{k}$ binning choices are possible in the multivariate setting.}
\item
  The \emph{Rough-Enhanced-Bayes mixture} (REBMIX) algorithm is implemented
  in the \CRANpkg{rebmix} (Nagode 2022) package and the complete
  pseudo-code is described thoroughly in (Nagode 2015; Panic, Klemenc, and Nagode 2020). First, the
  observations are processed using one of the three available methods:
  \emph{k}-nearest neighbours (KNN), Parzen kernel density estimation or
  binned intervals, the method we used for this paper. In this method, firstly, data are
  binned in \(\sqrt{n}^D\) intervals of equal lengths. Then, the mode of
  one of the components' distribution is given by the centre of the interval
  with the highest frequency. This interval is also used to define a
  ``rough'' parameter estimation of its associated component. All the
  other observations and intervals are then iteratively assigned to
  the currently estimated component or to the residual components that
  were not estimated. The algorithm switches to the estimation of
  another component when at least \(25\%\) of the currently non
  assigned intervals are associated to the currently estimated
  component.
  The decision of assigning an interval to either a currently estimated component or residuals relies on the measurement of the deviation between the observed and the expected
  frequency of the interval, a low value supporting the choice of the algorithm to
  assign the interval to the currently estimated component. Finally,
  all intervals assigned to the estimated component are used to
  determine the parameters of the associated Gaussian distribution (computation of the first and second moments of the distribution). Since this step relies on a higher number of observations to estimate the parameters, Nagode (2015) refers to it as ``enhanced'' components parameter estimation.
  The algorithm stops when all intervals are assigned to a cluster and
  the parameters of the several distributions' components are
  estimated. Accordingly, the rebmix algorithm can be interpreted as a natural and more general extension of the quantile method with a more rigorous statistical support. Two drawbacks of the algorithm are the need of an intensive calibration of the hyperparameters and its inadequacy for the
  estimation of highly overlapping mixture distributions (uncertainty
  of the cluster assignment is not taken into account).\footnote{The method we describe here to preprocess the observations in order to estimate the empirical density estimation, namely the ``histogram method'' is not well suited for high dimensional data, as the exponential growth of the volume with respect to dimensionality leads to data sparsity, related to the well-known issue of the ``curse of dimensionality''. Indeed, \(\sqrt{n}^D\) distinct intervals will be parsed by the method and the probability with an increasing number of features and decreasing number of observations that no clear local maximum emerges converges to 1. In high-dimensional context, the Parzen window or the KNN method should be favoured, see (Nagode 2015), p.~16.}.
\item
  The \emph{meta-methods} consist generally in short runs of EM-like
  algorithms, namely CEM, SEM and EM (see \protect\hyperlink{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{Appendix B: Extensions of the EM algorithm to overcome its limitations}), with alleviated convergence criterion. The main idea is
  to use several random initial estimates with shorter runs of the
  algorithm to explore a larger parameter space, and avoid being
  trapped in a local maximum. Yet, they highly depend on the choice of the initialisation algorithm to start the optimisation (Biernacki, Celeux, and Govaert 2003).
\item
  \color{blue}In the high-dimensional setting, if the number of dimensions \(D\) exceeds the number of observations \(n\), all previous methods must be adjusted, usually by first projecting the dataset into a smaller, suitable subspace and then inferring prior parameters in it. In particular, \pkg{EMMIXmfa}, in the mixture of common factor analysers (MCFA) approach, initialises the shared projection matrix \(\boldsymbol{Q}\) by either keeping the first \(d\) eigen vectors generated from standard principal component analysis or uses custom random initialisations (described in further details in the Appendix of Baek, McLachlan, and Flack (2010)). \color{black}
\end{itemize}

After this theoretical introduction, we evaluate empirically the
computational and statistical performances of the R packages in relation
with the choice of the initialisation algorithm and the complexity of
the simulation in the next section. The method used to compare the seven reviewed
packages is detailed in \protect\hyperlink{methods}{Methods}, while the key results are reported in
section \protect\hyperlink{results}{Results}. We conclude by providing a general simplified
framework to select the combination of package and initialisation method best suited to its objectives and the nature of the distribution of the dataset.

\hypertarget{a-comprehensive-benchmark-comparing-estimation-performance-of-gmms}{%
\section{A comprehensive benchmark comparing estimation performance of GMMs}\label{a-comprehensive-benchmark-comparing-estimation-performance-of-gmms}}

We searched CRAN and Bioconductor mirrors for packages that can retrieve
parameters of GMM models. Briefly, out of 54 packages dealing with GMMs
estimation, we focused on seven packages that all estimate the
MLE in GMMs using the EM algorithm, were recently
updated and let the user provide its own initialisation estimates:
\pkg{bgmm}, \pkg{EMCluster}, \pkg{flexmix},
\pkg{GMKMcharlie}, \pkg{mclust}, \pkg{mixtools} and \pkg{Rmixmod}. The
complete inclusion process is detailed in \protect\hyperlink{appendix-c-the-meta-analysis-workflow-for-the-final-selection-of-cran-and-bioconductor-platforms}{Appendix C: the meta-analysis workflow for the final selection of CRAN and Bioconductor platforms}. The flowchart summarising our choices is represented in Figure \ref{fig:flowchart}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/flowchart_packages_selection} 

}

\caption{A minimal roadmap used for the selection of the packages reviewed in our benchmark.}\label{fig:flowchart}
\end{figure}

In parallel, we include two additional packages dedicated specifically to high-dimensional settings (they are not tailored to deal with univariate or bivariate dimensions), namely \CRANpkg{EMMIXmfa} (Rathnayake et al. 2019) and \CRANpkg{HDclassif} (Berge, Bouveyron, and Girard 2019) to compare their performance with standard multivariate approaches in complex, but non degenerate cases. We summarise the main features and use cases of the seven + two reviewed
packages in Table
\ref{tab:low-comparison-package-pdf}.
The three most commonly used packages are \pkg{mixtools}, \pkg{mclust} and \pkg{flexmix}. However, the \pkg{mclust} package is by far the most complete with many features provided to visualise and evaluate the quality of the GMM estimate. \pkg{bgmm} requires the greatest number of packages for its installation, making its upkeep
a harder task, while \pkg{mclust} only depends of base R packages
implemented by default in each new R version. Additionally, in parallel
to clustering tasks, \pkg{flexmix} and \pkg{mixtools} packages perform
regression of mixtures and implement mixture models using other
parametric distributions or non-parametric methods via kernel-density
estimation.

\begin{table}[!h]

\caption{\label{tab:low-comparison-package-pdf}Main features of the reviewed packages, sorted by decreasing number of daily downloads.
               \textit{Downloads per day} returns the daily average number of downloads for each package on the last 2 years.
               \textit{Recursive dependencies} column counts the complete set of non-base packages required,
               as first-order dependencies may require as well installation of other packages.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llllrllrl}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{Version}} & \multicolumn{1}{c}{\textbf{Regression}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Implemented \\ models}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Downloads \\ per day}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Last \\ update}}} & \multicolumn{1}{c}{\textbf{Imports}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Recursive \\ dependencies}}} & \multicolumn{1}{c}{\textbf{Language}}\\
\midrule
\midrule
mclust & 5.4.7 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 5223 & 31/10/2022 & R ($\ge$ 3.0) & 0 & Fortran\\
flexmix & 2.3-17 & \includegraphics[scale=0.05]{figs/green_tick.png}& \makecell[c]{Poisson, binary,\\non-parametric,\\semi-parametric} & 3852 & 07/06/2022 & \makecell[c]{R ($\ge$ 2.15.0), modeltools,\\nnet, stats4} & 3 & R\\
mixtools & 1.2.0 & \includegraphics[scale=0.05]{figs/green_tick.png}& \makecell[r]{multinomial, gamma,\\Weibull, non-parametric,\\semi-parametric} & 178 & 05/02/2022 & \makecell[r]{R ($\ge$ 3.5.0), kernlab,\\segmented, survival} & 6 & C\\
Rmixmod & 2.1.5 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 39 & 18/10/2022 & \makecell[l]{R ($\ge$ 2.12.0), Rcpp,\\RcppEigen} & 4 & C++\\
EMCluster & 0.2-13 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 33 & 12/08/2022 & R ($\ge$ 3.0.1), Matrix & 3 & C\\
\addlinespace
bgmm & 1.8.4 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 27 & 10/10/2021 & \makecell[r]{\\R ($\ge$ 2.0),\\mvtnorm, combinat} & 77 & R\\
GMKMcharlie & 1.1.1 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 12 & 29/05/2021 & \makecell[l]{Rcpp, RcppParallel,\\RcppArmadillo} & 3 & C++\\
\midrule
EMMIXmfa & 2.0.11 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 12 & 16/12/2019 & NA & 0 & C\\
HDclassif & 2.2.0 & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & $\includegraphics[scale=0.05]{figs/red_cross.png}$ & 35 & 12/10/2022 & rARPACK & 13 & R\\
\bottomrule
\end{tabular}}
\end{table}

We further detail features specifically related to GMMs in Table
\ref{tab:high-comparison-packages-pdf}. We detail row after row its content below:

\begin{itemize}
\item
  The parametrisations used to provide parsimonious estimation of the GMMs are reviewed in \protect\hyperlink{parameter-estimation-in-finite-mixtures-models}{Parameter estimation in finite mixtures models} and summarised in rows 1 and 2 (Table \ref{tab:high-comparison-packages-pdf}) for the univariate and multivariate setting. We refer to them as \emph{canonical} when both homoscedastic and heteroscedastic parametrisations in the univariate setting, and the 14 parametrisations listed in Table \ref{tab:param-multivariate-gaussian} in the multivariate setting, are implemented. Adding the additional constraint of equi-balanced clusters results in a total to \(14 \times 2=28\) distinct models and \(2 \times 2=4\) parametrisations, respectively in the univariate and multivariate setting. \color{blue}Since \pkg{EMMIXmfa} and \pkg{HDclassif} are dedicated to the analysis of high-dimensional datasets, they project the observations in a smaller subspace and are not available in the univariate setting. Given an user-defined or prior computed intrinsic dimension, we can imagine using any of the standard parametrisations available for instance in the \pkg{mclust} package, and listed in \nameref{subsec:parsimonious-parametrisation}. In addition, \pkg{HDclassif} allows each cluster \(j\) to be represented with its own subspace intrinsic dimension \(d_j\), as we describe in further details in \nameref{subsec:high-dimensional}. \color{black}
\item
  \protect\hyperlink{the-em-algorithm}{The EM algorithm} is the most commonly used
  algorithm to retrieve the parameters of the GMMs but faster or variants complementary of the EM algorithm are reviewed in \protect\hyperlink{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{Appendix B: Extensions of the EM algorithm to overcome its limitations} and row 3 of Table \ref{tab:high-comparison-packages-pdf}. Especially, GMMs estimation is particularly impacted by the presence of outliers, justifying a specific benchmark ( see \protect\hyperlink{a-small-simulation-to-evaluate-the-impact-of-outliers}{A small simulation to evaluate the impact of outliers}).
  We briefly review the most common initialisation algorithms in section \protect\hyperlink{initialisation-of-the-em-algorithm}{Initialisation of the EM algorithm} and row 4 of Table \ref{tab:high-comparison-packages-pdf}, a necessary and tedious task for both the EM algorithm and its alternatives.
\item
  To select the best parametrisations and number of components that fit the mixture, several metrics are provided by the reviewed packages
  (\protect\hyperlink{model-selection}{Model selection} and row 5). Due to the complexity of computing the true distribution of the parameters estimated, bootstrap methods are commonly used used to derive confidence intervals (see \protect\hyperlink{derivation-of-confidence-intervals-in-gmms}{Derivation of
  confidence intervals in GMMs} and row 6 in Table \ref{tab:high-comparison-packages-pdf}.
\end{itemize}

\begin{itemize}
\tightlist
\item
  Six packages supply several visualisations, summarised in the last row of Table \ref{tab:high-comparison-packages-pdf}, to display either the distributions corresponding to the estimated parameters or compare quickly the performance between the packages. However, \pkg{mclust} is by far the most complete one with performance plots to quickly set apart several parametrisations with respect to a performance metric, density plots (in the univariate setting) and isodensity plots (bi-dimensional in the bivariate setting or in higher dimensions after appropriate dimensionality reduction), with the additional possibility of integrating custom confidence intervals and critical regions, and finally boxplot bootstrap representations to take in at a glance the distribution of the benchmarked estimated parameters.
  \color{blue}
  High-dimensional packages provide specific representations adjusted to the high-dimensional settings, notably allowing the user to visualise the projected factorial representation of its dataset in a two or three-dimensional subspace. They also provide specialised performance plots, notably scree plots or BIC scatter plots to represent in a compact way numerous projections and parametrisations.
  \color{black}
\end{itemize}

\begin{table}[!h]

\caption{\label{tab:high-comparison-packages-pdf}Custom features associated to GMMs estimation for any of the benchmarked packages.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{}l|>{\raggedright\arraybackslash}m{4cm}>{\raggedright\arraybackslash}m{1.6cm}>{\raggedright\arraybackslash}m{1.6cm}>{\raggedright\arraybackslash}m{1.6cm}>{\raggedright\arraybackslash}m{1.6cm}>{\raggedright\arraybackslash}m{1.6cm}>{}l|>{\raggedright\arraybackslash}m{1.6cm}>{\raggedright\arraybackslash}m{1.6cm}}
\toprule
\multicolumn{1}{c}{\textbf{ }} & \multicolumn{1}{c}{\textbf{mclust}} & \multicolumn{1}{c}{\textbf{flexmix}} & \multicolumn{1}{c}{\textbf{mixtools}} & \multicolumn{1}{c}{\textbf{Rmixmod}} & \multicolumn{1}{c}{\textbf{EMCluster}} & \multicolumn{1}{c}{\textbf{bgmm}} & \multicolumn{1}{c}{\textbf{GMKMcharlie}} & \multicolumn{1}{c}{\textbf{EMMIXmfa}} & \multicolumn{1}{c}{\textbf{HDclassif}}\\
\midrule
\textbf{Models Available (univariate)} & canonical & unconstrained & canonical & canonical & unconstrained & canonical & unconstrained & NA & NA\\
\midrule
\textbf{Models Available (multivariate)} & canonical & unconstrained diagonal or general & unconstrained & canonical & unconstrained & 4 models (diagonal and general, either component specific or global) & unconstrained & 4 models (either component-wise or common, on the intrinsic and diagonal residual error covariance matrices) & canonical on projected dimension\\
\midrule
\textbf{Variants of the EM algorithm} & VBEM & SEM, CEM & ECM & SEM, CEM & \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& CW-EM, MML & AECM & SEM, CEM\\
\midrule
\textbf{Initialisation} & hierarchical clustering, quantile & short-EM, random & random & random, short-EM, CEM, SEM & random, short-EM & k-means, quantile & k-means & k-means, random, heuristic & short-EM, random, k-means\\
\midrule
\textbf{Model or Cluster Selection} & BIC, ICL, LRTS & AIC, BIC, ICL & AIC, BIC, ICL, CAIC, LRTS & BIC, ICL, NEC & AIC, BIC, ICL, CLC & GIC & \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& BIC, ICL, CV\\
\midrule
\addlinespace
\textbf{Bootstrap Confidence Intervals} & \includegraphics[scale=0.05]{figs/green_tick.png}& \includegraphics[scale=0.05]{figs/green_tick.png}& \includegraphics[scale=0.05]{figs/green_tick.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}& \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
\textbf{Visualisation} & performance, histograms and boxplots of bootstrapped estimates, density plots (univariate),  scatter plots with uncertainity regions and boundaries (bivariate), isodensity (bivariate , 2D projected PCA or selecting coordinates) & \includegraphics[scale=0.05]{figs/red_cross.png}& density curves & density curves, scatter plots with uncertainty boundaries & \includegraphics[scale=0.05]{figs/red_cross.png}& performance, scatter plots with uncertainty boundaries & \includegraphics[scale=0.05]{figs/red_cross.png}& projected factorial map & projected factorial map, performance (Cattell's scree plot, BIC performance, slope heuristic)\\
\midrule
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

In addition to the the seven packages selected for our benchmark, we
include a custom R implementation of the EM algorithm used
as baseline, referred to as \emph{RGMMBench}, and for the high-dimensional setting we select packages \pkg{EMMIXmfa} and \pkg{HDclassif}, on the basis of criteria detailed in Appendix \protect\hyperlink{general-workflow}{General workflow}. Code for \emph{RGMMBench} is provided in \protect\hyperlink{application-of-the-em-algorithm-to-gmms}{Application of the EM algorithm to GMMs}. To compare the statistical performances of these packages, we performed \emph{parametric bootstrap} (\protect\hyperlink{derivation-of-confidence-intervals-in-gmms}{Derivation of confidence intervals in GMMs}) and built an experimental design to cover distinct mixture distributions parameter configurations, using prior user-defined parameters.

For each experiment, we assign each observation to an unique cluster by drawing \(n\) labels \(S_{1:n}\) from a multinomial distribution whose parameters were the prior user-defined proportions \(p=(p_1, \ldots, p_k)\). Then, each observation \(x_i\) assigned to hidden component \(j\) is drawn respectively using the R function \code{stats::rnorm()} for the univariate distribution and \code{MASS::mvrnorm} for the multivariate distribution. The complete code used for simulating data is
available on GitHub at \href{https://github.com/bastienchassagnol-servier/RGMMBench}{RGMMBench}. Finally, we obtain an empirical distribution of the estimated parameters by computing the MLE
of each randomly generated sample.

For all the packages, we have used the same combination of criterion threshold of
\(10^{-6}\) and maximal number, 1000, of iterations as a numerical proof of convergence. We simulated data with \(n = 200\) draws in the univariate setting and \(n=500\) in the bivariate framework to lower the probability of generating a sample without drawing any observation from one of the components, especially in case of a highly-unbalanced scenario \protect\hyperlink{practical-details-for-the-implementation-of-our-benchmark}{Practical details for the implementation of our benchmark}. Unless stated explicitly, we keep the default hyper-parameters and custom global options provided by each package. For instance, the \pkg{flexmix} package has a default option, \emph{minprior}, set by default to 0.05 and which removes any component present in the mixture with a ratio below \(0.05\). Besides, we only implement the fully unconstrained model in both univariate and multivariate settings, as it is the only parametrisation implemented in all the seven packages and the most popular to perform classic GMM clustering, since fewer hypothesis are required.

We compared the packages' performances using five
initialisation methods: random, quantile, \emph{k}-means, rebmix and hierarchical clustering in the univariate setting. We benchmarked the same initialisation methods in the multivariate setting, except for the quantile method which has no multivariate equivalent (see section \protect\hyperlink{initialisation-of-the-em-algorithm}{Initialisation of the EM algorithm}):

\begin{itemize}
\item
  We used the function \code{EMCluster::rand.EM()} with 10 random restarts and required minimal individual cluster size of 2 for the random initialisation. The method implemented by \pkg{EMCluster} is the most commonly used, described in details in Biernacki, Celeux, and Govaert (2003) and in section \protect\hyperlink{initialisation-of-the-em-algorithm}{Initialisation of the EM algorithm}.
\item
  We used the function \code{stats::kmeans()} function with a \(10^{-2}\) stopping criterion and 200 maximal number of iterations to implement the \emph{k}-means initialisation method.
  The initial centroid and covariance matrix for each component are then computed by restricting to the sample observations assigned to the corresponding component. The approach is close to the one adopted by the CEM algorithm (see \protect\hyperlink{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{Appendix B: Extensions of the EM algorithm to overcome its limitations}).
\item
  We use the \code{mclust::hcV()} function for the MBHC algorithm. This method has two main limitations: just like the \emph{k}-means implementation, it only returns a cluster assignment to each observation instead of the posterior probabilities, and the splitting process to generate the clusters results sometimes in clusters composed of only one observation. In that case, we add a small epsilon to each posterior probability to avoid Dirac degenerated distributions.
\item
  We used in the univariate setting \code{bgmm::init.model.params} for the quantiles initialisation.
\item
  To implement the rebmix method, we use the \code{rebmix::REBMIX} function, using the \emph{kernel density estimation} for the estimation of the empirical density distribution coupled with \emph{EMcontrol} set to one to prevent the algorithm from starting EM iterations.
\item
  Any of the seven packages could be used to implement the small EM method. We decided to use the \code{mixtools::normalmixEM} as it is the closest one to our custom implementation, setting 10 random restarts, a maximal number of iterations of 200 and with an alleviated absolute threshold of \(10^{-2}\). However, preliminary experiments suggest us to discard the small EM
  initialisation method, as it tends to smooth the differences between the packages, as illustrated in supplementary Figure \ref{fig:heatmap-all-correlation-plots-univariate}.
\end{itemize}

We sum up in Table \ref{tab:general-parameter-description-pdf} the general configuration used to run the scripts. Additionally, all simulations were run with the same stable R version 4.0.2 (2020-06-22) using an OS system under Linux and numerical precision of \(2.22 \times 10^{-16}\).

\begin{table}[!h]

\caption{\label{tab:general-parameter-description-pdf}Global options shared by all the benchmarked packages.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{\centering\arraybackslash}p{4.1cm}>{\centering\arraybackslash}p{4.1cm}>{\centering\arraybackslash}p{4.1cm}>{\centering\arraybackslash}p{4.1cm}>{\centering\arraybackslash}p{4.1cm}}
\toprule
Initialisation methods & Algorithms & Criterion threshold & Maximal iterations & Number of observations\\
\midrule
midrule
hc, kmeans, small EM,rebmix, quantiles, random & EM R, Rmixmod, bgmm, mclust, flexmix, EMCluster, mixtools, GMKMCharlie & $10^{-6}$ & 1000 & 100, 200, 500, 1000, 2000, 5000, 10000\\
\bottomrule
\end{tabular}}
\end{table}

Preliminary experiments suggested that the quality of the estimation of a
GMM is mostly affected by the overlap between components' distribution
and level of unbalance between components. We quantified the overlap between two components by the following overlap score (OVL) (see Equation \eqref{eq:overlap}), with a smaller score denoting well-separated components:

\begin{equation}
       \operatorname{OVL}(i, j) = \int \min (f_{\zeta_i} (x), f_{\zeta_j} (x)) dx \quad \text{ with } i \neq j
  \label{eq:overlap}
\end{equation}

We may generalise this definition to \(k\) components by averaging the
individual components' overlap. We use the function
\code{MixSim::overlap} from the \CRANpkg{MixSim} package (Melnykov, Chen, and Maitra 2021) that approximates this quantity using a Monte-Carlo based method (see appendices \protect\hyperlink{an-analytic-formula-of-the-overlap-for-univariate-gaussian-mixtures}{An analytic formula of the overlap for univariate Gaussian mixtures} and \protect\hyperlink{practical-details-for-the-implementation-of-our-benchmark}{Practical details for the implementation of our benchmark} for further details).

The level of imbalance may be evaluated with the entropy measure Equation \eqref{eq:entropy}, with equi-balanced clusters having an entropy of 1:

\begin{equation}
    H(S)=-\sum_{j=1}^k p_j \log_k (p_j)
\label{eq:entropy}
\end{equation}

with \(k\) the number of components and \(p_j=\mathbb{P}(S=j)\) the frequency of class \(j\).

We considered 9 distinct configuration parameters, associated with distinct values of \(\operatorname{OVL}\) and entropy in the univariate setting, 20 scenarios in the bivariate setting and 16 scenarios in the high-dimensional setting. Briefly, we compute any combination, with the same components means (0, 4, 8 and 12), three sets of ratio parametrisations \(\left[(0.25, 0.25, 0.25, 0.25); (0.2, 0.4, 0.2, 0.2); (0.1, 0.7, 0.1, 0.1) \right]\) and three variances: \((0.3, 1, 2)\) in the univariate setting. In the bivariate setting, we consider two sets of proportions: \(\left[(0.5, 0.5); (0.9, 0.1) \right]\), two sets of coordinate centroids: \(\left[(0; 20), (20, 0) \right]\) and \(\left[(0; 2), (2, 0) \right]\), the same variance of 1 for each feature and for each component for illustrative purposes of the direct relation linking the correlation and the level of OVL and five correlation sets:\([(-0.8, -0.8); (0.8, -0.8);\allowbreak (-0.8, 0.8); (0.8, 0.8); (0, 0)]\).

\color{blue}And finally, we tested eight scenarios in the high-dimensional framework, setting to \(D=10\) the number of dimensions, playing on the level of overlap, imbalance between the components' proportions and the underlying constraint assigned to the covariance matrix (either fully parametrised or spherical), without explicitly providing this restriction to the packages. Each of the parameter configuration tested in the high-dimensional setting was evaluated with \(n=200\) observations and \(n=2000\) observations. Additionally, instead of defining manually interesting parameters for a high-dimensional scenario, we take profit of the simulation framework set up in \code{MixSim} function, from the \CRANpkg{MixSim} package (Melnykov, Chen, and Maitra 2021). This function returns the user a fully parametrised GMM, with a prior defined level of maximum or average overlap\footnote{Unfortunately, as discussed in further details in Appendix \protect\hyperlink{an-analytic-formula-of-the-overlap-for-univariate-gaussian-mixtures}{An analytic formula of the overlap for univariate Gaussian mixtures}, the MixSim package does not compute the global distribution overlap, but instead returns the mean of pairwise overlap between any component (however, with two components, these two alternative definitions match.) Finally, it is not possible to set the proportions of the components before the generation of the parameters, except for clusters with equal proportions, and contrary to the expect behaviour of additional parameter \emph{PiLow}, supposed to define the smallest mixing proportion.}.
\color{black}

The complete list of parameters used is reported respectively in Table \ref{tab:parameter-configuration-univariate} for the univariate setting, Table \ref{tab:parameter-configuration-bivariate} for the bivariate setting and \ref{tab:parameter-configuration-HD} for the high-dimensional setting. We benchmarked simulations where the components were alternatively very distinct or instead very overlapping, as well as of equal proportions or instead very unbalanced. The adjustments made to meet the specific requirements of high dimensional packages are detailed in \protect\hyperlink{practical-details-for-the-implementation-of-our-benchmark}{Practical details for the implementation of our benchmark}.

We report the most significant results and features and the associated recommendations in next section \protect\hyperlink{results}{Results}.

\hypertarget{results}{%
\subsection{Results}\label{results}}

All the figures and performance overview tables are reported in \protect\hyperlink{supplementary-figures-and-tables-in-the-univariate-simulation}{Supplementary Figures and Tables in the univariate simulation} for the univariate setting, \protect\hyperlink{supplementary-figures-and-tables-in-the-bivariate-simulation}{Supplementary Figures and Tables in the bivariate simulation} for the bivariate scenario and \protect\hyperlink{supplementary-figures-and-tables-in-the-hd-simulation}{Supplementary Figures and Tables in the HD simulation} for the high dimensional scenario.

\textbf{Balanced and non-overlapping components}

In the univariate setting, with balanced components and low \(\operatorname{OVL}\) (scenario U1 in Table
\ref{tab:parameter-configuration-univariate}), the parameter
estimates are identical in most cases across initialisation methods and packages, notably the same estimates are returned with \emph{k}-means or rebmix initialisation. However, the random initialisation method leads
to a higher variance and bias on the parameter estimates than other
methods (Figure \ref{fig:four-component-balanced-separated} and Table
\ref{tab:balanced-well-separated-table-univariate-pdf}),
with various optimisations returning only local maxima far from the
optimal estimate.

Similar scenarios in the bivariate setting (scenarios B6-B10 in Table \ref{tab:parameter-configuration-bivariate}), with a focus on B6, B7 and B10 visualised in Supplementary Figures \ref{fig:general-balanced-well-separated-bivariate}, feature well-separated and balanced components. Consistent with conclusions from the corresponding univariate scenarios, all benchmarked packages return the same estimates across initialisation methods.

\textbf{Unbalanced and non-overlapping components}

However, with unbalanced classes and low \(\operatorname{OVL}\) (scenario U7 in
\ref{tab:parameter-configuration-univariate}), the choice of the
initialisation method is crucial, with quantiles and random
methods yielding more biased estimates and proned to fall in other local
maxima. Rebmix initialisation provides the best estimates, with the
smallest MSE and bias across packages (Figure
\ref{fig:four-component-unbalanced-separated} and Table
\ref{tab:unbalanced-well-separated-table-univariate-pdf}),
always associated with the highest likelihood. Overall, with
well-discriminated components, most of the differences on the estimation
originate from the choice of initialisation method, while the choice of
the package has only small impact.

We detail expensively in our bivariate simulations two scenarios featuring both strongly unbalanced and well-separated components, similarly to scenario U3 in Table \ref{tab:parameter-configuration-univariate}: the scenarios B12 (Figure \ref{fig:multivariate-overlapping-unbalanced-opposite-correlated} and Table \ref{tab:multivariate-overlapping-unbalanced-opposite-correlated-pdf}) and B14 (Figure \ref{fig:multivariate-overlapping-unbalanced-positive-correlated} and Table \ref{tab:multivariate-overlapping-unbalanced-positive-correlated-pdf}). Similarly, scenarios B16, B17 and B20 (Table \ref{tab:parameter-configuration-bivariate}) with similar characteristics are summarised in supplementary Figure \ref{fig:general-unbalanced-well-separated-bivariate}. For all these scenarios, neither the initialisation method nor the package have a statistical significant impact on the overall performance.

\color{blue}

Similarly, scenarios HD1a-HD4b in Table \ref{tab:parameter-configuration-HD}) in the high dimensional setting, display well-separated clusters, with a representative outcome represented in Supplementary Figure \ref{fig:HD-separated-unbalanced-ellipsoidal-plot} and Table \ref{tab:HD-separated-unbalanced-ellipsoidal-pdf}. Consistently with results from the corresponding univariate and bivariate scenarios, most of the previously benchmarked packages return sensibly the same estimates with the hierarchical clustering and \emph{k}-means initialisation. However, \pkg{bgmm} and \pkg{EMCluster} clearly differ by lower performances with the rebmix initialisation method (however, overall, rebmix performs poorly, regardless of the package used for estimation). Notably, initialisations with the rebmix package tend to display a much larger number of poor estimations, some of which can be identified with the local maxima associated with parameter switching between the two classes. Finally, the two additional packages dedicated to high-dimensional clustering display the worst performances, with \pkg{EMMIXmfa} returning the most biased parameters and \pkg{HDclassif} the most noisy estimates. \pkg{EMMIXmfa} is the only package that returned highly biased estimates of the components' proportions in this setting.
\color{black}

\textbf{Balanced and overlapping components}

When the overlap between components increases, the bias and variability of the estimates tends to increase while the choice of initialisation method becomes becomes valuable.
The least biased and noisy estimations with balanced components in the univariate setting (scenario U3 in Table
\ref{tab:parameter-configuration-univariate}) are obtained with
the \emph{k}-means initialisation (Figure
\ref{fig:four-component-balanced-overlapping} and Table
\ref{tab:balanced-overlapping-table-univariate-pdf}) while
the rebmix initialisation returns the most biased and noisy
estimates. Similar results are found in the bivariate setting with a balanced and highly overlapping two-components GMM (scenarios B1-B5 from Table \ref{tab:parameter-configuration-bivariate}), with the best performance reached with the \emph{k}-means initialisation method, followed by MBHC. This is emphasised in supplementary Figure \ref{fig:general-balanced-overlapping-bivariate}, in the top three most complex scenarios, namely B1, B2 and B5. If the shape of the covariance matrix is pretty well-recovered, no matter the package, the Hellinger distances are significantly higher (and thus the estimate further away from the target distribution) with the random and rebmix methods.

\color{blue}

Similarly, in the high-dimensional scenario HD7 of Table \ref{tab:parameter-configuration-HD}), presenting balanced but highly overlapping clusters with a full covariance structure, the best performances was obtained with \emph{k}-means initialisation, while the rebmix initialisation returns the most biased and noisy estimates. While \pkg{EMMIXmfa} performed well when it provided estimates, it returned an error in most cases (see Column \emph{Success} of Table \ref{tab:HD-overlapping-balanced-ellipsoidal-pdf}). The least biased estimates are returned by \pkg{mixtools} and \pkg{Rmixmod} and the least noisy by \pkg{flexmix}, \pkg{mclust} and \pkg{GMKMCharlie} (smaller MSE). Interestingly, in the high-dimensional setting, the packages \pkg{EMCluster} and \pkg{bgmm} differentiate from the other packages, exhibiting worse performance. In particular, on Panel E of Figure (fig:HD-overlapping-balanced-ellipsoidal-plot), the components' proportions span the \(]0-1[^k\) simplex.
On the contrary, the \pkg{EMCluster} package, and to a lesser extent, the \pkg{bgmm} package, perform surprisingly well when datasets were simulated with an underlying spherical covariance structure, even though the estimation was not performed explicitly with this constraint (Table \ref{tab:HD-overlapping-spherical-pdf}
). Indeed, it seems like that that the off-diagonal terms tend to converge towards 0, as showcased in Figure \ref{fig:HD-overlapping-spherical-plot}, in Panel C, in which the fourth row from top represents the bootstrap intervals associated to the pairwise covariance between dimension 1 and 2 of each cluster.\\
\color{black}

\textbf{Unbalanced and overlapping components}

With unbalanced components and high \(\operatorname{OVL}\) (scenario U9 in Table
\ref{tab:parameter-configuration-univariate}), all packages, no
matter the initialisation method, are biased, with a higher variability
of the parameter estimates compared to other scenarios. The least biased
estimates are obtained with \emph{k}-means or random initialisation, but with
a higher variability on the estimates with random initialisation (Table
\ref{tab:unbalanced-overlapping-table-univariate-pdf}).
However, deepening the analysis to the individual components' parameter estimates
reveals that the least unbiased estimate is obtained with rebmix initialisation
for the two most distinguishable components, namely clusters 2 and 4 (Figure \ref{fig:four-component-unbalanced-overlapping} and
Table
\ref{tab:unbalanced-overlapping-table-univariate-pdf}), consistently with this method's assumption to use modes to initialize the components (Nagode 2015). With highly-overlapping distributions and unbalanced components, both the choice of the initialisation algorithm and the package have a
substantial impact on the quality of the estimation of this mixture.

Two scenarios in our bivariate simulation feature distributions with both strong OVL and unbalanced components. Especially, the scenario B11 (Table \ref{tab:parameter-configuration-bivariate}) has the strongest OVL overall, with notably a risk of wrongly assigning minor component 2 to major component 1 of 0.5 (a random method classifying each observation to cluster 1 or 2 would have the same performance).

First, we observe that the the random and rebmix initialisation methods have similar performance, significantly better than \(k\)-means or MBHC (Figure \ref{fig:multivariate-overlapping-unbalanced-negative-correlated}). Specifically, the rebmix method returns the least biased estimates, while the random method is associated with the lowest MSE (respectively for scenarios B11 and B15, the Tables \ref{tab:multivariate-overlapping-unbalanced-negative-correlated-pdf}
and \ref{tab:multivariate-overlapping-unbalanced-uncorrelated-pdf}). Second, the estimates differ across packages only in these two complex scenarios, with packages \pkg{Rmixmod} and \pkg{mixtools} returning more accurate estimates than the others.
It it is particularly emphasised in Scenario B15, where the component-specific covariance matrices are diagonal with same non-null input, and thus should present spherical density distributions. However, only the first class of packages correctly recovers the spherical bivariate \(95\%\) confidence regions while they are slightly ellipsoidal with the second class of packages (Panel B, Figure \ref{fig:multivariate-overlapping-unbalanced-uncorrelated}).

\color{blue}

With full covariance structures and unbalanced proportions, as depicted in the high-dimensional Scenario HD8a) and b) of Table \ref{tab:parameter-configuration-HD}, the general observations stated in the previous subsection for the high dimensional setting hold, namely that the least biased estimates are returned by packages not specifically designed for high-dimensional data, with the \emph{k}-means initialisation (Table @ref(tab:HD-impact-num-observations-pdf-tab2 and Figure \ref{fig:HD-impact-num-observations}). Furthermore, the pkg\{EMCluster\} and \pkg{bgmm} packages and the two packages dedicated to high-dimensional, perform similarly with \(n=200\) observations (sub-scenario a) and \(n=2000\) observations (sub-scenario b), whereas we would expect narrower and less biased confidence intervals by increasing the number of observations by a factor of 10.

Finally, with spherical covariance structures and unbalanced proportions, the best performances, both in terms of bias and variability, are obtained with \pkg{flexmix}, \pkg{mclust} and \pkg{GMKMCharlie}. Indeed, as detailed later in \protect\hyperlink{conclusions}{Conclusions}, these packages are more sensitive to the choice of the initialisation method and have a greater tendency to get trapped in the neighbourhood of the initial estimates (Table \ref{tab:HD-overlapping-spherical-pdf} and Figure \ref{fig:HD-overlapping-spherical-plot}). Accordingly, \emph{k}-means initialisation performs best since it assumes independent and homoscedastic features for each cluster.
Furthermore, \pkg{EMMIXmfa} is the package that best estimates the off-diagonal terms in this setting, as highlighted in Table \ref{tab:HD-overlapping-spherical-pdf-offterms}.
\color{black}

\textbf{Identification of two classes of packages with distinct behaviours}

By summarizing the results obtained across all simulations, we identify two classes of packages with distinct behaviours (Figure
\ref{fig:dichotomy-package-conclusion}):

\begin{itemize}
\item
  The first class of packages, represented by \pkg{Rmixmod} and \pkg{mixtools}, returns similar estimates to our baseline EM implementation. The estimates returned by these packages are less biased but at the extent of a higher variability on the estimates. Additionally, with overlapping mixtures, they tend to be slower compared to the second class, since they require additional steps to reach convergence.
\item
  The second class of packages, composed of the other
  reviewed packages, is more sensitive to the initialisation method. This leads to more
  biased but less variable estimates, especially when assumptions done by
  the initialisation algorithm are not met.
\end{itemize}

Panels A, B and C display, respectively in the univariate, bivariate and high-dimensional setting, the heatmap of the Pearson correlation between the estimated parameters across the benchmarked packages for the most discriminative scenario (the one featuring the most unbalanced and overlapping components): scenario U9, Table \ref{tab:parameter-configuration-univariate} in the univariate setting, scenario B11, Table \ref{tab:parameter-configuration-bivariate}, for the bivariate simulation and scenario HD8, Table \ref{tab:parameter-configuration-HD} for the high-dimensional simulation, with the \emph{k}-means initialisation.

We further identified with this representation minor differences for the estimation of the parameters between \pkg{Rmixmod} and \pkg{mixtools}, while three subgroups can be identified in the second class of packages: the first subset with \pkg{bgmm} and \pkg{mclust}, the second subset with \pkg{EMCluster} and \pkg{GMKMcharlie} packages and the \pkg{flexmix} package, which clearly stands out from the others, as being the one most likely to be trapped at the boundaries of the parameter space. After examining the source codes of the packages, we attribute this differences to custom implementation choices of the EM algorithm, such as the way numerical underflow is managed or the choice of a relative or absolute scale to compare consecutive computed log-likelihoods (see \protect\hyperlink{em-implementation-differences-across-reviewed-packages}{EM-implementation differences across reviewed packages} and Panel D, Figure \ref{fig:dichotomy-package-conclusion}). \color{blue} In the high-dimensional setting, the second class of packages showed additional heterogeneity, with \pkg{EMCluster} and \pkg{bgmm} setting themselves apart from the other three packages.\color{black}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/dichotomy_package_conclusion} 

}

\caption{Panels A, B and C show respectively the heatmap of the Pearson correlation in the univariate, bivariate and high-dimensional framework between the parameters estimated by the packages, evaluated for the most discriminating and complex scenario. The correlation matrix was computed using the function \code{stats::cor} with option \textit{complete} to remove any missing value related to a failed simulation, and the heatmap generated with the Bioconductor package \BIOpkg{ComplexHeatmap}. Panel D represents a tree summarising the main differences between the benchmarked packages, in terms of the EM implementation. They are discussed in more detail in Appendix \nameref{sec:em-differences}.}\label{fig:dichotomy-package-conclusion}
\end{figure}

\textbf{Failed estimations}

Finally, in some cases, neither the specific EM algorithm implemented by each package nor the initialisation method were able to return an estimate with the expected number of components, or even worse fell into a degenerate case (e.g., with infinite or zero variances). In that case, we considered the estimation failed and accordingly we did not include it into the visualizations and the summary metric tables. Most of the failed estimations occurred with the rebmix initialisation. Indeed, an updated version of the package forces the user to provide a set of possible positive integer values for the number of components, with at least a difference of two between the model with the most components and the model with the least components (we therefore set the parameter \emph{cmax} to \$k+1 and \emph{cmin} to \(k-1\)). In the most complex scenarios, this constraint leads to an increased risk of returning an overestimation or an underestimation of the actual number of components.

In most cases, this phenomenon occurs with mixtures featuring both strongly unbalanced and highly-overlapping components, with up to \(20\%\) initialisations failed for the most difficult scenario B11 (Table \ref{tab:parameter-configuration-bivariate}), \(10\%\) for the second most difficult one, namely scenario B15 and on average, an over-estimation of three components in \(4\%\) of the estimations.

Removing errors proceeding from the initialisation method, only the \pkg{flexmix} package failed in returning an estimate matching the user criteria in some scenarios of the univariate and bivariate settings. In both cases, the strong assumption that any cluster with less than \(5\%\) of the observations is irrelevant, results in trimming one or more components\footnote{With a two-components mixture like our bivariate scenario, this even implies to consider an unimodal distribution of the dataset}. This strong agnostic constraint on component proportions led to failures in scenarios featuring strongly overlapping clusters, with up to \(20\%\) failed estimations with the random initialisation method in scenario B11 (Table \ref{tab:parameter-configuration-bivariate}) and \(80\%\) failed estimations in the univariate setting\footnote{the gap proceeds from the stronger level of imbalance and the greater number of components} with the rebmix initialisation with scenario U9, Table \ref{tab:parameter-configuration-univariate}.

\color{blue}

In a relatively high dimensional framework, as tested on our third benchmark (Table \ref{tab:parameter-configuration-HD}), none of the initialisations performed with the random method, carried out with \code{EMCluster::rand.EM()}, succeed in returning a valid parametrisation. Indeed, over the 16 scenarios tested, the covariance returned during the initialisation was systematically non-positive definite for at least one of the components, violating the properties of covariance matrices.
Furthermore, as shown by the comparison of summary metrics with \(n=200\) and \(n=2000\) observations in Tables \ref{tab:HD-impact-num-observations-pdf-tab1}
and \ref{tab:HD-impact-num-observations-pdf-tab2}, respectively for the simplest scenario HD1 and the most complex one HD8, the rebmix initialisation on the one hand, and the packages dedicated to high dimensionality or those of the second class of packages that show a particular behaviour, present much more failures than the \emph{k}-means or hierarchical clustering initialisation, especially with the first class or the other packages of the second class.
\color{black}

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

There is a wide diversity of packages that implement the EM algorithm to
retrieve the parameters of GMMs. But only few are regularly updated,
implement both the unconstrained univariate and multivariate GMM while enabling the user to provide
its own initial estimates. Hence, among the 54 packages dealing
with GMMs available on CRAN or Bioconductor repositories, we focused our
review on 7 packages implementing all these features. We believe that our in-depth review of the packages can help users to quickly find the best package for their clustering
pipeline and highlight limitations in the implementation of some
packages. Our benchmark covers a much broader range of scenarios
than the previously-published studies (Nityasuddhi and Böhning 2003; Lourens et al. 2013; Leytham 1984; D. Xu and Knight 2010), as we studied the impact of the
level of overlap and the imbalance of the mixture proportions on the
quality of the estimation.

Interestingly, the EM algorithm sometimes tends to returns biased and
inefficient estimators when the components of the GMM under study highly overlap, confirming the experimental observations reported in papers (Lourens et al. 2013; Leytham 1984; D. Xu and Knight 2010). This seems to contradict the statement from Leytham (1984) that shows the theoretical asymptotic consistency, unbiasedness and efficiency of the MLE of GMMs. However, the key assumption of the demonstration is the definition of a local environment, implying to define borders for which the conditions of the theorem hold. Thus, it can expected that the EM algorithm sometimes fail to reach the global maximum of the distribution, with multiple local extremes.

When all components are well-separated or with a relatively small number of components (three or fewer), we found that the best
estimation (lowest MSE and bias) is reached with the latest initialisation method developed, namely the rebmix one. Notably, the global maximum is always properly found in our simulations with distinguishable components. Yet, with overlapping components, the least biased and variable estimates overall are obtained with \emph{k}-means initialisation, enforcing the robustness of the method while the assumptions for using it are not met. On the contrary, with unbalanced and numerous components (above three), the quantiles initialisation leads to the most biased estimates while the rebmix initialisation induces the highest variability. Indeed, rebmix initialisation is not fitted for highly overlapping mixtures, since one of its most restrictive assumption is that each generated interval of the empirical mixture distribution can be associated unambiguously to a component (see \protect\hyperlink{initialisation-of-the-em-algorithm}{Initialisation of the EM algorithm} and p15 in Nagode (2015)). \color{blue} Furthermore, rebmix is not particularly adjusted to deal with high-dimensionality, displaying systematically poorer performance compared to other initialisation strategies, such as \emph{k}-means or hierarchical clustering, as illustrated by the summary metrics listed in Appendix \protect\hyperlink{supplementary-figures-and-tables-in-the-hd-simulation}{Supplementary Figures and Tables in the HD simulation}. Higher risk of returning a sub-optimal extremum likely arises from the increased data sparsity in high dimensional datasets, which grows as the square root of the number of dimensions \(\sqrt{D}\) \href{https://en.wikipedia.org/wiki/Curse_of_dimensionality\#Distance_function}{Convergence of distance definitions}. Thus, we expect that most of the equally-large intervals binning the sampling space and that are used to initiate the rebmix algorithm contain either no or only observation, preventing from retrieving the numerically defined mode of the distribution and increasing the risk of initiating the algorithm in a spurious neighbourhood. \color{black}

About the remaining initialisation strategies, we observed that random initialisation yields for some simulations, even in the well-separated case, highly biased estimates, far from the true
parameters. Consistent with our observations, it was shown in
Jin et al. (2016) that the probability for the EM algorithm to converge from
randomly initialised estimates to a local suboptimal maximum is non null above two
components, increasing with the number of components. Additionally, the
local maximum of the likelihood function obtained can be arbitrarily
worse than the global maximum. Finally, hierarchical clustering does not
take into account any uncertainty on the assignment for an observation
to a given class, which explains its rather bad performances with
overlapping components. Overall, there is always an initialisation
algorithm performing better than the hierarchical clustering, whereas it
is by far the slowest and most computationally intensive initialisation
method, as best illustrated in supplementary Figure \ref{fig:univariate-initialisation-time-computations}.

Our study reveals differences in the estimates obtained across different packages while the EM algorithm is supposed to be deterministic. We were able to link these differences with custom choices of EM implementations across the benchmarked packages.
Specifically, two classes set apart, with distinct choices to deal with some limitations of the EM algorithm: the first
one, represented by \pkg{mixtools} and \pkg{Rmixmod}, tend to provide
smaller biased estimates, less sensitive to the choice of initialisation
method but with higher variability and longer running times required to reach the convergence. The second one, composed of the remaining
packages, provide estimates with reduced MSE, but at the extent of a
higher bias on the estimates. One plausible explanation is that the first class of packages, comparing absolute iterations of the function to be maximised, tends on average to perform more iterations. The estimated results are accordingly more consistent and closer to the true MLE estimation but at the expense of an increased risk of getting trapped in a local extremum or a plateau, explaining the greater number of outliers observed. Among them, \pkg{flexmix} stands out by choosing
an unbiased but non MLE-estimate of the covariance matrix, without any clear improvement
of the overall performance in our simulations.

Based on these results, we design a decision tree indicating the best choice of package and initialisation method in relation with the shape of the distribution, displayed in Figure \ref{fig:decision-tree-GMMs}. Interestingly, our conclusions are consistent between the univariate and bivariate settings. \color{blue} Furthermore, most of the general recommendations on the best choices of packages with respect to the characteristics of the mixture model generally hold in a relatively higher dimensional setting\footnote{We should note, however, that a larger sample space revealed that the packages \textit{bgmm} and \textit{EMCluster} display more biased and noisy parameters compared to the other packages benchmarked and that their performance was surprisingly unaffected by the number of simulated realisations}. From this, we assume that projection into a lower-dimensional space is only beneficial in a really high-dimensional setting, for example when the number of dimensions exceeds the number of observations, or when unrestricted parameter estimation (with the full covariance structure) is practically infeasible for computational reasons.
\color{black}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/decision_tree_GMMs} 

}

\caption{A decision tree to select the best combination of package and initialisation method with respect to the main characteristics of the mixture. It's worth pointing that in both univariate and low dimension multivariate settings, the recommandations are similar.}\label{fig:decision-tree-GMMs}
\end{figure}

Comparing all these packages suggest several improvements.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The use of C++ code speeds up the convergence of the EM algorithm
  compared to a full R implementation.
\item
  All packages dealing with GMMs should use \emph{k}-means for overlapping, complex mixtures and rebmix initialisation for well-separated components, provided that the dimension of the dataset is relatively small. The final choice between these two could be set after a first run or visual inspection aiming at determining roughly the level of entropy across mixture proportions and the degree of overlap between components.
\item
  The packages should allow the user to set their own termination criteria (either relative or absolute log-likelihood or over the estimates after normalisation). Interestingly, \pkg{EMMIXmfa} is the only package among those examined that allows the user to consider an absolute or relative convergence endpoint of the EM algorithm, through the \texttt{conv\_measure} attribute, with \texttt{diff} and \texttt{ratio} options respectively.
\item
  With a great number of components or complex overlapping distributions, the optimal package should integrate partial information when available or a Bayesian estimation of the
  estimates.
\end{enumerate}

While \pkg{mclust} appeared as the most complete package to model GMMs in R, none of the packages reviewed in this report features all the characteristics mentioned above.
We thus strongly believe that our observations will help users identify the most suitable packages and parameters for their analyses and guide the development or updates of future packages.

\hypertarget{bibliography}{%
\section{Bibliography}\label{bibliography}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-alberich-carraminana_etal17}{}}%
Alberich-Carramiñana, Maria, Borja Elizalde, and Federico Thomas. 2017. {``New Algebraic Conditions for the Identification of the Relative Position of Two Coplanar Ellipses.''} \emph{Computer Aided Geometric Design}. \url{https://doi.org/10.1016/j.cagd.2017.03.013}.

\leavevmode\vadjust pre{\hypertarget{ref-R-DPP}{}}%
Avila, Luis M., Michael R. May, and Jeff Ross-Ibarra. 2018. \emph{DPP: Inference of Parameters of Normal Distributions from a Mixture of Normals}.

\leavevmode\vadjust pre{\hypertarget{ref-bacci_etal12}{}}%
Bacci, Silvia, Silvia Pandolfi, and Fulvia Pennoni. 2012. {``A Comparison of Some Criteria for States Selection in the Latent {Markov} Model for Longitudinal Data.''} \url{http://arxiv.org/abs/1212.0352}.

\leavevmode\vadjust pre{\hypertarget{ref-baek_etal10}{}}%
Baek, Jangsun, Geoffrey J. McLachlan, and Lloyd K. Flack. 2010. {``Mixtures of {Factor} {Analyzers} with {Common} {Factor} {Loadings}: {Applications} to the {Clustering} and {Visualization} of {High}-{Dimensional} {Data}.''} \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}. \url{https://doi.org/10.1109/TPAMI.2009.149}.

\leavevmode\vadjust pre{\hypertarget{ref-R-polySegratioMM}{}}%
Baker, Peter. 2018. \emph{polySegratioMM: Bayesian Mixture Models for Marker Dosage in Autopolyploids}.

\leavevmode\vadjust pre{\hypertarget{ref-banfield_raftery93}{}}%
Banfield, Jeffrey D., and Adrian E. Raftery. 1993. {``Model-{Based Gaussian} and {Non-Gaussian Clustering}.''} \emph{Biometrics}. \url{https://doi.org/10.2307/2532201}.

\leavevmode\vadjust pre{\hypertarget{ref-basford_etal97}{}}%
Basford, K., D. Greenway, G. Mclachlan, et al. 1997. {``Standard Errors of Fitted Component Means of Normal Mixtures.''} \emph{Computational Statistics}. \url{https://www.researchgate.net/publication/37625647/_Standard/_errors/_of/_fitted/_component/_means/_of/_normal/_mixtures}.

\leavevmode\vadjust pre{\hypertarget{ref-mixtools2009}{}}%
Benaglia, Tatiana, Didier Chauveau, David R. Hunter, et al. 2009. {``{mixtools}: An {R} Package for Analyzing Finite Mixture Models.''} \emph{Journal of Statistical Software}.

\leavevmode\vadjust pre{\hypertarget{ref-R-HDclassif}{}}%
Berge, Laurent, Charles Bouveyron, and Stephane Girard. 2019. \emph{HDclassif: High Dimensional Supervised Classification and Clustering}.

\leavevmode\vadjust pre{\hypertarget{ref-berge_etal12}{}}%
Bergé, Laurent, Charles Bouveyron, and Stéphane Girard. 2012. {``{HDclassif}: {An} {R} {Package} for {Model}-{Based} {Clustering} and {Discriminant} {Analysis} of {High}-{Dimensional} {Data}.''} \emph{Journal of Statistical Software}. \url{https://doi.org/10.18637/jss.v046.i06}.

\leavevmode\vadjust pre{\hypertarget{ref-biernacki_etal00}{}}%
Biernacki, Christophe, Gilles Celeux, and Gerard Govaert. 2000. {``Assessing a {Mixture Model} for {Clustering} with the {Integrated Completed Likelihood}.''} \emph{Pattern Analysis and Machine Intelligence, IEEE Transactions on}. \url{https://doi.org/10.1109/34.865189}.

\leavevmode\vadjust pre{\hypertarget{ref-biernacki_etal03}{}}%
Biernacki, Christophe, Gilles Celeux, and Gérard Govaert. 2003. {``Choosing Starting Values for the {EM} Algorithm for Getting the Highest Likelihood in Multivariate {Gaussian} Mixture Models.''} \emph{Computational Statistics \& Data Analysis}. \url{https://doi.org/10.1016/S0167-9473(02)00163-9}.

\leavevmode\vadjust pre{\hypertarget{ref-R-turboEM}{}}%
Bobb, Jennifer F., and Ravi Varadhan. 2021. \emph{turboEM: A Suite of Convergence Acceleration Schemes for EM, MM and Other Fixed-Point Algorithms}.

\leavevmode\vadjust pre{\hypertarget{ref-bouveyron_etal11}{}}%
Bouveyron, Charles, Gilles Celeux, and Stéphane Girard. 2011. {``Intrinsic {Dimension Estimation} by {Maximum Likelihood} in {Isotropic Probabilistic PCA}.''} \emph{Pattern Recognition Letters}. \url{https://doi.org/10.1016/j.patrec.2011.07.017}.

\leavevmode\vadjust pre{\hypertarget{ref-bouveyron_girard09}{}}%
Bouveyron, Charles, and Stéphane Girard. 2009. {``{Robust Supervised Classification} with {Mixture Models}: {Learning} from {Data} with {Uncertain Labels}.''} \emph{Pattern Recognition}. \url{https://doi.org/10.1016/j.patcog.2009.03.027}.

\leavevmode\vadjust pre{\hypertarget{ref-bouveyron_etal07}{}}%
Bouveyron, Charles, Stéphane Girard, and Cordelia SCHMID. 2007. {``High-{Dimensional} {Data} {Clustering}.''} \emph{Computational Statistics \& Data Analysis}. \url{https://doi.org/10.1016/j.csda.2007.02.009}.

\leavevmode\vadjust pre{\hypertarget{ref-browne_mcnicholas14}{}}%
Browne, Ryan P., and Paul D. McNicholas. 2014. {``Estimating Common Principal Components in High Dimensions.''} \emph{Advances in Data Analysis and Classification}. \url{https://doi.org/10.1007/s11634-013-0139-1}.

\leavevmode\vadjust pre{\hypertarget{ref-R-RobMixReg}{}}%
Cao, Sha, Wennan Chang, and Chi Zhang. 2020. \emph{RobMixReg: Robust Mixture Regression}.

\leavevmode\vadjust pre{\hypertarget{ref-cao_spall12}{}}%
Cao, X., and J. C. Spall. 2012. {``Relative Performance of Expected and Observed Fisher Information in Covariance Estimation for Maximum Likelihood Estimates.''} \url{https://doi.org/10.1109/ACC.2012.6315584}.

\leavevmode\vadjust pre{\hypertarget{ref-cattell66}{}}%
Cattell, Raymond B. 1966. {``The {Scree} {Test} {For} {The} {Number} {Of} {Factors}.''} \emph{Multivariate Behavioral Research}. \url{https://doi.org/10.1207/s15327906mbr0102/_10}.

\leavevmode\vadjust pre{\hypertarget{ref-celeux_etal12}{}}%
Celeux, Gilles, Stéphane Chrétien, and Florence Forbes. 2012. {``A {Component-wise EM Algorithm} for {Mixtures}.''}

\leavevmode\vadjust pre{\hypertarget{ref-celeux_etal18}{}}%
Celeux, Gilles, Sylvia Fruewirth-Schnatter, and Christian P. Robert. 2018. {``Model Selection for Mixture Models - Perspectives and Strategies.''} arXiv. \url{https://doi.org/10.48550/ARXIV.1812.09885}.

\leavevmode\vadjust pre{\hypertarget{ref-celeux_govaert92}{}}%
Celeux, Gilles, and Gérard Govaert. 1992. {``A Classification {EM} Algorithm for Clustering and Two Stochastic Versions.''} \emph{Computational Statistics \& Data Analysis}. \url{https://doi.org/10.1016/0167-9473(92)90042-E}.

\leavevmode\vadjust pre{\hypertarget{ref-chen_chen08}{}}%
Chen, J., and Z. Chen. 2008. {``Extended {Bayesian} Information Criteria for Model Selection with Large Model Spaces.''} \emph{Biometrika}. \url{https://doi.org/10.1093/biomet/asn034}.

\leavevmode\vadjust pre{\hypertarget{ref-chen16}{}}%
Chen, Jiahua. 2016. {``Consistency of the {MLE} Under Mixture Models.''} \url{https://doi.org/10.1214/16-STS578}.

\leavevmode\vadjust pre{\hypertarget{ref-R-EMCluster}{}}%
Chen, Wei-Chen, and Ranjan Maitra. 2022. \emph{EMCluster: EM Algorithm for Model-Based Clustering of Finite Mixture Gaussian Distribution}.

\leavevmode\vadjust pre{\hypertarget{ref-R-pmclust}{}}%
Chen, Wei-Chen, and George Ostrouchov. 2021. \emph{Pmclust: Parallel Model-Based Clustering Using Expectation-Gathering-Maximization Algorithm for Finite Mixture Gaussian Model}.

\leavevmode\vadjust pre{\hypertarget{ref-R-oclust}{}}%
Clark, Katharine M., and Paul D. McNicholas. 2019. \emph{Oclust: Gaussian Model-Based Clustering with Outliers}.

\leavevmode\vadjust pre{\hypertarget{ref-cochran34}{}}%
Cochran, W. G. 1934. {``The Distribution of Quadratic Forms in a Normal System, with Applications to the Analysis of Covariance.''} \emph{Mathematical Proceedings of the Cambridge Philosophical Society}. \url{https://doi.org/10.1017/S0305004100016595}.

\leavevmode\vadjust pre{\hypertarget{ref-otrimle2016b}{}}%
Coretto, Pietro, and Christian Hennig. 2016. {``Consistency, Breakdown Robustness, and Algorithms for Robust Improper Maximum Likelihood Clustering.''} \emph{Journal of Machine Learning Research}.

\leavevmode\vadjust pre{\hypertarget{ref-R-otrimle}{}}%
---------. 2021. \emph{Otrimle: Robust Model-Based Clustering}.

\leavevmode\vadjust pre{\hypertarget{ref-dai_mukherjea01}{}}%
Dai, Ming, and Arunava Mukherjea. 2001. {``Identification of the {Parameters} of a {Multivariate Normal Vector} by the {Distribution} of the {Maximum}.''} \emph{Journal of Theoretical Probability}. \url{https://doi.org/10.1023/A:1007889519309}.

\leavevmode\vadjust pre{\hypertarget{ref-R-clustvarsel}{}}%
Dean, Nema, Adrian E. Raftery, and Luca Scrucca. 2020. \emph{Clustvarsel: Variable Selection for Gaussian Model-Based Clustering}.

\leavevmode\vadjust pre{\hypertarget{ref-delattre_kuhn19}{}}%
Delattre, Maud, and Estelle Kuhn. 2019. {``Estimating {Fisher Information Matrix} in {Latent Variable Models} Based on the {Score Function}.''} https://doi.org/\url{https://doi.org/10.48550/arXiv.1909.06094}.

\leavevmode\vadjust pre{\hypertarget{ref-dempster_etal77}{}}%
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. {``Maximum {Likelihood} from {Incomplete Data Via} the {\emph{EM}} {Algorithm}.''} \emph{Journal of the Royal Statistical Society}. \url{https://doi.org/10.1111/j.2517-6161.1977.tb01600.x}.

\leavevmode\vadjust pre{\hypertarget{ref-efron_tibshirani93}{}}%
Efron, Bradley, and Robert Tibshirani. 1993. \emph{An {Introduction} to the {Bootstrap}}.

\leavevmode\vadjust pre{\hypertarget{ref-R-bgmm}{}}%
Ewa Szczurek, Przemyslaw Biecek \&. 2021. \emph{Bgmm: Gaussian Mixture Modeling Algorithms and the Belief-Based Mixture Modeling}.

\leavevmode\vadjust pre{\hypertarget{ref-R-pcensmix}{}}%
Fallah, Lida, and John Hinde. 2017. \emph{Pcensmix: Model Fitting to Progressively Censored Mixture Data}.

\leavevmode\vadjust pre{\hypertarget{ref-figueiredo_jain02}{}}%
Figueiredo, M. A. T., and A. K. Jain. 2002. {``Unsupervised Learning of Finite Mixture Models.''} \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}. \url{https://doi.org/10.1109/34.990138}.

\leavevmode\vadjust pre{\hypertarget{ref-fonseca08}{}}%
Fonseca, Jaime. 2008. {``The Application of Mixture Modeling and Information Criteria for Discovering Patterns of Coronary Heart Disease.''} \emph{Journal of Applied Quantitative Methods}. \url{https://doi.org/10.1109/EMS.2008.36}.

\leavevmode\vadjust pre{\hypertarget{ref-fraley98}{}}%
Fraley, Chris. 1998. {``Algorithms for {Model-Based Gaussian Hierarchical Clustering}.''} \emph{SIAM Journal on Scientific Computing}. \url{https://doi.org/10.1137/S1064827596311451}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mclust}{}}%
Fraley, Chris, Adrian E. Raftery, and Luca Scrucca. 2022. \emph{Mclust: Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation}.

\leavevmode\vadjust pre{\hypertarget{ref-R-sensory}{}}%
Franczak, Brian C., Ryan P. Browne, and Paul D. McNicholas. 2016. \emph{Sensory: Simultaneous Model-Based Clustering and Imputation via a Progressive Expectation-Maximization Algorithm}.

\leavevmode\vadjust pre{\hypertarget{ref-gallegos_ritter05}{}}%
Gallegos, María Teresa, and Gunter Ritter. 2005. {``A Robust Method for Cluster Analysis.''} \emph{The Annals of Statistics}. \url{https://doi.org/10.1214/009053604000000940}.

\leavevmode\vadjust pre{\hypertarget{ref-R-SMNCensReg}{}}%
Garay, Aldo M., Monique Bettio Massuia, and Victor Lachos. 2022. \emph{SMNCensReg: Fitting Univariate Censored Regression Model Under the Family of Scale Mixture of Normal Distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-BayesCR}{}}%
Garay, Aldo M., Monique B. Massuia, Victor H. Lachos, et al. 2017. \emph{BayesCR: Bayesian Analysis of Censored Regression Models Under Scale Mixture of Skew Normal Distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-garcia-escudero_etal08}{}}%
García-Escudero, Luis A., Alfonso Gordaliza, Carlos Matrán, et al. 2008. {``A General Trimming Approach to Robust Cluster Analysis.''} \emph{The Annals of Statistics}. \url{https://doi.org/10.1214/07-AOS515}.

\leavevmode\vadjust pre{\hypertarget{ref-geary36}{}}%
Geary, R. C. 1936. {``The {Distribution} of "{Student}'s" {Ratio} for {Non-Normal Samples}.''} \emph{Supplement to the Journal of the Royal Statistical Society}. \url{https://doi.org/10.2307/2983669}.

\leavevmode\vadjust pre{\hypertarget{ref-R-flexmix}{}}%
Gruen, Bettina, and Friedrich Leisch. 2022. \emph{Flexmix: Flexible Mixture Modeling}.

\leavevmode\vadjust pre{\hypertarget{ref-he_chen16}{}}%
He, Yawei, and Zehua Chen. 2016. {``The {EBIC} and a Sequential Procedure for Feature Selection in Interactive Linear Models with High-Dimensional Data.''} \emph{Annals of the Institute of Statistical Mathematics}. \url{https://doi.org/10.1007/s10463-014-0497-2}.

\leavevmode\vadjust pre{\hypertarget{ref-R-RPMM}{}}%
Houseman, E. Andres, Sc.D., Devin C. Koestler, et al. 2017. \emph{RPMM: Recursively Partitioned Mixture Model}.

\leavevmode\vadjust pre{\hypertarget{ref-R-MixAll}{}}%
Iovleff, Serge. 2019. \emph{MixAll: Clustering and Classification Using Model-Based Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-tclust}{}}%
Iscar, Agustin Mayo, Luis Angel Garcia Escudero, and Heinrich Fritz. 2022. \emph{Tclust: Robust Trimmed Clustering}.

\leavevmode\vadjust pre{\hypertarget{ref-jaki_etal18}{}}%
Jaki, Thomas, Ting-Li Su, Minjung Kim, et al. 2018. {``An Evaluation of the Bootstrap for Model Validation in Mixture Models.''} \emph{Communications in Statistics: Simulation and Computation}. \url{https://doi.org/10.1080/03610918.2017.1303726}.

\leavevmode\vadjust pre{\hypertarget{ref-jin_etal16}{}}%
Jin, Chi, Yuchen Zhang, Sivaraman Balakrishnan, et al. 2016. {``Local {Maxima} in the {Likelihood} of {Gaussian Mixture Models}: {Structural Results} and {Algorithmic Consequences}.''} Curran Associates, Inc. https://doi.org/\url{https://doi.org/10.48550/arXiv.1609.00978}.

\leavevmode\vadjust pre{\hypertarget{ref-R-EMMIXgene}{}}%
Jones, Andrew Thomas. 2020. \emph{EMMIXgene: A Mixture Model-Based Approach to the Clustering of Microarray Expression Data}.

\leavevmode\vadjust pre{\hypertarget{ref-R-SAGMM}{}}%
Jones, Andrew T., and Hien D. Nguyen. 2019. \emph{SAGMM: Clustering via Stochastic Approximation and Gaussian Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-Melissa}{}}%
Kapourani, C. A. 2022. \emph{Melissa: Bayesian Clustering and Imputationa of Single Cell Methylomes}.

\leavevmode\vadjust pre{\hypertarget{ref-R-epigenomix}{}}%
Klein, Hans-Ulrich, and Martin Schaefer. 2022. \emph{Epigenomix: Epigenetic and Gene Transcription Data Normalization and Integration with Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixAK}{}}%
Komárek, Arnošt. 2022. \emph{mixAK: Multivariate Normal Mixture Models and Mixtures of Generalized Linear Mixed Models Including Model Based Clustering}.

\leavevmode\vadjust pre{\hypertarget{ref-koopman36}{}}%
Koopman, B. O. 1936. {``On {Distributions Admitting} a {Sufficient Statistic}.''} \emph{Transactions of the American Mathematical Society}. \url{https://doi.org/10.2307/1989758}.

\leavevmode\vadjust pre{\hypertarget{ref-R-RMixtComp}{}}%
Kubicki, Vincent, Christophe Biernacki, and Quentin Grimonprez. 2021. \emph{RMixtComp: Mixture Models with Heterogeneous and (Partially) Missing Data}.

\leavevmode\vadjust pre{\hypertarget{ref-kurban_etal17}{}}%
Kurban, Hasan, Mark Jenne, and Mehmet M. Dalkilic. 2017. {``Using Data to Build a Better {EM}: {EM}\textasteriskcentered for Big Data.''} \emph{International Journal of Data Science and Analytics}. \url{https://doi.org/10.1007/s41060-017-0062-1}.

\leavevmode\vadjust pre{\hypertarget{ref-kwedlo13}{}}%
Kwedlo, Wojciech. 2013. {``A {New Method} for {Random Initialization} of the {EM Algorithm} for {Multivariate Gaussian Mixture Learning}.''} In \emph{Proceedings of the {8th International Conference} on {Computer Recognition Systems CORES 2013}}, edited by Robert Burduk, Konrad Jackowski, Marek Kurzynski, Michał Wozniak, and Andrzej Zolnierek. Springer International Publishing. \url{https://doi.org/10.1007/978-3-319-00969-8/_8}.

\leavevmode\vadjust pre{\hypertarget{ref-R-Rmixmod}{}}%
Langrognet, Florent, Remi Lebret, Christian Poli, et al. 2021. \emph{Rmixmod: Classification with Mixture Modelling}.

\leavevmode\vadjust pre{\hypertarget{ref-leytham84}{}}%
Leytham, K. M. 1984. {``Maximum {Likelihood Estimates} for the {Parameters} of {Mixture Distributions}.''} \emph{Water Resources Research}. \url{https://doi.org/10.1029/WR020i007p00896}.

\leavevmode\vadjust pre{\hypertarget{ref-li91}{}}%
Li, Ker-Chau. 1991. {``Sliced {Inverse} {Regression} for {Dimension} {Reduction}.''} \emph{Journal of the American Statistical Association}. \url{https://doi.org/10.2307/2290563}.

\leavevmode\vadjust pre{\hypertarget{ref-R-MixtureInf}{}}%
Li, Shaoting, Jiahua Chen, and Pengfei Li. 2016. \emph{MixtureInf: Inference for Finite Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-MMDvariance}{}}%
Li, Xuan, Yuejiao Fu, Xiaogang Wang, et al. 2018. \emph{MMDvariance: Detecting Differentially Variable Genes Using the Mixture of Marginal Distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-fmerPack}{}}%
Li, Yan, and Kun Chen. 2021. \emph{fmerPack: Tools of Heterogeneity Pursuit via Finite Mixture Effects Model}.

\leavevmode\vadjust pre{\hypertarget{ref-lindsay95}{}}%
Lindsay, Bruce G. 1995. {``Mixture {Models}: {Theory}, {Geometry} and {Applications}.''} \emph{NSF-CBMS Regional Conference Series in Probability and Statistics}. \url{https://www.jstor.org/stable/4153184}.

\leavevmode\vadjust pre{\hypertarget{ref-R-GMKMcharlie}{}}%
Liu, Charlie Wusuo. 2021. \emph{GMKMcharlie: Unsupervised Gaussian Mixture and Minkowski and Spherical k-Means with Constraints}.

\leavevmode\vadjust pre{\hypertarget{ref-louis82}{}}%
Louis, Thomas A. 1982. {``Finding the {Observed Information Matrix} When {Using} the {EM Algorithm}.''} \emph{Journal of the Royal Statistical Society}. \url{https://doi.org/10.1111/j.2517-6161.1982.tb01203.x}.

\leavevmode\vadjust pre{\hypertarget{ref-lourens_etal13}{}}%
Lourens, Spencer, Ying Zhang, Jeffrey D Long, et al. 2013. {``Bias in {Estimation} of a {Mixture} of {Normal Distributions}.''} \emph{Journal of Biometrics \& Biostatistics}. \url{https://doi.org/10.4172/2155-6180.1000179}.

\leavevmode\vadjust pre{\hypertarget{ref-R-bpgmm}{}}%
Lu, Xiang, Yaoxiang Li, and Tanzy Love. 2022. \emph{Bpgmm: Bayesian Model Selection Approach for Parsimonious Gaussian Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixdist}{}}%
Macdonald, Peter, and with contributions from Juan Du. 2018. \emph{Mixdist: Finite Mixture Distribution Models}.

\leavevmode\vadjust pre{\hypertarget{ref-maitra_melnykov10}{}}%
Maitra, Ranjan, and Volodymyr Melnykov. 2010. {``Simulating {Data} to {Study Performance} of {Finite Mixture Modeling} and {Clustering Algorithms}.''} \emph{Journal of Computational and Graphical Statistics}. \url{https://doi.org/10.1198/jcgs.2009.08054}.

\leavevmode\vadjust pre{\hypertarget{ref-ariane2022}{}}%
Marandon, Ariane, Tabea Rebafka, Etienne Roquain, et al. 2022. {``False Clustering Rate Control in Mixture Models.''} arXiv. \url{https://doi.org/10.48550/ARXIV.2203.02597}.

\leavevmode\vadjust pre{\hypertarget{ref-R-MGMM}{}}%
McCaw, Zachary. 2021. \emph{MGMM: Missingness Aware Gaussian Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-mclachlan_etal03}{}}%
McLachlan, G. J., D. Peel, and R. W. Bean. 2003. {``Modelling High-Dimensional Data by Mixtures of Factor Analyzers.''} \emph{Computational Statistics \& Data Analysis}, Recent {Developments} in {Mixture} {Model},. \url{https://doi.org/10.1016/S0167-9473(02)00183-4}.

\leavevmode\vadjust pre{\hypertarget{ref-mclachlan_peel00}{}}%
McLachlan, Geoffrey, and David Peel. 2000. \emph{Finite {Mixture Models}: {McLachlan}/{Finite Mixture Models}}. John Wiley \& Sons, Inc. \url{https://doi.org/10.1002/0471721182}.

\leavevmode\vadjust pre{\hypertarget{ref-mclachlan_peel00a}{}}%
Mclachlan, G., and David Peel. 2000. {``Mixtures of {Factor} {Analyzers}.''} In. \url{https://doi.org/10.48550/arXiv.1507.02801}.

\leavevmode\vadjust pre{\hypertarget{ref-mcnicholas_etal10}{}}%
McNicholas, P. D., T. B. Murphy, A. F. McDaid, et al. 2010. {``Serial and Parallel Implementations of Model-Based Clustering via Parsimonious Gaussian Mixture Models.''} \emph{Computational Statistics \& Data Analysis}. \url{https://doi.org/10.1016/j.csda.2009.02.011}.

\leavevmode\vadjust pre{\hypertarget{ref-mcnicholas_murphy08}{}}%
McNicholas, Paul David, and Thomas Brendan Murphy. 2008. {``Parsimonious Gaussian Mixture Models.''} \emph{Statistics and Computing}. \url{https://doi.org/10.1007/s11222-008-9056-0}.

\leavevmode\vadjust pre{\hypertarget{ref-R-pgmm}{}}%
McNicholas, Paul D., Aisha ElSherbiny, Aaron F. McDaid, et al. 2022. \emph{Pgmm: Parsimonious Gaussian Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-mcnicholas_murphy10}{}}%
McNicholas, Paul D., and Thomas Brendan Murphy. 2010. {``Model-Based Clustering of Microarray Expression Data via Latent Gaussian Mixture Models.''} \emph{Bioinformatics}. \url{https://doi.org/10.1093/bioinformatics/btq498}.

\leavevmode\vadjust pre{\hypertarget{ref-R-clustMD}{}}%
McParland, Damien, and Isobel Claire Gormley. 2017. \emph{clustMD: Model Based Clustering for Mixed Data}.

\leavevmode\vadjust pre{\hypertarget{ref-R-MixSim}{}}%
Melnykov, Volodymyr, Wei-Chen Chen, and Ranjan Maitra. 2021. \emph{MixSim: Simulating Data to Study Performance of Clustering Algorithms}.

\leavevmode\vadjust pre{\hypertarget{ref-meng16}{}}%
Meng, Lingyao. 2016. {``Method for Computation of the Fisher Information Matrix in the Expectation-Maximization Algorithm.''} arXiv. \url{https://doi.org/10.48550/ARXIV.1608.01734}.

\leavevmode\vadjust pre{\hypertarget{ref-meng_rubin93}{}}%
MENG, XIAO-LI, and Donald Rubin. 1993. {``Maximum {Likelihood Estimation} via the {ECM Algorithm}: {A General Framework}.''} \emph{Biometrika}. \url{https://doi.org/10.1093/biomet/80.2.267}.

\leavevmode\vadjust pre{\hypertarget{ref-meng_vandyk97}{}}%
Meng, Xiao-Li, and David Van Dyk. 1997. {``The {EM} {Algorithm}--an {Old} {Folk}-Song {Sung} to a {Fast} {New} {Tune}.''} \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}. \url{https://doi.org/10.1111/1467-9868.00082}.

\leavevmode\vadjust pre{\hypertarget{ref-R-microbenchmark}{}}%
Mersmann, Olaf. 2021. \emph{Microbenchmark: Accurate Timing Functions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-bmixture}{}}%
Mohammadi, Reza. 2021. \emph{Bmixture: Bayesian Estimation for Finite Mixture of Distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-ClusterR}{}}%
Mouselimis, Lampros. 2022. \emph{ClusterR: Gaussian Mixture Models, k-Means, Mini-Batch-Kmeans, k-Medoids and Affinity Propagation Clustering}.

\leavevmode\vadjust pre{\hypertarget{ref-murphy12}{}}%
Murphy, Kevin. 2012. \emph{Machine {Learning A Probabilistic Perspective}}. Adaptive Computation; Machine Learning series. \url{https://doi.org/10.1080/09332480.2014.914768}.

\leavevmode\vadjust pre{\hypertarget{ref-rebmix2015a}{}}%
Nagode, Marko. 2015. {``Finite Mixture Modeling via REBMIX.''} \emph{Journal of Algorithms and Optimization}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rebmix}{}}%
---------. 2022. \emph{Rebmix: Finite Mixture Modeling, Clustering \& Classification}.

\leavevmode\vadjust pre{\hypertarget{ref-nityasuddhi_bohning03}{}}%
Nityasuddhi, Dechavudh, and Dankmar Böhning. 2003. {``Asymptotic Properties of the {EM} Algorithm Estimate for Normal Mixture Models with Component Specific Variances.''} \emph{Computational Statistics \& Data Analysis}. \url{https://doi.org/10.1016/S0167-9473(02)00176-7}.

\leavevmode\vadjust pre{\hypertarget{ref-nowakowska_etal14}{}}%
Nowakowska, Ewa, Jacek Koronacki, and Stan Lipovetsky. 2014. {``Tractable {Measure} of {Component Overlap} for {Gaussian Mixture Models}.''} \url{https://doi.org/10.48550/arXiv.1407.7172}.

\leavevmode\vadjust pre{\hypertarget{ref-oakes99}{}}%
Oakes, D. 1999. {``Direct Calculation of the Information Matrix via the {EM}.''} \emph{Journal of the Royal Statistical Society}. \url{https://doi.org/10.1111/1467-9868.00188}.

\leavevmode\vadjust pre{\hypertarget{ref-rebmix2020b}{}}%
Panic, Branislav, Jernej Klemenc, and Marko Nagode. 2020. {``Improved Initialization of the EM Algorithm for Mixture Model Parameter Estimation.''} \emph{Mathematics}. \url{https://doi.org/10.3390/math8030373}.

\leavevmode\vadjust pre{\hypertarget{ref-R-fabMix}{}}%
Papastamoulis, Panagiotis. 2020. \emph{fabMix: Overfitting Bayesian Mixtures of Factor Analyzers with Parsimonious Covariance and Unknown Number of Components}.

\leavevmode\vadjust pre{\hypertarget{ref-pastore_calcagni19}{}}%
Pastore, Massimiliano, and Antonio Calcagnì. 2019. {``Measuring {Distribution Similarities Between Samples}: {A Distribution-Free Overlapping Index}.''} \emph{Frontiers in Psychology}. \url{https://doi.org/10.3389/fpsyg.2019.01089}.

\leavevmode\vadjust pre{\hypertarget{ref-Petersen2008}{}}%
Petersen, K. B., and M. S. Pedersen. 2008. {``The Matrix Cookbook.''} Technical University of Denmark.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixture}{}}%
Pocuca, Nik, Ryan P. Browne, and Paul D. McNicholas. 2022. \emph{Mixture: Mixture Models for Clustering and Classification}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixsmsn}{}}%
Prates, Marcos, Victor Lachos, and Celso Cabral. 2021. \emph{Mixsmsn: Fitting Finite Mixture of Scale Mixture of Skew-Normal Distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-nlsmsn}{}}%
Prates, Marcos, Victor Lachos, and Aldo Garay. 2021. \emph{Nlsmsn: Fitting Nonlinear Models with Scale Mixture of Skew-Normal Distributions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-EMMIXmfa}{}}%
Rathnayake, Suren, Geoff McLachlan, David Peel, et al. 2019. \emph{EMMIXmfa: Mixture Models with Component-Wise Factor Analyzers}.

\leavevmode\vadjust pre{\hypertarget{ref-R-coseq}{}}%
Rau, Andrea. 2022. \emph{Coseq: Co-Expression Analysis of Sequencing Data}.

\leavevmode\vadjust pre{\hypertarget{ref-R-semisup}{}}%
Rauschenberger, Armin. 2022. \emph{Semisup: Semi-Supervised Mixture Model}.

\leavevmode\vadjust pre{\hypertarget{ref-redner_walker84}{}}%
Redner, Richard A., and Homer F. Walker. 1984. {``Mixture {Densities}, {Maximum Likelihood} and the {Em Algorithm}.''} \emph{SIAM Review}. https://doi.org/\url{239https://doi.org/10.1137/1026034}.

\leavevmode\vadjust pre{\hypertarget{ref-robert_casella10}{}}%
Robert, Christian, and George Casella. 2010. \emph{Introducing {Monte Carlo Methods} with {R}}. Springer. \url{https://doi.org/10.1007/978-1-4419-1576-4}.

\leavevmode\vadjust pre{\hypertarget{ref-R-CAMAN}{}}%
Schlattmann, Peter, Johannes Hoehne, and Maryna Verba. 2022. \emph{CAMAN: Finite Mixture Models and Meta-Analysis Tools - Based on c.a.MAN}.

\leavevmode\vadjust pre{\hypertarget{ref-schwarz78}{}}%
Schwarz, Gideon. 1978. {``Estimating the {Dimension} of a {Model}.''} \emph{The Annals of Statistics}. \url{https://doi.org/10.1214/aos/1176344136}.

\leavevmode\vadjust pre{\hypertarget{ref-scrucca10}{}}%
Scrucca, Luca. 2010. {``Dimension Reduction for Model-Based Clustering.''} \emph{Statistics and Computing}. \url{https://doi.org/10.1007/s11222-009-9138-7}.

\leavevmode\vadjust pre{\hypertarget{ref-scrucca_etal16}{}}%
Scrucca, Luca, Michael Fop, T. Brendan Murphy, et al. 2016. {``Mclust 5: {Clustering}, {Classification} and {Density Estimation Using Gaussian Finite Mixture Models}.''} \emph{The R Journal}. \url{https://doi.org/10.32614/RJ-2016-021}.

\leavevmode\vadjust pre{\hypertarget{ref-scrucca_raftery15}{}}%
Scrucca, Luca, and Adrian E. Raftery. 2015. {``Improved Initialisation of Model-Based Clustering Using {Gaussian} Hierarchical Partitions.''} \emph{Advances in Data Analysis and Classification}. \url{https://doi.org/10.1007/s11634-015-0220-z}.

\leavevmode\vadjust pre{\hypertarget{ref-shimizu_kaneko20}{}}%
Shimizu, Naoto, and Hiromasa Kaneko. 2020. {``Direct Inverse Analysis Based on {Gaussian} Mixture Regression for Multiple Objective Variables in Material Design.''} \emph{Materials \& Design}. \url{https://doi.org/10.1016/j.matdes.2020.109168}.

\leavevmode\vadjust pre{\hypertarget{ref-R-fmrs}{}}%
Shokoohi, Farhad. 2022. \emph{Fmrs: Variable Selection in Finite Mixture of AFT Regression and FMR Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-AdaptGauss}{}}%
Thrun, Michael, Onno Hansen-Goos, and Alfred Ultsch. 2020. \emph{AdaptGauss: Gaussian Mixture Models (GMM)}.

\leavevmode\vadjust pre{\hypertarget{ref-tipping_bishop99}{}}%
Tipping, Michael E., and Christopher M. Bishop. 1999. {``Probabilistic {Principal} {Component} {Analysis}.''} \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}. \url{https://doi.org/10.1111/1467-9868.00196}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixreg}{}}%
Turner, Rolf. 2021. \emph{Mixreg: Functions to Fit Mixtures of Regressions}.

\leavevmode\vadjust pre{\hypertarget{ref-R-deepgmm}{}}%
Viroli, Cinzia, and Geoffrey J. McLachlan. 2020. \emph{Deepgmm: Deep Gaussian Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-walsh75}{}}%
Walsh, Gordon Raymond. 1975. \emph{Methods of {Optimization}}. Wiley.

\leavevmode\vadjust pre{\hypertarget{ref-R-IMIX}{}}%
Wang, Ziqiao. 2022. \emph{IMIX: Gaussian Mixture Model for Multi-Omics Data Integration}.

\leavevmode\vadjust pre{\hypertarget{ref-xu_knight10}{}}%
Xu, Dinghai, and John Knight. 2010. {``Continuous {Empirical Characteristic Function Estimation} of {Mixtures} of {Normal Parameters}.''} \emph{Econometric Reviews}. \url{https://doi.org/10.1080/07474938.2011.520565}.

\leavevmode\vadjust pre{\hypertarget{ref-R-dppmix}{}}%
Xu, Yanxun, Peter Mueller, Donatello Telesca, et al. 2020. \emph{Dppmix: Determinantal Point Process Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-yang05}{}}%
Yang, Yuhong. 2005. {``Can the Strengths of {AIC} and {BIC} Be Shared? {A} Confict Between Model Identification and Regression Estimation.''} \emph{Biometrika}. \url{https://doi.org/10.1093/biomet/92.4.937}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixtools}{}}%
Young, Derek, Tatiana Benaglia, Didier Chauveau, et al. 2020. \emph{Mixtools: Tools for Analyzing Finite Mixture Models}.

\leavevmode\vadjust pre{\hypertarget{ref-R-mixR}{}}%
Yu, Youjiao. 2021. \emph{mixR: Finite Mixture Modeling for Raw and Binned Data}.

\end{CSLReferences}

\appendix

\hypertarget{appendix-a-in-depth-statistical-elements-about-parameters-estimation-in-gmms}{%
\section{Appendix A: In-depth statistical elements about parameters estimation in GMMs}\label{appendix-a-in-depth-statistical-elements-about-parameters-estimation-in-gmms}}

\hypertarget{application-of-the-em-algorithm-to-gmms}{%
\subsection{Application of the EM algorithm to GMMs}\label{application-of-the-em-algorithm-to-gmms}}

While solving Equation \eqref{eq:mapping-function-derivative} to retrieve the MLE estimates in the M-step of the EM algorithm, we have to enforce the non-negativity and sum-to-one constraint of the mixture models (Equation
\eqref{eq:2}). This is enabled by the \emph{Lagrange multipliers} tip, which consists in practice to add the equality constraint over the parameters to estimate, here \(-\lambda (\sum_{j=1}^k p_j -1)\), to the function to be optimised (Walsh 1975).

The evaluation of the roots of the derivative of the auxiliary function
(see Equation \eqref{eq:mapping-function-derivative}) at the parameter \(p_j\) with the additional unit simplex constraint \eqref{eq:2} allows to readily compute a MLE estimate of the ratios, valid for any finite mixture model (Equation \eqref{eq:ratios-estimate}):

\begin{equation}
\hat{p_j}= \frac{\sum_{i=1}^n \eta_{i}(j)}{n}
\label{eq:ratios-estimate}
\end{equation}

Additionally, we restrained in both the univariate and
multivariate settings to the fully \emph{unconstrained parametrisation}, in which each component follows its own parametric distribution. The general
derivative of the auxiliary function with respect to each
component parametric distribution \(\zeta_j\), is given by Equation
\eqref{eq:parameter-component-derivative}\footnote{It is equivalent to compute the MLE of a sample following distribution \(f_{\zeta_j}\) weighted by the vector of posterior probabilities.}:

\begin{equation}
\frac{\partial Q(\theta|\hat{\theta}_{q-1})}{\partial \zeta_j}=\sum_{i=1}^n  \eta_{i}(j) \frac{\partial \log (f_{\zeta_j}(X_i|S_i=j))}{\partial \zeta_j}
\label{eq:parameter-component-derivative}
\end{equation}

Accordingly, if a closed form for the computation of the MLE in
supervised cases is known (and fortunately this is the case for both the univariate and multivariate Gaussian distributions), the computation of the maximum of the auxiliary function can be readily calculated.

Plug-in the corresponding parametric distribution in the auxiliary function \eqref{eq:mapping-function-derivative} yields the following formula for the univariate GMM (Equation
\eqref{eq:univariate-auxiliary}):

\begin{equation}
Q(\theta| \hat{\theta}_{q-1}) = \sum_{i=1}^n \sum_{j=1}^k \eta_i(j) \left( \log (p_j) - \log (\sigma_j) - \frac{(X_i-\mu_j)^2}{2\sigma_j^2} \right) + K
\label{eq:univariate-auxiliary}
\end{equation}

and Equation \eqref{eq:multivariate-auxiliary} for the multivariate GMM:

\begin{equation}
Q(\theta| \hat{\theta}_{q-1}) = \sum_{i=1}^n \sum_{j=1}^k \eta_i(j) \left[ \log (p_j) - \frac{1}{2} \left( \log(\operatorname{det}(\boldsymbol{\Sigma}_j)) + (x_i - \boldsymbol{\mu}_j)^\top \boldsymbol{\Sigma}_j^{-1}(x_i - \boldsymbol{\mu}_j)\right) \right] + K
\label{eq:multivariate-auxiliary}
\end{equation}

\(K\) is a constant with respective values of \(\frac{-nD\log(2\pi)}{2}\) and \(\frac{-n\log(2\pi)}{2}\) in the univariate and multivariate setting.

In the univariate setting, the individual MLE mean \(\mu_j\), and variance, \(\sigma_j\), estimates are readily available (Equations \eqref{eq:mean-univariate} - \eqref{eq:sigma-univariate}):

\begin{equation}
 \frac{\partial Q(\theta|\hat{\theta}_{q-1})}{\partial \mu_j} = 0
 \Leftrightarrow
  \mu_j = \frac{\sum_{i=1}^n \eta_i(j) X_i}{\sum_{i=1}^n \eta_i(j)}
\label{eq:mean-univariate}
\end{equation}

\begin{equation}
 \frac{\partial Q(\theta|\hat{\theta}_{q-1})}{\partial \sigma_j} = 0
 \Leftrightarrow
\sigma_j^2  =  \frac{\sum_{i=1}^n \eta_i(j) (x_i - \mu_j)^2 }{\sum_{i=1}^n \eta_i(j)}
\label{eq:sigma-univariate}
\end{equation}

Before finding the optimum of the auxiliary function in the multivariate setting, we remind the interested reader of some relevant calculus formulas below:

\hypertarget{properties-matrix}{}
\begin{blackbox}{\textbf{Transpose matrix properties}}

\begin{cols}

\begin{col}{0.3\textwidth}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \(\operatorname{det}(p\boldsymbol{A})=p^G \operatorname{det} (\boldsymbol{A})\)
\end{enumerate}

\end{col}

\begin{col}{0.3\textwidth}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(\operatorname{det}(A^{-1})=\frac{1}{\operatorname{det}(A)}\)
\end{enumerate}

\end{col}

\begin{col}{0.3\textwidth}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \(\left(\boldsymbol{A}^{-1} \right)^\top=\boldsymbol{A}^{-1}\)\footnote{when \(\boldsymbol{A}\) is itself symmetric, as by definition,
    \(\boldsymbol{A}^\top=\boldsymbol{A}\)}
\end{enumerate}

\end{col}

\end{cols}

\end{blackbox}

\hypertarget{properties-calculus}{}
\begin{blackbox}{\textbf{Matrix calculus}}

Given a symmetric matrix \(\boldsymbol{A}\) of full rank \(D\) and two vectors
\(\boldsymbol{x}\) and \(\boldsymbol{\mu}\) of size \(D\), the following derivative
properties hold:

\begin{cols}

\begin{col}{0.3\textwidth}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \(\frac{\partial \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x}}{\partial \boldsymbol{A}} = \boldsymbol{x} \boldsymbol{x}^\top\)
\end{enumerate}

\end{col}

\begin{col}{0.3\textwidth}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(\frac{\partial (\boldsymbol{x}-\boldsymbol{\mu})^\top \boldsymbol{A} (\boldsymbol{x}-\boldsymbol{\mu})}{\partial \boldsymbol{\mu}} = -2 \boldsymbol{A}(\boldsymbol{x}-\boldsymbol{\mu})\)
\end{enumerate}

\end{col}

\begin{col}{0.3\textwidth}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \(\frac{\partial \log\left(\operatorname{det}(\boldsymbol{A})\right)}{\partial \boldsymbol{A}^{-1}} = -\boldsymbol{A}\)
  \footnote{Other matrix calculus formulas and notations are available on \href{https://en.wikipedia.org/wiki/Matrix_calculus\#Scalar-by-vector}{Matrix
    calculus}
    and demonstration details from \emph{The Matrix Cookbook} (Petersen and Pedersen 2008).}
\end{enumerate}

\end{col}

\end{cols}

\end{blackbox}

Using the calculus formulas derived in the previous boxes, a closed form for the MLE estimate of the mean, \(\boldsymbol{\mu}_j\), and covariance, \(\boldsymbol{\Sigma}_j\), is readily computed (see Equations \eqref{eq:mean-bivariate} - \eqref{eq:sigma-bivariate}):

\begin{equation}
\frac{\partial Q(\theta|\hat{\theta}_{q-1})}{\partial \boldsymbol{\mu}_j} =
\sum_{i=1}^n \eta_i(j) \boldsymbol{\Sigma}_j^{-1}(x_i - \boldsymbol{\mu}_j) =0
\Leftrightarrow
  \mu_j = \frac{\sum_{i=1}^n \eta_i(j) \boldsymbol{x}_i}{\sum_{i=1}^n \eta_i(j)}
\label{eq:mean-bivariate}
\end{equation}

\begin{equation}
 \frac{\partial Q(\theta|\hat{\theta}_{q-1})}{\partial \boldsymbol{\Sigma}_j^{-1}}=\frac{1}{2} \sum_{i=1}^n \eta_i(j) \left[ \boldsymbol{\Sigma}_j  - (x_i - \boldsymbol{\mu}_j)(x_i - \boldsymbol{\mu}_j)^\top \right]= 0
\Leftrightarrow
  \boldsymbol{\Sigma}_j = \frac{\sum_{i=1}^n \eta_i(j) (x_i - \boldsymbol{\mu}_j)(x_i - \boldsymbol{\mu}_j)^\top }{\sum_{i=1}^n \eta_i(j)}
\label{eq:sigma-bivariate}
\end{equation}

Explicitly optimising the equations (\eqref{eq:univariate-auxiliary}-\eqref{eq:multivariate-auxiliary}) yield the following MLE parameters in both the univariate and multivariate settings (Table \ref{tab:summary-em-pdf}), as detailed in (Leytham 1984; Redner and Walker 1984):

\begin{table}[!h]

\caption{\label{tab:summary-em-pdf}An overview of the practical implementation of the EM algorithm in GMMs.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{}l|>{}l|>{\raggedright\arraybackslash}m{6.8cm}}
\toprule
\multicolumn{1}{c}{\textbf{ }} & \multicolumn{1}{c}{\textbf{Univariate GMM}} & \multicolumn{1}{c}{\textbf{Multivariate GMM}}\\
\midrule
\multicolumn{1}{c}{E-step} & \multicolumn{1}{c}{$\eta_i(j) = \frac{\hat{p}_j^q \mathcal{N} (x_i|\hat{\mu}_j^q, \hat{\sigma}_j^q)}{\sum_{j=1}^k \hat{p}_j^q \mathcal{N} (x_i|\hat{\mu}_j^q, \hat{\sigma}_j^q)}$} & \multicolumn{1}{c}{$\eta_i(j) = \frac{\hat{p}_j^q \mathcal{N}_D (\boldsymbol{x}_i|\boldsymbol{\hat{\mu}}_j^q, \boldsymbol{\hat{\Sigma}}_j^q)}{\sum_{j=1}^k \hat{p}_j^q \mathcal{N}_D (\boldsymbol{x}_i|\boldsymbol{\hat{\mu}}_j^q, \boldsymbol{\hat{\Sigma}}_j^q)}$}\\
\midrule
\multicolumn{1}{c}{Ratios estimation} & \multicolumn{1}{c}{$\hat{p}_j^{q+1}= \frac{\sum_{i=1}^n \eta_{i}(j)}{n}$} & \multicolumn{1}{c}{$\hat{p}_j^{q+1}= \frac{\sum_{i=1}^n \eta_{i}(j)}{n}$}\\
\midrule
\multicolumn{1}{c}{Mean estimation} & \multicolumn{1}{c}{$\hat{\mu}_j^{q+1} = \frac{\sum_{i=1}^n \eta_i(j) x_i}{\sum_{i=1}^n \eta_i(j)}$} & \multicolumn{1}{c}{$\hat{\boldsymbol{\mu}}_j^{q+1} = \frac{\sum_{i=1}^n \eta_i(j) \boldsymbol{x}_i}{\sum_{i=1}^n \eta_i(j)}$}\\
\midrule
(Co)Variance estimation & $\left(\hat{\sigma}_j^2\right)^{q+1}  =  \frac{\sum_{i=1}^n \eta_i(j) \left(x_i - \hat{\mu}_j^{q+1}\right)^2 }{\sum_{i=1}^n \eta_i(j)}$ & $\left(\boldsymbol{\hat{\Sigma}}_j^2\right)^{q+1} = \frac{\sum_{i=1}^n \eta_i(j) \left(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_j^{q+1} \right)\left(\boldsymbol{x}_i - \hat{\boldsymbol{\mu}}_j^{q+1} \right)^\top }{\sum_{i=1}^n \eta_i(j)}$\\
\bottomrule
\end{tabular}}
\end{table}

In both cases, obtaining the parameters of each component's parametric distribution
turn to be equivalent to the computation of the mean and variance of a weighted sample, which can be computed in R with \code{stats::weighted.mean} and
\code{stats::cov.wt} functions\footnote{We assign ``ML'' to the argument \emph{method} to get the biased but true MLE estimate of the covariance}. Importantly, the value of the mapping function only depends on the set of the observations \(X\), but does not depend on the parameter to estimate \(\theta\). Indeed, the statistic computed by the EM algorithm is sufficient, which is one of its main advantages.

The complete code associated to our R implementation is implemented respectively
with \texttt{enmix\_univariate} and \texttt{enmix\_bivariate} for the univariate and multivariate setting,
available on GitHub at \href{https://github.com/bastienchassagnol-servier/RGMMBench}{RGMMBench}, as well as the
programs used to generate the several plots and tables of the article.
We additionally made two choices not clearly set in the literature:

\begin{itemize}
\tightlist
\item
  The algorithm stops when when the absolute difference between
  consecutive log-likelihoods falls below a user-defined threshold
  \emph{epsilon}, with a maximal number of \emph{itmax} iterations allowed to reach this convergence.
\item
  In order to avoid numerical underflows resulting in inconsistent ratios, of type \(0/0\), we rely on the fact that Gaussian distributions belong to the exponential family to log-rescale our observations and compute efficiently the posterior probabilities in the E-step of the EM algorithm. First, to avoid null values for highly unlikely observations, those far from the centroids, we use the log attribute of \code{stats::dnorm} and
  \code{mvtnorm::dmvnorm} functions, see Equation \eqref{eq:sample-likelihood}:
\end{itemize}

\begin{equation}
\begin{split}
        \ell (\theta|x) &= \log (\sum_{j=1}^k p_j f{\zeta_j} (x)) \\
        & = \log \left( \exp \left[ \log(p_j) + \log(f{\zeta_j} (x)) \right]\right) \quad
\end{split}
\label{eq:sample-likelihood}
\end{equation}

Second, we rewrite our sum of exponentials, the one enclosed into the log, to use the Taylor' series of \(\log(1+x)\), with \(|x|\ll 1\), see Equation \eqref{eq:logsumexp}:

\begin{equation}
\begin{split}
\log \left( \sum_{j=1}^k e^{a_j}\right)&=\log \left( \exp(a_j') \times \left[ 1+ \sum_{j\neq j'}\exp\left(\frac{a_j}{a_{j'}}\right) \right] \right) \\
& = a_{j'} + \operatorname{log1p} \left(\sum_{j\neq j'}\exp\left(\frac{a_j}{a_{j'}}\right)\right), \, \text{ with } j'=\arg \max_{\forall j \in \{1, \ldots, k\}} (e^{a_j})
\end{split}
\label{eq:logsumexp}
\end{equation}

with \code{log1p} the R function dedicated for this Taylor's development. The posterior probabilities are then given by Equation
\eqref{eq:sample-posterior-probability}:

\begin{equation}
\log\left(\mathbb{P}_\theta(S=j | X=x)\right)=\log(p_j) + \log(f_{\zeta_j}) - \ell (\theta |x)
\label{eq:sample-posterior-probability}
\end{equation}

\begin{itemize}
\tightlist
\item
  We stop the algorithm early when the estimates are trapped in the boundaries of the parameter space, typically when the ratio of a component or its associated variance tends to zero. This case rarely occurs in our simulations: once in univariate and never in multivariate.
\end{itemize}

\hypertarget{parsimonious-parametrisation-of-multivariate-gmms}{%
\subsection{Parsimonious parametrisation of multivariate GMMs}\label{parsimonious-parametrisation-of-multivariate-gmms}}

\label{subsec:parsimonious-parametrisation}

Parsimonious parametrisation of GMMs models are provided by the following \emph{eigenvalue} factorisation of the covariance matrix (Equation \eqref{eq:covariance-decomposition}):

\begin{equation}
    \boldsymbol{\Sigma}_j=\lambda_j \boldsymbol{Q}_j \boldsymbol{D}_j \boldsymbol{Q}_j^\top
    \label{eq:covariance-decomposition}
\end{equation}

with \(\lambda_j=\operatorname{det}\left(\boldsymbol{\Sigma}_j\right)^{\frac{1}{D}}\) a scalar proportional to the total volume of the ellipsoid (or area in bidimensional setting), \(\boldsymbol{D}_j\) a diagonal matrix storing the eigenvalues normalised such that \(|\boldsymbol{D}_j|=1\)\footnote{Langrognet et al. (2021) enforces an additional but, in our opinion, superfluous constraint that the eigen values are sorted by decreasing order} and \(\boldsymbol{Q}_j\) a \(\mathcal{M}_D(\mathbb{R})\) \emph{orthogonal matrix} whose
columns are \(D\) linearly independent \emph{eigenvectors} generating an orthonormal basis in \(\mathbb{R}^D\) while \(\boldsymbol{Q}_j^\top\) is its corresponding transpose matrix. The existence of the decomposition is guaranteed by the positive definiteness constraint over the covariance matrix while the orthogonality of \(\boldsymbol{Q}_j\) results from its symmetry. When the matrix to factorise is positive-definite and symmetric, we also refer to it as \emph{spectral decomposition}, a special case of \emph{eigendecomposition}.

Each of these matrices can be constrained to be equal or variable across clusters, hence this decomposition reveals 14 possible models with different geometric characteristics, namely:

\begin{itemize}
\item
  two models with the \emph{spherical family}, for which only \(\lambda_j\) is used to control the \emph{isotropic} (same radius in any dimension) volume of each component of the corresponding distribution structure
\item
  four models with the \emph{diagonal family}, using \(\lambda_j\) with possibly distinct diagonal elements and \(\boldsymbol{D}_j\) to specify the shape of the density contours. In that context, \(\boldsymbol{Q}_j\) is henceforth a permutation matrix, whose inputs are only zeros and an unique one per row.
\item
  eight models with the \emph{general family}, using additionally \(\boldsymbol{Q}_j\) to determine the orientation of the main axes of the ellipsoids. Indeed, in the last two families described, this matrix was equal to the identity, hence the axis of the ellipsoids were aligned with the standard \(\mathbb{R}^D\) basis.
\end{itemize}

We detail the main characteristics of the 14 parametrisations (28 if we add for each model the equiproportional hypothesis) in Table (\ref{tab:parameter-configuration-bivariate}):

\begin{table}

\caption{\label{tab:param-multivariate-gaussian}The 14 canonical parametrisations of the within-group covariance matrix $\boldsymbol{\Sigma_j}$ with the corresponding geometric representations.}
\centering
\resizebox{\linewidth}{!}{
\fontsize{7}{9}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}m{2cm}>{\raggedright\arraybackslash}m{3cm}>{\raggedright\arraybackslash}m{3cm}>{\raggedright\arraybackslash}m{2cm}>{\raggedright\arraybackslash}m{4.5cm}>{\raggedright\arraybackslash}m{5cm}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Notation}} & \multicolumn{1}{c}{\textbf{Family}} & \multicolumn{1}{c}{\textbf{M-step}} & \multicolumn{1}{c}{\textbf{Number of parameters}} & \multicolumn{1}{c}{\textbf{Representation}}\\
\midrule
\multicolumn{1}{c}{EII} & \multicolumn{1}{c}{$[\lambda I]$} & \multicolumn{1}{c}{Spherical} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + 1$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EII.png}\\
\midrule
\multicolumn{1}{c}{VII} & \multicolumn{1}{c}{$[\lambda_j I]$} & \multicolumn{1}{c}{Spherical} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + k$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VII.png}\\
\midrule
\multicolumn{1}{c}{EEI} & \multicolumn{1}{c}{$[\lambda D]$} & \multicolumn{1}{c}{Diagonal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + d$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EEI.png}\\
\midrule
\multicolumn{1}{c}{VEI} & \multicolumn{1}{c}{$[\lambda_j D]$} & \multicolumn{1}{c}{Diagonal} & \multicolumn{1}{c}{IP} & \multicolumn{1}{c}{$\alpha + d + k - 1$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VEI.png}\\
\midrule
\multicolumn{1}{c}{EVI} & \multicolumn{1}{c}{$[\lambda D_j]$} & \multicolumn{1}{c}{Diagonal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + kd - k + 1$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EVI.png}\\
\midrule
\addlinespace
\multicolumn{1}{c}{VVI} & \multicolumn{1}{c}{$[\lambda_j D_j]$} & \multicolumn{1}{c}{Diagonal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + kd$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VVI.png}\\
\midrule
\multicolumn{1}{c}{EEE} & \multicolumn{1}{c}{$[\lambda Q D Q^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + \beta$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EEE.png}\\
\midrule
\multicolumn{1}{c}{EVE} & \multicolumn{1}{c}{$[\lambda QD_j Q^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{IP} & \multicolumn{1}{c}{$\alpha + \beta$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EVE.png}\\
\midrule
\multicolumn{1}{c}{VEE} & \multicolumn{1}{c}{$[\lambda_j Q D Q^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{IP} & \multicolumn{1}{c}{$\alpha + \beta + (k-1)(d-1)$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VEE.png}\\
\midrule
\multicolumn{1}{c}{VVE} & \multicolumn{1}{c}{$[\lambda_j QD_j Q^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{IP} & \multicolumn{1}{c}{$\alpha + \beta + d(k-1)$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VVE.png}\\
\midrule
\addlinespace
\multicolumn{1}{c}{EEV} & \multicolumn{1}{c}{$[\lambda Q_j D Q_j^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + k \beta - d(k-1)$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EEV.png}\\
\midrule
\multicolumn{1}{c}{VEV} & \multicolumn{1}{c}{$[\lambda_j Q_j D Q_j^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{IP} & \multicolumn{1}{c}{$\alpha + k \beta - (k-1)(d-1)$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VEV.png}\\
\midrule
\multicolumn{1}{c}{EVV} & \multicolumn{1}{c}{$[\lambda Q_j D_j Q_j^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + k \beta - k + 1$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/EVV.png}\\
\midrule
\multicolumn{1}{c}{VVV} & \multicolumn{1}{c}{$[\lambda_j Q_j D_j Q_j^\top]$} & \multicolumn{1}{c}{Ellipsoidal} & \multicolumn{1}{c}{CF} & \multicolumn{1}{c}{$\alpha + k \beta$} & \multicolumn{1}{c}{}\includegraphics[width=0.67in, height=0.39in]{./tables/gaussian_param/VVV.png}\\
\midrule
\bottomrule
\end{tabular}}
\end{table}

\begin{itemize}
\item
  The first column describes in general and understandable terms each parametrisation, with I meaning invariant (alternatively, not used in the parametrisation), E means equal and V variable while the second column matches the corresponding matrix decomposition of the covariance matrix. These 14 models are all included in one of the three super-families: spherical, diagonal and ellipsoidal listed before. As an example, the model VEI has variable volumes \(\lambda_j\) in relation with the cluster, however shares same general shape (as we can note on the Representations, all isodensities are distributed along the \(x\)-axis) and invariant directions (in other words, the transition matrix is the identity matrix, entailing that all scatter plots are aligned with the Cartesian coordinate axes).
\item
  Varying the volume \(\lambda_j\), given a fixed \(\boldsymbol{Q}\) and \(\boldsymbol{D}\), amounts to an \emph{enlargement} (when all dimensions of a figure are changed in the same scale, also referred to as \emph{isotropic} transformation), varying the eigenvectors \(\boldsymbol{Q}_j\), given a fixed volume \(\lambda_j\) and \(\boldsymbol{D}\) is equivalent to a rotation and finally varying the diagonal matrix \(\boldsymbol{D}_j\), given the other parameters of Equation \eqref{eq:covariance-decomposition} are fixed, results in a \emph{distortion} of the representation.
\item
  CF means that the M-step is in closed form while IP entails that the M-step is iterative.
\item
  The number of parameters enumerates the \emph{degrees of freedom}, namely the number of parameters to truly estimate once the sum-to-one constraint is enforced (Equation \eqref{eq:2}). In detail, \(k\) is the number of components of the GMM model, \(D\) its dimension, \(\alpha=kD + k - 1\) is the number of parameters required to identify the mean vector of each component (\(kD\)) and the ratios \(k-1\) and \(\beta=\frac{D(D+1)}{2}\) the number of covariance terms to estimate for a given component (\(D\) variance diagonal terms, the remaining terms being the pairwise symmetric covariance terms between the features). Note that the complexity of the covariance matrix in the fully unconstrained model (Model VVV) grows linearly with the number of components while exploding in the order \(\mathcal{O}(D)\) with the number of dimensions. Meantime, the complexity of the parametrisation with the homoscedastic spherical family (Model EII) is constant.
\item
  Last column displays the 14 most common GMMs parametrisations, by plotting the ellipses and centroids of a three components bivariate GMM parametrised by the mean vector and covariance of each component. For any additional detail, we refer the interested reader to \pkg{mclust} (Scrucca et al. 2016) and \pkg{Rmixmod} (Langrognet et al. 2021) vignettes for a general introduction to GMMs and to (Banfield and Raftery 1993; Celeux and Govaert 1992; Browne and McNicholas 2014) for the closed formulas of the models.
\end{itemize}

\hypertarget{parameters-estimation-in-a-high-dimensional-context}{%
\subsection{Parameters estimation in a high-dimensional context}\label{parameters-estimation-in-a-high-dimensional-context}}

\label{subsec:high-dimensional}

However, while parsimonious representations can largely reduce the computational burden, none of them in the general family is able to handle degenerate cases where the number of features, \(D\), exceeds the number of observations \(n\). Likewise situations, when the number of features is consequent, are referred to as high-dimensional, raising the well-known issue of the ``curse of the dimensionality''. Two distinct approaches have been developed in the literature to handle these degenerate cases:

\begin{itemize}
\item
  The most naive approach aims to eliminate the least informative variables by applying a strong Lasso-type penalty on the parameters to be estimated. We only came across such an approach twice among the reviewed R packages, in the specific context of regressions of mixtures (see\pkg{RobMixReg} and \pkg{fmerPack} packages).
\item
  The second category includes a larger diversity of methods, all inspired from the \emph{factor analysis} approach whose paradigm is to consider that all the \(D\) features used to describe the observations can be spanned in a smaller subspace without lose of information. Precisely, the factor analysis theory describes the variability among observed and correlated variables by a substantial lower number of unobserved variables called \emph{factors} or \emph{latent variables}.
  In practice, for a given component \(j\), the diagonal matrix storing the eigenvalues is decomposed into two-blocks. The first upper-right diagonal block, assumed generally of dimension \(d_j \ll D\), stores the largest \(d_j\) eigenvalues and model the variance of the actual data of component \(j\) while the lower-left diagonal block, of dimension \(D-d_j\), stores an unique parameter that can be interpreted as the variance of the residual error terms, constrained to be strictly inferior to the lowest variability of the informative variables. The dimension \(d_j\) can be considered as the intrinsic dimension of the latent subspace of cluster \(j\) spanned by the first \(d_j\) eigenvectors of \(\boldsymbol{Q}_j\)\footnote{Starting from eigen-decomposition described in (Equation \eqref{eq:covariance-decomposition}), this approach is equivalent to consider only the \(d_j\) largest eigenvalues resulting from the decomposition and sets the others to null.}.
\end{itemize}

When the sub dimension \(d_j\) is known, a closed version is generally available for the M-step of the EM algorithm, however \(d_j\) is itself an hyperparameter to estimate. Though, (Bouveyron, Celeux, and Girard 2011) has shown that a classical Cattell's scree-test could be used to asymptotically estimate the intrinsic dimension of each cluster. Compared to the previous approach, this method has a strong theoretical background and strong impact on the running times performance.

Taking a concrete use case from the help documentation of the package \pkg{HDclassif}, it enabled to cluster a dataset of 10 classes with 130 observations overall and described in a 1024-dimensional space (consider the famous machine-learning digit recognition problem). Variants of these approaches have been developed in the following packages: \pkg{HDclassif}, \pkg{fabMix}, \pkg{EMMIXmfa} and \pkg{pgmm}. We refer the interested reader to the educational vignette of \pkg{HDclassif}: \href{https://rdrr.io/pkg/HDclassif/man/HDclassif-package.html}{HDclassif} and papers (Paul David McNicholas and Murphy 2008; P. D. McNicholas et al. 2010; Paul D. McNicholas and Murphy 2010).

\color{blue}

Historically, the first mention of a probabilistic framework with an application to dimension reduction in the context of finite mixture models goes back to Tipping and Bishop (1999), based on principal component analysis. G. J. McLachlan, Peel, and Bean (2003) and Mclachlan and Peel (2000) extend this original model by postulating that the distribution of the data within any latent class could be described using the tools of the factor analysis field\footnote{Although principal component analysis and factor analysis are closely related, we can differentiate both approaches by their differing objective: while PCA seeks to capture the overall variability of the dataset, factor analysis focuses on describing the intra-variability between covariates. In practice, the differences between the two approaches are minor, we can notably show that the output of PCA is one of the solutions suggested by standard factor analysis.} Finally, building on the parsimonious parametrisations already theorised for GMMs (see previous section) , Paul David McNicholas and Murphy (2008), P. D. McNicholas et al. (2010) and Bouveyron, Girard, and SCHMID (2007) proposed a variety of constraints, but this time directly defined on the projected subspace. Since all methods based on factor analysis provide a transition matrix, using the two or three most informative eigen values and their associated eigen vectors in order to project the dataset on a smaller subspace provides a simple visualisation tool for representing high dimensional datasets.
However, this method may is not suitable for unravelling the clustering structure. Instead, \emph{the GMMDR method}, first proposed by Scrucca (2010) and implemented in the \code{MclustDR} function, from \pkg{mclust} package, aims at recovering the subspace that best captures the underlying latent clustering structure (we notably expect invariance of the global overlap in the sampling space and the corresponding projected subspace). More precisely, the main objective of the GMMDR technique is to infer the global \emph{change-of-basis matrix} \(\boldsymbol{Q}\) that minimises the differences in the a posteriori probabilities of assigning each observation \(i\) to a given cluster \(s_i\), knowing the value of the vector of observed covariates \(\boldsymbol{x}_i\). Namely, we are looking for the orientation matrix \(\boldsymbol{Q}\) that maximally ensures the following objective (Eq. \eqref{eq:GMMDR}):

\begin{equation}
 \hat{\boldsymbol{Q}} = \arg \max_{\boldsymbol{Q}} \,  \left(\mathbb{P}_{\theta} (S_i=j | \boldsymbol{X} =\boldsymbol{x}_i) = \mathbb{P}_{\theta} (S_i=j | \boldsymbol{X} \boldsymbol{Q}) \right)
\text{ such that } S \perp \boldsymbol{X} | \boldsymbol{X} \boldsymbol{Q}
    \label{eq:GMMDR}
\end{equation}

This procedure itself derives from the \emph{sliced inverse regression} algorithm (K.-C. Li 1991), but instead of conditioning on the known response variable, GMMDR conditions on the estimated MAP cluster assignments. Since the solution returned by the following optimization problem is not unique, we generally constrain the projection matrix to be orthonormal (any of the vectors forming the basis are pairwise orthogonal, and individually of norm 1).
\color{black}

\hypertarget{model-selection}{%
\subsection{Model selection}\label{model-selection}}

When comparing several models with several number of components or
parametrisations, the likelihood is uninformative as it can be
arbitrarily minimised by increasing the complexity of the model or
adding components. it is then necessary to penalise for complexity when
comparing them. The general form of the penalty metric, \emph{GIC} (for
generalised information criteria), is given by Equation
\eqref{eq:GIC-score}:

\begin{equation}
    \operatorname{GIC} (\theta) =  \underbrace{p(\theta)}_{\text{penalty term}} - \underbrace{2 \ell (X | \theta)}_{\text{log-likelihood of the model}}
\label{eq:GIC-score}
\end{equation}

Among them, we set apart scores focused on selecting selecting the right
number of parameters and components, namely the \emph{degrees of freedom}
(d.o.f.) of the model (\(3k-1\) parameters for the univariate
unconstrained GMM), and those focusing on retrieving readable clusters.

In the first category, the \emph{AIC} (Akaike information criterion)
(Schwarz 1978) is a \emph{minimax-rate optimal} (score that minimises the risk
in the worst case) but inconsistent metric (Yang 2005) , proned to
overestimate the true number of components. \emph{BIC} (Bayesian Information
Criterion), and CAIC (consistent AIC), accounting for both the number of
parameters and the sample size, are consistent metrics. Finally, the
\emph{MDL}(Minimum Description Length) criterion accounts for the number of
parameters, sample size and number of components. Its core objective
differs from the others as it aims at reducing the amount of code to
encode both parameters and observations but is practically close to the
BIC metric. A thorough description of these scores, with their formulas
and theoretical properties, can be found in Fonseca (2008), Celeux, Fruewirth-Schnatter, and Robert (2018).

In the second category, the most commonly implemented is the \emph{ICL
(integrated complete-data likelihood)}, a BIC criterion with an
additional entropy penalty (G. McLachlan and Peel 2000). As opposed to \emph{BIC},
the entropy term reduces the number of components to a well-separated
and readable clustering. Hence, it tends to underestimate their true
number when components are overlapping. Alternative similar metrics are
the \emph{CLC} (Classification Likelihood Criterion), \emph{AWE} (Approximate
Weight of Evidence) and \emph{NEC} (Normalised Entropy Criterion) metrics
(Bacci, Pandolfi, and Pennoni 2012). The several metrics implemented by the reviewed
packages are listed in
\ref{tab:high-comparison-packages-pdf}.

The \emph{Likelihood-ratio test} (LRTS) can also be used to compare \emph{nested
models}, with additional advantage to possibly derive a \emph{p}-value
yielding the probability that a complex model (with more components)
should preferentially be used over a simpler one. Traditionally, common
process is to add one component after the other, until hypothesis \(H_0\)
can not be rejected anymore. Under standard regularity conditions of
Cramer's theorem, Wilk's theorem states that the Likelihood Ratio
distribution follows asymptotically a \(\chi^2\) distribution, but
unfortunately these conditions are not met in mixture models
(G. McLachlan and Peel 2000). To counterbalance it, bootstrap inference
(G. McLachlan and Peel 2000) is often used to derive an empirical distribution of
the Likelihood Ratio.

\hypertarget{derivation-of-confidence-intervals-in-gmms}{%
\subsection{Derivation of confidence intervals in GMMs}\label{derivation-of-confidence-intervals-in-gmms}}

\emph{Punctual estimation}, with a single estimate \(\hat{\theta}\) for a given
\emph{n}-sample, is not enough to evaluate the performance of a specific
method, as drawing another \emph{n}-sample using the same parameters is
likely to lead to a different distribution and estimation of
\(\hat{\theta}\). Instead, it can be interesting to retrieve the
distribution or at least the variability of the estimated parameters,
which can reveal useful to derive confidence intervals. However,
obtaining the distribution or even an asymptotic approximation of the
distribution of the parameters is not feasible in practice with mixture
models (G. McLachlan and Peel 2000). Hence, most authors recommend to use
bootstrap methods for the generation of confidence intervals, as
suggested in (Efron and Tibshirani 1993; Basford et al. 1997).

Bootstrap distributions of the parameters are generally retrieved via
\emph{empirical} or \emph{parametric} bootstrap, both available in the
\pkg{mclust} package. In the \emph{empirical} or \emph{non-parametric} bootstrap
Jaki et al. (2018), we draw iteratively \(N\) samples of size \(n\) with
replacement from the original observed variable \(x_{1:n}\). In the
\emph{parametric} bootstrap, \(N\) simulations are built from the parameter
estimated with the available observations of \(X\), via the EM algorithm
or any method used for parameter estimation. In both cases, we obtain an
empiric distribution of the parameter estimate:
\(\hat{\theta}_{1:N}= (\hat{\theta}^1, \ldots, \hat{\theta}^N)\). Sample
mean and standard deviation (SD) of this empirical distribution can be
used to retrieve an asymptotic estimate of the variability of the
parameter estimate \(\hat{\theta}\), the bias or the MSE of the parameter
estimates. To get unbiased estimates of the true standard deviation and
mean of the estimates, it is of common practice to compute the empirical
covariance matrix of the sample
\(\operatorname{cov} [\hat{\theta}]= \frac{\sum_{j=1}^N (\hat{\theta_j} - \mathbb{E} [\hat{\theta}]) (\hat{\theta_j} - \mathbb{E} [\hat{\theta}])^\mathrm{T}}{N - 1}\),
the square roots of its diagonal terms corresponding to the empiric SDs.
Symmetric \(1-\alpha\) asymptotic confidence intervals using the Central
Limit Theorem (CLT) can then be simply derived Equation
\eqref{eq:confidenceinterval}:

\begin{equation}
  \mathbb{E} [\hat{\theta}_t] \pm \frac{1}{\sqrt{n}}z_{1 - \frac{\alpha}{2}} \sqrt{\operatorname{var}(\hat{\theta}_t}, \quad \forall t \in \{1, \ldots, 3k\}
\label{eq:confidenceinterval}
\end{equation}

with \(z_{1 - \frac{\alpha}{2}}\) the \(1 - \frac{\alpha}{2}\) quantile of
the standard Gaussian distribution.

If computing the covariance matrix is not possible analytically, it can
be approximated by the expected Fisher Information Matrix
\(\mathcal{I}_{\exp}(\theta)\) (FIM), given by Equation
\eqref{eq:expected-fim}:

\begin{equation}
     \left[\mathcal{I}_{\exp}(\theta)\right]_{1\le i \le 3k,1 \le j \le 3k} = - \mathbb{E} \left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \ell(\theta|X)\right]
\label{eq:expected-fim}
\end{equation}

Indeed, the Cramér-Rao theorem states that the diagonal elements of the
inverse of the FIM are upper bounded by the variability of the
parameters:
\(\operatorname{var}(\hat{\theta}) \ge \frac{1}{\mathcal{I}(\theta)}\). This
implies that the ratio between inverse of the FIM and the variance
\(e(\hat{\theta})=\frac{\mathcal{I}(\hat{\theta})^{-1}}{\operatorname{var}(\hat{\theta})}\)
converges to 1, using the asymptotic efficiency of the MLE estimate of
GMMs.

Unfortunately, the computation of the expected FIM is still a hard task.
Hence it is generally replaced by the observed FIM, the negative of the
Hessian matrix of the incomplete log-likelihood function:
\(\mathcal{I}_\text{obs}(\theta)= -\frac{\partial^2}{\partial \theta_i \partial \theta_j} \ell(\theta|X)\).
Exact general formulas are provided for the univariate case in Louis (1982)
and for the multivariate case in Oakes (1999). Yet, it has to be noted that
the expected FIM generally outperforms the observed FIM in estimating
the covariance matrix of the MLE (X. Cao and Spall 2012).

However all these methods require to compute second derivatives of the
log-likelihood leading to some disadvantages from a computational point
of view. More recently, L. Meng (2016) and Delattre and Kuhn (2019) proposed an
accelerated algorithm requiring only computation of first order
derivatives. A similar alternative is implemented in the
\CRANpkg{mixsmsn} package (Prates, Lachos, and Cabral 2021):
\code{mixsmsn::im.smsn}, in which the Hessian matrix is approximated by the cross-product of the
gradient of the log-likelihood Equation
\eqref{eq:approximation-fim-gradient}:

\begin{equation}
        \mathcal{I}_{\text{obs}}(\theta) \approx - \frac{\partial \log (\ell (\theta|X))}{\partial \theta} \frac{\partial \log (\ell (\theta|X))}{\partial \theta}^T
    \label{eq:approximation-fim-gradient}
\end{equation}

according to an idea developed in paper Basford et al. (1997). For a more general introduction to Gaussian mixtures, including other models and parametrisations in the multivariate case, we refer the reader to the reference book \emph{Gaussian parsimonious clustering models}
Celeux and Govaert (1992).

\hypertarget{an-analytic-formula-of-the-overlap-for-univariate-gaussian-mixtures}{%
\subsection{An analytic formula of the overlap for univariate Gaussian mixtures}\label{an-analytic-formula-of-the-overlap-for-univariate-gaussian-mixtures}}

From an analytic point of view, the overlap between \(k\) components of
variable \(X\) is given by Equation \eqref{eq:general-overlap}:

\begin{equation}
    \operatorname{OVL} (X)= 1 - \int_{\mathbb{R}} \max_j (p_j \varphi_{\zeta_j} (x)) dx
\label{eq:general-overlap}
\end{equation}

The 1 in Equation \eqref{eq:general-overlap} corresponds to the
integration of probability \(f_{\theta}(X)\) distribution over its domain.
The second part is the area under the curve of the component density
function maximised on \(\mathbb{R}\), with \(j\) the index of the component
maximised at that point. It should be noted that the definition used here for the overlap is closely related to the definition of the \emph{false clustering rate} (FCR) (Marandon et al. 2022).

Equation \eqref{eq:general-overlap} simplifies for a two component
mixture distribution to Equation \eqref{eq:two-component-overlap}:

\begin{equation}
    \operatorname{OVL} (X)= \int_{\mathbb{R}} \min \left(p_1 \varphi_{\zeta_1} (x), p_2 \varphi_{\zeta_2} dx (x)\right)
\label{eq:two-component-overlap}
\end{equation}

From a probabilistic point of view, we can rewrite Equation
\eqref{eq:two-component-overlap} as the overall probability of assigning
a wrong label to a given observation. With two components, this simply
decomposes as the sum of the probability of mistakenly assigning an
observation from component 2 to component 1 and the probability of
assigning an observation from component 1 to component 2 Equation
\eqref{eq:misclassification-prob}:

\begin{equation}
\begin{split}
\operatorname{OVL} (1, 2) & = \operatorname{OVL} (1 | 2) + \operatorname{OVL} (2 | 1) \\
& = \mathbb{P}\left(p_1 \varphi (X, \mu_1, \sigma_1) \le p_2 \varphi (X, \mu_2, \sigma_2) \right) + \mathbb{P}\left(p_2 \varphi (X, \mu_2, \sigma_2) \le p_1 \varphi (X, \mu_1, \sigma_1)\right) \\
& = \int_{\mathbb{R}} p_1 \varphi_{\zeta_1} (x) \mathrm{1}_{p_1 \varphi_{\zeta_1}  \le p_2 \varphi_{\zeta_2}} dx + \int_{\mathbb{R}} p_2 \varphi_{\zeta_2} (x) \mathrm{1}_{p_2 \varphi_{\zeta_2} \le p_1 \varphi_{\zeta_1}} dx
\end{split}
\label{eq:misclassification-prob}
\end{equation}

We illustrate the computation of the overlap in some hard-hitting cases
below, showing relation between the level of entropy and the individual
standard deviations with the overlap measured in Figure
\ref{fig:OVL-represention-pdf}.
Means of component 1 and 2 are 5.28 and 8.45. Panels A and C correspond
to balanced classes, while in panel B and D, class 1 is more abundant
with a frequency of 0.9. Finally, in panels A and B, the variance of
component 1 is smaller than the variance of component 2 with respective
SDs of 1 and 3 and reciprocally for panels B and D. Interestingly, in
panel D, using the MAP as defined in Equation \eqref{eq:posteriori}, all
observations issued from class 2 are wrongly assigned to class 1. The
red area corresponds to the probability of misclassifying component 1 as
component 2, while the green area corresponds to the probability of
misclassifying component 2 as component 1. Total overlap is since the
sum of red and green area, in Figure
\ref{fig:OVL-represention-pdf}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{chassagnol-becht-nuel-benchmark-of-Gaussian-mixtures_files/figure-latex/OVL-represention-pdf-1} 

}

\caption{Illustration of the overlaps between a two-components GMM. Density function of component 1 is given by the red line, its of component 2 by the green line, and total density function $f_\theta(X)$ is represented in blue. The total overlap is given by the sum of the green and red areas.}\label{fig:OVL-represention-pdf}
\end{figure}

There are two intersection points, \(x_1\) and \(x_2\) , with
\(\mu_1 < \mu_2\) when solving equation Equation
\eqref{eq:intersection-points}:

\begin{equation}
p_1 \varphi(x, \mu_1, \sigma_1)  = p_2 \varphi(x, \mu_2, \sigma_2)
\label{eq:intersection-points}
\end{equation}

in following case: if \(\sigma_2 > \sigma_1\), then we must have
\(p_1 > \frac{\sigma_1}{\sigma_1 + \sigma_2}\), else if
\(\sigma_2 < \sigma_1\), then
\(p_1 < \frac{\sigma_1}{\sigma_1 + \sigma_2}\). In that case, they are
given by following formula Equation \eqref{eq:roots}:

\begin{equation}
\begin{split}
(x_1, x_2) = \left(\frac{\sigma_1^2 \mu_2 - \sigma_2^2 \mu_1 \pm \sigma_1 \sigma_2 \sqrt{(\mu_1 - \mu_2)^2 + 2(\sigma_2^2 - \sigma_1^2)\left[ \log (\frac{p1}{p2}) + \log (\frac{\sigma_2}{\sigma_1})\right]}}{\sigma_1^2 - \sigma_2^2}\right)
\end{split}
\label{eq:roots}
\end{equation}

Again, sign of term \(A\) and order of the roots yield two several cases,
depending whether \(\sigma_1\) is greater or not than \(\sigma_2\). Both
situations with unbalanced classes are illustrated in panel B and D on
Figure
\ref{fig:OVL-represention-pdf}:

\begin{itemize}
\tightlist
\item
  When \(\sigma_1 < \sigma_2\), then \(x_2 < x_1\) and
  \(p_1 \varphi(x, \mu_1, \sigma_1) < p_2 \varphi(x, \mu_2, \sigma_2)\)
  on interval \([x_2, x_1]\). Hence, total overlap is given by Equation
  \eqref{eq:OVL-general-case1}:
\end{itemize}

\begin{equation}
\begin{split}
\operatorname{OVL} (1, 2) = p_1 \left(\Phi(x_2, \mu_1, \sigma_1) +1 - \Phi(x_1, \mu_1, \sigma_1) \right) + p_2 \left( \Phi(x_1, \mu_2, \sigma_2) -\Phi(x_2, \mu_2, \sigma_2) \right)
\end{split}
\label{eq:OVL-general-case1}
\end{equation}

\begin{itemize}
\tightlist
\item
  When \(\sigma_1 > \sigma_2\), then \(x_1 < x_2\) and
  \(p_1 \varphi(x, \mu_1, \sigma_1) < p_2 \varphi(x, \mu_2, \sigma_2)\)
  on interval \([x_1, x_2]\). Hence, total overlap is given by Equation
  \eqref{eq:OVL-general-case2}:
\end{itemize}

\begin{equation}
\begin{split}
\operatorname{OVL} (1, 2) = p_2 \left(\Phi(x_1, \mu_2, \sigma_2) +1 - \Phi(x_2, \mu_2, \sigma_2) \right) + p_1 \left( \Phi(x_2, \mu_1, \sigma_1) -\Phi(x_1, \mu_1, \sigma_1) \right)
\end{split}
\label{eq:OVL-general-case2}
\end{equation}

An interesting result is obtained with the homoscedascity and balanced
classes' assumptions of the \emph{k}-means algorithm. There is only one
intersection point in that case: \(x_c= \frac{\mu_1 + \mu_2}{2}\), that is
simply the centre of the segment bounded by the means of the two
components. The overlap is simply then
\(\operatorname{OVL} (1, 2) = 2 \Phi (- \frac{|\mu_1 - \mu_2|}{2\sigma})\).

To our knowledge, no closed formula has been determined returning the overlap generalised to more than two components (combinatorial set of inequations to solve), in the unconstrained multivariate setting (cubic equation to solve in bidimensional space). Indeed, even restraining the study to the bivariate setting (the calculation of the OVL then amounts to estimating the zone of intersection between two ellipses), the exact computation of the OVL involves multiple integration and the algebraic resolution of a quartic equation. A first step is provided by (Alberich-Carramiñana, Elizalde, and Thomas 2017), stating algebraic conditions for the existence of an intersection region and computing where applicable a closed formula of the OVL between two coplanar ellipses.

Accordingly, only stochastic approximations, relying on randomised algorithms, such as the Monte-Carlo integration with a rejection technique (knowing that the total area under the curve is normalised to one, we randomly simulate observations and the ratio of the number of observations falling in the intersection area is then used as a proxy of the overlap), are available so far (Maitra and Melnykov 2010; Pastore and Calcagnì 2019; Nowakowska, Koronacki, and Lipovetsky 2014).

\hypertarget{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{%
\section{Appendix B: Extensions of the EM algorithm to overcome its limitations}\label{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}}

Two main alternatives were developed in parallel to the EM algorithm and
are implemented in some of the reviewed packages: the CEM and the SEM
algorithm. However, they do not have its theoretical properties,
especially guarantee of the consistency of the algorithm.

The M-step of the \emph{classification EM} (CEM) algorithm
(Biernacki, Celeux, and Govaert 2000) maximises a function where each observation was
assigned to the maximum a posteriori (MAP) estimate Equation
\eqref{eq:posteriori}. It generalises the well-known \emph{k}-means algorithm
making no assumption of homoscedascity or equibalanced clusters. Its
main drawback is to not take into account uncertainty of the cluster
assignment, inducing \emph{inconsistency} of the algorithm
(G. McLachlan and Peel 2000). EM*, referred in Kurban, Jenne, and Dalkilic (2017) and implemented
in the \CRANpkg{DCEM} package, is a faster implementation of the CEM
algorithm, with roughly a twice smaller complexity. To do so, only the
posterior distributions associated to the lower half of the most
uncertainly assigned observations are re-computed in the E-step of the
EM-algorithm. This normally avoids to recompute data that is unlikely to
change of cluster attribution from an iteration to another. However, the
higher speed of this algorithm has not been theoretically proven, as the
gain of running time per iteration of the algorithm may be alleviated by
a greater number of steps to reach the convergence.

The \emph{Stochastic EM} (SEM) replaces the MAP value for \(S\) in the E-step
of the CEM algorithm by a random draw (or \(N\) of them in the \emph{N-}
variant of the algorithm) of the posterior distribution
\(\mathbb{P}_\theta (S|X)\). As this algorithm does not converge to a
unique solution, but rather oscillates around a local maximum, the
estimation is usually performed by averaging the late estimated values
while ignoring the first estimates from the \emph{burn-in phase}. A
theoretical description of these algorithms, with discussion on their
convergence properties, is detailed in Celeux and Govaert (1992). SEM algorithm
has also a relatively faster convergence than EM algorithm but it is
more proned to be trapped in a local maximum or to remove a component.
Increasing the number of draws \(N\) may alleviate this issue, but at the
extent of computational performances.

A wide variety of fast algorithms derived from the EM algorithm have
been developed. cwEMM (component-wise EM algorithm), described in
Celeux, Chrétien, and Forbes (2012), is a variation of the EM algorithm aiming at speeding up
its convergence. The M-step at each iteration is only performed for one
of the components \(\theta_j=(p_j, \mu_j, \sigma_j)\), implying that the
parameters of a given component are estimated sequentially rather than
simultaneously. The theory behind relies on a \emph{Gauss-Seidel} scheme and
was first used by the SAGE algorithm. However, the constraints on the
proportions set in Equation \eqref{eq:2} are only guaranteed if the
algorithm converges. Additionally, faster convergence is not
theoretically proven for any situation. A list of general acceleration
methods for the EM algorithm, not specific to GMMs, is available on
\CRANpkg{turboEM} (Bobb and Varadhan 2021).

Other EM-inspired algorithms focus on counterbalancing the main
limitations of the EM algorithm. The \emph{Variational Bayesian EM} (VBEM)
algorithm performs a Bayesian estimation of the parameters. Indeed, the
large space of all possible parameter estimates \(\Theta\) can be hard to
explore and the usual initialisation methods are uninformative, not
taking into account expert recommendations. VBEM uses these prior
assumptions on the parameters' distribution \(\mathbb{P} (\theta)\) to
optimise the posterior distribution \(\mathbb{P} (\theta|X)\), based on
Bayes' rule. Direct determination of the Bayesian posterior law of the
parameters is generally an intractable problem, hence Variational Bayes
only maximises an approximation of the true posterior, assuming that the
parameters can be partitioned in independent distributions. This
hypothesis is known as \emph{mean-field approximation} (Murphy 2012).

The minimum message length (MML) EM algorithm, implemented in the
\pkg{GMKMcharlie} package, is a completely unsupervised algorithm as it
does not require any prior selection of the number of components
(Figueiredo and Jain 2002) , by dealing explicitly with the possibility of
discarding a component during the iteration. To do so, the selection
criteria for the number of components is directly included in the
optimisation procedure. However, its implementation is close from a
Bayesian estimation of the parameters of the model, setting a
non-informative Dirichlet prior distribution on the ratios and the
higher expected performances of the algorithm are not demonstrated on
real use cases (Figueiredo and Jain 2002).

The Expectation/Conditional Maximisation (ECM) (MENG and Rubin 1993) belongs to the super-family of GEM (general EM) algorithms, generally used when the maximisation of the auxiliary function yields a non-closed form to solve. To do so, the ECM algorithm replaces the intractable M-step of the EM algorithm by a number of computationally simpler conditional maximization (CM) steps (instead of inferring all parameters at once, the conditional step retrieves a set of optimal parameters, conditioned by the current value of the others). ECM is for instance used with GMMs including an additional linear constraint on the means of the components, as provided by the \pkg{mixtools}
package. As documented in Table \ref{tab:high-comparison-packages-pdf}, \pkg{EMMIXmfa} implements an extension of the ECM, termed alternating expectation--conditional maximization (AECM) algorithm (X.-L. Meng and Van Dyk 1997), and which can be used to reduce the computational burden of estimating the parameters of mixtures of factor analysers. The AECM algorithm is an extension of the ECM algorithm that allows the complete data used for estimation to differ on each CM-step (generally, in order to speed the computation, by selecting a subset of the most leveraged observations). GEM algorithms share the same asymptotic theoretical properties of the EM algorithm, especially the local consistency of the estimates returned.

\hypertarget{a-small-simulation-to-evaluate-the-impact-of-outliers}{%
\subsection{A small simulation to evaluate the impact of outliers}\label{a-small-simulation-to-evaluate-the-impact-of-outliers}}

Classical methods used for the parameters' estimation, especially the
maximum likelihood estimation (MLE), are sensitive to the presence of
outliers. A naive solution consists in assigning null weights to
observations suspected to be outliers, so that they do not contribute
\footnote{The use of weighted distributions has more general applications.
  It can be used to deal with a component distribution that does not
  fit exactly a Gaussian shape. For instance, to deal with heavy tail
  distributions, more weight can be given to central components and
  less weight to the tails.}. Trimming aberrant observations from the distribution is justified
theoretically by the principle of the \emph{spurious outlier model}
(Gallegos and Ritter 2005). However, this method is quite stringent,
requiring human expertise or the use of general outlier detection tools
not necessarily adapted to GMM estimation.

Two general approaches for dealing with outliers with a well-defined
theoretical background are the \emph{outliers mixture modelling} and the
\emph{trimming approach}. \emph{Outliers mixture modelling} integrate an
additional component accounting for the outliers in the distribution.
Notably, the \pkg{mclust} (Fraley, Raftery, and Scrucca 2022) and \CRANpkg{otrimle} (Coretto and Hennig 2021)
packages use an improper uniform distribution to model the distribution
of outliers. Unlike \pkg{mclust}, the \pkg{otrimle} package does not
require the user to set in advance the proportion of outliers in the
mixture (Coretto and Hennig 2016). As opposed, in the \emph{trimming approach},
outliers are first removed before the complete estimation of parameters.
Such methods are implemented in \CRANpkg{tclust} (Iscar, Escudero, and Fritz 2022) and
\CRANpkg{oclust} (Clark and McNicholas 2019) packages.

\CRANpkg{tclust} (Iscar, Escudero, and Fritz 2022) uses a robust constrained clustering
method, where the user has to set an upper threshold to the ratio
between the highest and the lowest variability among all components and
a trimming ratio \(\alpha\). It extends the work of
García-Escudero et al. (2008), with released constraints on the Gaussian
distribution. First, the maximal degree of affinity, defined in Equation
\eqref{eq:affinity-degree}:

\begin{equation}
    D(x_i|\theta)=\max_j \left(p_{j} \varphi_{\zeta_j} (x_i) \right)
\label{eq:affinity-degree}
\end{equation}

is computed for each observation \(x_i\), and corresponds for each point
to the maximum probability to observe it in the distribution, given
parameter \(\theta\). Then, \(\alpha\) observations the least likely to be
observed are trimmed for the estimation of the parameters. When we reach
convergence of the estimated parameter and there is no change in the
outliers identification from one iteration to another, the iterative
algorithm stops. The use of constraints is an additional feature that
avoids building over-dispersed or unbalanced clusters, the highest
constraint of a ratio of 1 yielding clusters with equal sizes. However,
the identification of an observation as aberrant is highly dependant on
the variability constraint and the determination of these two
hyperparameters is complex and highly dependant on the shape of the
distribution. Additionally, a CEM algorithm is used to retrieve the
parameters and the proportion of outliers, for which the MLE, in
contrast to the EM algorithm, is not asymptotically consistent nor
efficient.

Unlike \pkg{tclust}, \CRANpkg{oclust} (Clark and McNicholas 2019) both retrieves the
proportion of outliers and identifies them. To do so, it compares the
complete log-likelihood of the mixture \(\ell(\theta|X)\) with its value
removing one observation \(\ell(\theta | X \setminus X_i)\), for all
observations. Observations are iteratively removed, based on the
assumption that the Kullback-Leibler divergence between the original
log-likelihood and the trimmed log-likelihood
\(\text{KL}\left(\ell(\theta|X)|| \ell(\theta | X \setminus X_i)\right)\)
follows a Beta distribution. At each step, the observation that
maximises the Kullback-Leibler divergence at a statistically significant
threshold is removed. The algorithm stops trimming outliers, when this
measure is not anymore statistically significant. However, the
assumption of a Beta distribution only holds asymptotically and with
non-overlapping clusters.

To integrate the impact of outliers in the estimation, we simulated a
two-components GMM with well-separated and balanced clusters. The
outliers distribution, corresponding to the additional noise component,
was retrieved by randomly selecting \emph{prop.outliers} points out of the
total number of observations and drew their values from an uniform
distribution bounded by an interval five times as big as the 0.05 and
0.95 quantiles of \(f_\theta(X)\). All estimates were obtained comparing
the five reviewed initialisation methods, except with \pkg{otrimle}
which has its own hierarchical clustering initialisation method.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/univariate/outliers} 

}

\caption{A) Execution times for the nine reviewed packages using hierarchical clustering initialisation, with on the left $2\%$ of outliers in proportion and on the right, $4\%$ of outliers.  B) and C) Boxplots of the estimated parameters with $N=200$ repetitions, $n=2000$ observations and respectively $2\%$ and $4\%$ of outliers. The red dashed horizontal line corresponds to the true value of the parameters.}\label{fig:outliers}
\end{figure}

The slowest package is \pkg{otrimle}, most of the time being taken by
the initialisation step where proportion and identification of the
outliers is performed. Running times of the other packages are generally
not impacted by the presence of outliers.

Most of the reviewed packages, except the \pkg{bgmm} package, are not
impacted by the choice of initialisation method. Additionally, the
proportions are rather correctly estimated (related to the choice of an
uniform distribution to model outliers), but the reviewed packages tend
to overestimate the true variability of each component, with the worst
results obtained with \pkg{rebmix} initialisation. \pkg{bgmm} sets apart
from the others by its reduced bias on the means and standard deviations
estimated, a feature left undocumented. However, increasing the number
of outliers (Figure \ref{fig:outliers}, panel C) lead also to biased
estimations for \pkg{bgmm}, while \pkg{otrimle}, a dedicated package, is
still able to correctly estimate the individual parameters of the
components' distributions with a high proportion of outliers. Yet,
analysing the code used to implement the \pkg{bgmm} reveals that there
is no dedicated feature to remove outliers but rather a specific method
used to deal with numerical underflow that artificially increases the
probability of observing outlying distributions (\protect\hyperlink{em-implementation-differences-across-reviewed-packages}{EM-implementation
differences across reviewed packages}).

\hypertarget{appendix-c-the-meta-analysis-workflow-for-the-final-selection-of-cran-and-bioconductor-platforms}{%
\section{Appendix C: the meta-analysis workflow for the final selection of CRAN and Bioconductor platforms}\label{appendix-c-the-meta-analysis-workflow-for-the-final-selection-of-cran-and-bioconductor-platforms}}

\hypertarget{general-workflow}{%
\subsection{General workflow}\label{general-workflow}}

Table \ref{tab:searched-packages-pdf}
lists the terms used in the search, the number of packages returned by
the search, the number of packages excluded from review after the
search, and the names of the packages ultimately selected for review.
Indeed, the CRAN and Bioconductor platforms are the two most popular
repositories for R packages, with a constraining review before
publication.

Most packages we excluded from review did not focus on the GMM model,
but on supplying tools for visualising and asserting the quality of a
given clustering. For instance, the search term ``cluster'' returned many
packages implementing other unsupervised clustering methods, such as
\emph{k}-means, KNN or graph clustering, were specifically dedicated to
specific data, such as single cell analyses. The search term ``mixture''
returned either packages dealing only with non-Gaussian components, such
as \CRANpkg{fitmix} with log-normal distributions or were dedicated to chemical mixture designs.

\begin{table}[!h]

\caption{\label{tab:searched-packages-pdf}Meta-analysis summary about the selection of packages implementing the estimation of GMMs,
              on CRAN and Bioconductor.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{}l>{}l>{}r>{}r>{}l>{}l}
\toprule
\multicolumn{1}{c}{\textbf{Platforms}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Searched\\terms}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Number of\\returned packages}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Number of packages\\implementing GMMs}}} & \multicolumn{1}{c}{\textbf{Packages implementing GMMs}} & \multicolumn{1}{c}{\textbf{Packages kept}}\\
\midrule
\midrule
\multicolumn{1}{c}{Bioconductor} & \multicolumn{1}{c}{mixture} & \multicolumn{1}{c}{15} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{epigenomix, fmrs, semisup} & \multicolumn{1}{c}{$\varnothing$}\\
\midrule
\multicolumn{1}{c}{Bioconductor} & \multicolumn{1}{c}{cluster} & \multicolumn{1}{c}{69} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{Melissa} & \multicolumn{1}{c}{$\varnothing$}\\
\midrule
\multicolumn{1}{c}{CRAN} & \multicolumn{1}{c}{mixture} & \multicolumn{1}{c}{179} & \multicolumn{1}{c}{44} & \multicolumn{1}{c}{\makecell[r]{AdaptGauss, bgmm\\bmixture, bpgmm, CAMAN, ClusterR, deepgmm\\DPP, dppmix, EMCluster, EMMIXgene\\EMMIXmfa, fabMix, \\flexmix, fmerPack, GMKMcharlie, IMIX, ManlyMix\\mclust, MGMM, mixAK, MixAll, mixdist, mixR\\mixreg, mixsmsn, mixtools, mixture\\MixtureInf, MMDvariance, nor1mix\\pcensmix, pgmm, pmclust, polySegratioMM\\rebmix, Rmixmod, RMixtComp\\RobMixReg, RPMM, SAGMM, sensory, SMNCensReg}} & \multicolumn{1}{c}{\makecell[r]{bgmm, EMCluster\\flexmix, GMKMcharlie\\mclust, mixtools, Rmixmod}}\\
\midrule
\multicolumn{1}{c}{CRAN} & \multicolumn{1}{c}{cluster} & \multicolumn{1}{c}{418} & \multicolumn{1}{c}{16} & \multicolumn{1}{c}{\makecell[l]{ClusterR, clustMD, DCEM, EMCluster, HDclassif\\ManlyMix, mclust, mixAK, MixAll\\mixture, oclust, otrimle, pmclust, rebmix\\Rmixmod,  tclust}} & \multicolumn{1}{c}{\makecell[l]{ EMCluster\\mclust, Rmixmod}}\\
\midrule
\bottomrule
\end{tabular}}
\end{table}

At this stage, too many packages for a tractable benchmark remained. We
hence perform stricter selection of them, based on the following
criteria:

\begin{itemize}
\item
  Some of the packages did not implement the unconstrained GMM (no
  constraint of homoscedascity or equibalanced proportions). Hence,
  \BIOpkg{epigenomix} (Klein and Schaefer 2022) , \CRANpkg{EMMIXgene}
  (Andrew Thomas Jones 2020) , \CRANpkg{pcensmix} (Fallah and Hinde 2017) , \CRANpkg{mixAK}
  (Komárek 2022) (homoscedastic components), \CRANpkg{mixture}
  (Pocuca, Browne, and McNicholas 2022) (multi-dimension only), \CRANpkg{AdaptGauss}
  (Thrun, Hansen-Goos, and Ultsch 2020) and \CRANpkg{MMDvariance} (X. Li et al. 2018) add
  constraints on the number of components, on the standard deviation
  of each component or on mean values of each population, leaving no
  choice to the user to remove such assumptions. \BIOpkg{semisup}
  (Rauschenberger 2022) restrains on mixtures with two components, for which a
  part of the observations are labelled. Additionally, it is designed
  for GWAS or differential analyses. Other packages were designed to
  deal with high-dimensional data, projecting the data on a smaller
  subspace using a factor analysis model. Hence, these packages can
  not learn a GMM for an univariate distribution, as we can not
  project on a smaller space than the unidimensional space. This led
  to the exclusion of \pkg{HDclassif},
  \CRANpkg{fabMix} (Papastamoulis 2020) , \pkg{EMMIXmfa} and
  \CRANpkg{pgmm} (Paul D. McNicholas et al. 2022) packages. The \CRANpkg{sensory}
  (Franczak, Browne, and McNicholas 2016) package both imputes missing data and performs factor
  regression on a subspace up to 3 dimensions at most, but requires
  the user to provide its own initial estimates. \color{blue} Alternatively, \CRANpkg{clustvarsel} (Dean, Raftery, and Scrucca 2020) discards the least informative variables, in an attempt to find a locally optimal subset of variables that best discriminate clusters. \color{black}
\item
  We assume that our original data is continuous. However, some
  packages are dedicated to deal with discrete data, for instance
  binned size distributions of medical patients. This led to the
  exclusion of \CRANpkg{mixdist} (Macdonald and Juan Du 2018).
\item
  We restrained our review to packages that use the classic EM
  algorithm, using maximum likelihood estimation to retrieve the
  parameters of GMMs. For instance, some packages offer a Bayesian
  estimation of the parameters of the model using MCMC methods, such
  as \CRANpkg{bmixture} Mohammadi (2021){]}, \CRANpkg{bpgmm} (Lu, Li, and Love 2022), \CRANpkg{DPP} (Avila, May, and Ross-Ibarra 2018)
  , \CRANpkg{dppmix} (Y. Xu et al. 2020), \CRANpkg{BayesCR} (Garay et al. 2017) and
  \BIOpkg{Melissa} (Kapourani 2022). \CRANpkg{polySegratioMM} (Baker 2018) uses the Bayesian
  framework JAGS's interface in R. Alternatively, other algorithms
  focusing on maximising the likelihood do exist, but rely on
  different statistical methods, such as \CRANpkg{RPMM} (Houseman et al. 2017)
  which implements a recursive algorithm, and \CRANpkg{SAGMM}
  (Andrew T. Jones and Nguyen 2019) offering a stochastic approximation.
\end{itemize}

We then removed the packages in which the MLE estimation of the
unconstrained GMM model was an ancillary task:

\begin{itemize}
\item
  We removed the packages that focus on learning mixture of Gaussian
  regressions such as \BIOpkg{fmrs} (Shokoohi 2022) , \CRANpkg{mixreg}
  (Turner 2021) or \CRANpkg{fmerPack} (Y. Li and Chen 2021) , an extension of
  the \CRANpkg{flexmix} package with an additional feature selection
  using the lasso method. \CRANpkg{nlsmsn} (Prates, Lachos, and Garay 2021) implements
  regression of skewed Gaussian mixtures, but in unidimensional space
  only. \CRANpkg{RobMixReg} (S. Cao, Chang, and Zhang 2020) performs robust regression
  of Gaussian mixtures using five several methods: CTLERob, a
  component-wise adaptive trimming likelihood estimation; mixbi,
  bi-square estimation; mixL, Laplacian distribution; mixt,
  t-distribution; TLE, trimmed likelihood estimation, and flexmix
  which only performs flexmix regressions with multiple random starts.
\item
  Some packages were built to deal with highly specific tasks.
  \CRANpkg{RMixtComp} (Kubicki, Biernacki, and Grimonprez 2021) and \CRANpkg{clustMD}
  (McParland and Gormley 2017) deal with mixed data (continuous + discrete). The
  \CRANpkg{deepgmm} (Viroli and McLachlan 2020) package learns deep Gaussian mixture
  models, generalising the classical GMM with multiple layers.
  \CRANpkg{IMIX} (Wang 2022) focuses on clustering multi-omic data that
  is learnt with the \pkg{mclust} package, and \BIOpkg{coseq}
  (Rau 2022) implements RNA-Seq transcriptome clustering using the
  \CRANpkg{Rmixmod} package.
\item
  Some extend the EM algorithm on Gaussian distributions and overcome
  its main limitations. The \CRANpkg{MGMM} (McCaw 2021) package deal with
  missing data, which is not relevant in unique dimension. The
  \pkg{mixsmsn} package estimates skewed GMMs. \CRANpkg{SMNCensReg}
  (Garay, Massuia, and Lachos 2022) fit univariate right, left or interval censored
  data. Some packages offer a robust implementation of the algorithm,
  automatically trimming possible outliers. \pkg{otrimle} models the
  presence of outliers by an extra component following an improper
  uniform distribution, while \pkg{tclust} and \pkg{oclust}
  automatically removes possible outliers before the estimation step
  (\protect\hyperlink{a-small-simulation-to-evaluate-the-impact-of-outliers}{A small simulation to evaluate the impact of outliers}).
\item
  We also removed packages that were limited in their functionalities
  or complex to install. Indeed, \CRANpkg{ClusterR} (Mouselimis 2022)
  (\emph{k}-means), \pkg{rebmix} (REBMIX), \CRANpkg{nor1mix} (univariate
  dimension only, wrong initialisation process), \CRANpkg{MixAll}
  (Iovleff 2019) (random and small EM) do not allow to perform the EM
  algorithm with its own initial estimates. The function to provide its own initial estimates for the \textbackslash pkg\{DCEM{]} package is only internal, and not supposed to be available for the common user. \CRANpkg{pmclust} (W.-C. Chen and Ostrouchov 2021) depends on
  the availability of the OpenMPI framework for its parallelised
  implementation of the EM algorithm.
\item
  We also removed the \CRANpkg{MixtureInf} (S. Li, Chen, and Li. 2016) ,
  \CRANpkg{mixR} (Yu 2021) and \CRANpkg{CAMAN} (Schlattmann, Hoehne, and Verba 2022) packages
  which have not been updated in the last two years or still in beta
  version.
\end{itemize}

The popularity of the selected packages varies largely, as shown in
Figure
\ref{fig:trend-package-pdf}.
Among them, \pkg{mclust} and \pkg{flexmix} are largely the most popular,
followed by \pkg{mixtools} and increasingly popular \pkg{Rmixmod}
package.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{chassagnol-becht-nuel-benchmark-of-Gaussian-mixtures_files/figure-latex/trend-package-pdf-1} 

}

\caption{Number of daily downloads (logarithmic scale) from the CRAN mirror from the $1^\text{st}$ of January to the $30^{\text{th}}$ April 2022 for the seven R packages reviewed.}\label{fig:trend-package-pdf}
\end{figure}

\color{blue}

Only the packages dedicated to high-dimensionality, listed in our first bullet point, are relevant to benchmark their performance as a function of the number of dimensions. Indeed, although some packages computing mixtures of regressions do implement features allowing to to handle high-dimensional datasets, such as \pkg{RobMixReg} and \pkg{fmerPack}, they all assume a diagonal covariance structure, and accordingly independent covariates.

The two existing strategies are then limited to projection to a smaller subspace, usually within the theoretical framework of factor analysis or to perform a feature selection strategy.\\
We quickly discarded \pkg{fabMix}, since it only retrieves the parameters of GMMs within a Bayesian framework, while we focused on strategies retrieving the MLE via the EM algorithm. The core function \code{pgmmEM} in the \pkg{pgmm} package unfortunately includes a seed for the the algorithm's initialisation that cannot be disabled. Such a feature is generally not recommended for reproducibility, since by defining the seed internally in the function, we were not able to independently generate new reproducible datasets in our benchmark (instead, it is recommended to set the seed value once and for all at the beginning of the virtual simulation). Additionally, while implementing the same AECM variant of the EM algorithm as \pkg{EMMIXmfa}, as detailed in \protect\hyperlink{appendix-b-extensions-of-the-em-algorithm-to-overcome-its-limitations}{Appendix B: Extensions of the EM algorithm to overcome its limitations}, its convergence criteria differs from the other benchmarked packages. Indeed, instead of considering a limiting number of iterations along with a prior threshold, either \emph{absolute} or \emph{relative}, it examines only the difference between the current value of the log-likelihood and a corresponding asymptotic estimate, based on the Aitken acceleration (Lindsay 1995). In brief, the asymptotic value of the log-likelihood is the limiting sum of a geometric series, whose common ratio, the so-called Aitken acceleration, is the relative fraction of the log-likelihood gain of the current iteration. Therefore, the use of a different termination criterion precludes any further fair comparison with the other benchmarked packages, as there is no direct equivalence between the two methods.

Finally, \pkg{clustvarsel} is not really tailored for datasets with a large number of dimensions, but rather for datasets with a small number of observations. Indeed, by performing a sequential search in the model space in a forward-backward process (namely by adding variables to the null model till we recover the full model, with all features), the algorithm requires intensive computational resources (for instance, there are already \(2^{10}=1024\) models to be tested in dimension 10). Instead of a greedy strategy, it would have been interesting to implement a in independent and highly parallelizable feature selection process by sampling the model space. To do so, (J. Chen and Chen 2008; He and Chen 2016) suggests a stochastic and greedy feature selection strategy, using notably the \emph{eBIC} criteria in order to have an equal chance to draw a model of any dimension\footnote{Indeed, by simply uniformly sampling among the \(2^D\) models available, the probability of getting models with \(D/2\) features is much higher than drawing models at the boundaries, displaying either few or close to \(|D|\) covariates.}. Such a strategy is commonly used in \emph{ensemble learning}.

\color{black}

\hypertarget{practical-details-for-the-implementation-of-our-benchmark}{%
\subsection{Practical details for the implementation of our benchmark}\label{practical-details-for-the-implementation-of-our-benchmark}}

\label{subsec:benchmark-details}

First, the number of observations (\(n=200\) and \(n=500\) respectively in the univariate and bivariate setting) was chosen enough high to both lower the probability of generating a sample without drawing any observation from one of the components in case of highly-unbalanced clusters and decreases the \emph{margin of error} related to the random sampling error. Specifically, the probability of generating at least one simulation among the \(N\) generated fo which less than two observations proceed from component \(j\) (the minimal number of elements required to estimate both the mean and the variance of the corresponding cluster), with a two-components mixture of \(n\) observations, is given by the following formula (Equation \eqref{eq:random-sampling-risk}):

\begin{equation}
1 - \left(1 - (1-p_j)^n - n \times (1-p_j)^{n-1}\times p_j\right)^N
\label{eq:random-sampling-risk}
\end{equation}

Interestingly, the probability of generating a sample among the \(N\) repetitions increases exponentially as the level of imbalance increases. For instance, considering \(N=100\) repetitions, \(n=200\) observations per sample and proportion for the minor component \(p_j=0.1\), the probability of generating a degenerate simulation is insignificant: \(1.63 \times 10^{-6}\) while the risk considerably increases, keeping the same general simulation parameters and setting minor proportion to \(p_j=0.05\), with a probability of \(0.04\). \color{blue} We have focused on one of the impacts of high dimensionality, namely that related to the homogenisation and convergence of any distance norm and the increase in sparsity in relation with the number of features added. We deliberately do not consider the case where the number of dimensions exceeds the number of observations (namely, when \(D>n\)), since in this configuration, the covariance matrix is no longer of full rank and invertible, implying that the corresponding probability distribution does spans completely over a smaller subspace. However, with so few observations, (\(n=200\) in scenarios identified as a), we reveal the impact in terms of the quality of the estimation when the number of observations is closed to the number of free parameters required to parametrise the full GMM model (with \(k=2\) clusters and \(D=10\) dimensions, \(k \times \frac{D(D+1)}{2} + kD + 1 = 131\) are needed.).\color{black}

Unless stated explicitly, we keep the default hyper-parameters and custom global options provided by each package. For instance, the \pkg{flexmix} package has a default option, \emph{minprior}, set by default to 0.05 which removes any component present in the mixture with a ratio below \(0.05\). Besides, we only implement the fully unconstrained model in both univariate and multivariate settings, as it is the only parametrisation implemented in all the seven packages and the most popular to perform classic GMM clustering, as no restrictive and difficult-to-test assumptions are required.

\color{blue} Additionally, as stated in \protect\hyperlink{parameters-estimation-in-a-high-dimensional-context}{Parameters estimation in a high-dimensional context}, the intrinsic dimension \(d_j\) for each cluster \(j\) is a hyperparameter, which is generally inferred independently from the GMM estimation itself. While a variety of methods from the field of factor analysis, enumerated in \href{https://en.wikipedia.org/wiki/Factor_analysis\#Criteria_for_determining_the_number_of_factors}{Factor criteria selection}, have been developed to estimate the intrinsic dimension, to our knowledge, only two of them have been implemented in CRAN packages: the \emph{Cattell's scree-test} (Cattell 1966) or the dimension selection graph using one of the \emph{penalty metric} discussed in the appendix \protect\hyperlink{model-selection}{Model selection} (Bergé, Bouveyron, and Girard 2012). However, while \pkg{HDclassif} natively implements a performance criterion method for determining the dimension of the spanning space, performed under the hood by function \code{mixsmsn::hdcc}, none of the other packages evaluated implemented a dimension selection feature. Instead, we infer it for each of the packages dedicated to high-dimensionality with HDclassif, using using the so-called model ``AkjBkQkD'', for which the intrinsic dimension is common to all components but the characteristics unique for each component Finally, we use among all supplied parametrisations, the least constrained one. Namely, we used the model ``AkjBkQkDk'' with HDclassif, in which not only the individual features of the covariance matrix but also the spanning dimension are unique for each cluster, and function \code{mcfa} of the \pkg{ EMMIXmfa} package, in which the \emph{transition matrix} is common to all components (referred to as the orientation matrix in Appendix \protect\hyperlink{parameters-estimation-in-a-high-dimensional-context}{Parameters estimation in a high-dimensional context}.
\color{black}

If all the seven reviewed packages accept initial estimates provided by the user, both the input and the output format differ between them, requiring an intensive processing to standardise both the initial estimates input, and the output estimates. Notably, a well-known issue with the mixture models is that they identifiable up to a permutation of the components (alternatively, changing the index of the labels do not change the likelihood of the model). Assigning one component of the mixture to a specific index is generally immaterial, as the main objective is to return the estimates. However, when it comes to compare the estimated parameters with the true estimates, we must associate unequivocally each component to a specific index. To do so, we set a partial ordering, sorting the components by increasing order of their mean components. Actually, if the ratio or the covariance estimates can be equal for all the components, it is generally not the case for the centroids, as this would result into a degenerate distribution. The consequence and some illustrations of the non-identifiability of the mixture distributions are discussed in section \href{https://stats.stackexchange.com/questions/265898/why-is-a-normal-mixture-model-not-identifiable-and-why-does-it-matter}{Identifiability of finite mixture models}, in Dai and Mukherjea (2001) and in Book Robert and Casella (2010).

We detail below some additional functions we implement to both homogenise input and output of the packages and ease the user's task when comparing the performance of these packages:

\begin{itemize}
\item
  The input observations, mean and covariance matrices have to be transposed compared to the conventional format in packages \pkg{bgmm}, \pkg{EMCluster}, \pkg{GMKMcharlie} and \pkg{Rmixmod}, namely \(D \times k\) mean matrix and \(D^2 \times k\) covariance array (\(D^2\) matrix to store each component variance).
\item
  To save some storage, the \pkg{EMCluster} package reshapes the covariance matrix, benefiting from its symmetry. Hence, instead of a three-dimensional array, \pkg{EMCluster} expects a compressed \(k \times \frac{D(D+1)}{2}\) matrix, each line storing the upper triangular part of the covariance. The memory gain is yet controversial, as decreasing only by a factor two the total space required for the computation. To switch from one format to another, we developed specifically two functions: \emph{trig\_mat\_to\_array()} and \emph{array\_to\_trig\_mat()} in our GitHub package \emph{RGMMBench}, partly inspiring from \emph{vec2sym} function \href{https://rdrr.io/github/patr1ckm/patr1ckm/man/vec2sym.html}{Handy R functions}.
\item
  Instead of the covariance matrix, the \pkg{mclust} package requires the lower triangular matrix resulting from its Cholesky decomposition. One of the main advantages of this input, in addition to save storage space, is that it ensures that the covariance matrix is indeed positive-definite, as the Cholesky factorisation is only defined if this condition is respected \href{https://en.wikipedia.org/wiki/Cholesky_decomposition}{Cholesky decomposition}.
\item
  \pkg{flexmix} starts by the M-step of the EM algorithm instead of the E-step. Hence, it expects the posterior probabilities assigned to each cluster \(j\) for each observation \(i\), \(\eta_i(j)\) (Equation \eqref{eq:posteriori}), instead of the initial estimates. Both approaches are, however, equivalent.
\end{itemize}

\color{blue}

On the contrary, none of the packages we evaluated that were dedicated to high-dimensional datasets allow the user to provide its own estimates. Thus, when any of the benchmarked initialisation methods listed in Table \ref{tab:general-parameter-description-pdf}, was internally available in the package, we use it with the same hyperparameters described in \protect\hyperlink{initialisation-of-the-em-algorithm}{Initialisation of the EM algorithm}. If not, we provide instead a vector containing the MAP assignments inferred by the native initialisation method, in a process similar to that used used with hierarchical clustering.
\color{black}

In addition to the plots displaying the bootstrap parameter estimations associated to Scenarios in Tables \ref{tab:parameter-configuration-univariate}, \ref{tab:parameter-configuration-bivariate} and \ref{tab:parameter-configuration-HD}, we have computed summary statistics to compare the performances of the reviewed packages:

\begin{itemize}
\item
  The \emph{bias} measures the deviation between the sample mean value of the
  estimate and the true parameter:
  \(\operatorname{Bias}(\hat{\theta})=\mathbb{E} [\hat{\theta}] - \theta\).
\item
  The Mean Squared Error (MSE) summarises both the variability of the
  estimator and its bias: \(\operatorname{MSE} (\hat{\theta})=\mathbb{E} \left[ (\hat{\theta} - \theta)^2 \right] = \operatorname{var}(\hat{\theta}) + \operatorname{Bias}(\hat{\theta})^2\), where \(\operatorname{var}(\hat{\theta})\) is the empiric variance of each
  estimator given by the diagonal terms of the empiric covariance matrix.
\item
  We enumerate the number of successes (either the package or the initial method returns an error, or fails in returning a set of parameters enforcing standard constraints of multivariate GMMs, namely the unit simplex constraint over the ratios, positive-definite covariance matrices and in general no missing or infinite value).
\item
  For each scenario, we measured independently the running times taken by the initialisation step and by the estimation of the parameters by the EM
  algorithm. To do so, the \CRANpkg{microbenchmark} package
  (Mersmann 2021) was used for its higher accuracy and flexibility for
  the computation of the running times in place of \code{System.time}.
\end{itemize}

The main differences across packages as well as performance results obtained across packages in each univariate, bivariate and high-dimensional simulation scenario are thoroughly described in the next section.

\hypertarget{appendix-d-comprehensive-report-from-the-univariate-and-multivariate-benchmark}{%
\section{Appendix D: comprehensive report from the univariate and multivariate benchmark}\label{appendix-d-comprehensive-report-from-the-univariate-and-multivariate-benchmark}}

\hypertarget{em-implementation-differences-across-reviewed-packages}{%
\subsection{EM-implementation differences across reviewed packages}\label{em-implementation-differences-across-reviewed-packages}}

\label{sec:em-differences}

Most of the distinct behaviours between the packages result from additional choices external to the EM algorithm itself, aiming at partly overcoming its main limitations (Panel B, Figure \ref{fig:heatmap-all-correlation-plots-univariate}). We detail below their differences ranked by decreasing order of their leverage effect on the final estimate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Most of the differences between the two classes of packages (Figure \ref{fig:dichotomy-package-conclusion}) are
  related to the either relative or absolute choice for the termination criterion of the EM algorithm. Given an user-defined threshold, the \emph{absolute method} early stops the estimation by comparing the difference between two consecutive log-likelihoods, \(|\ell(\hat{\theta}_{q}|X) - \ell(\hat{\theta}_{q-1}|X)|\), while the \emph{relative method} examines the variation rate, \(\left\lvert\frac{\ell(\hat{\theta}_{q}|X) - \ell(\hat{\theta}_{q-1}|X)}{\ell(\hat{\theta}_{q-1}|X)}\right\lvert\).
\item
  Several methods can be used to deal with numerical underflow, mostly happening with highly unlikely observations, distant from any centroid.

  \begin{itemize}
  \item
    The least elaborate feature is from \pkg{Rmixmod}, returning an error when either any of the posterior probabilities or any of the estimated parameters goes below to the precision threshold of the machine (\ensuremath{2.22\times 10^{-16}} for most OS).
  \item
    If the maximal value of any posterior probability is null, \pkg{bgmm} subtracts the minimal logarithm posterior probability to any log-computed probability. This method avoids numerical
    underflow by preventing computation of null ratios but the correctness of the estimates is no longer enforced\footnote{Additionally, \pkg{bgmm} does
      not update the estimated variances if any newly computed variance is
      below the criterion stop. A remarkable side-effect of these features, as
      shown in Figure \ref{fig:outliers}, is that the \pkg{bgmm} package is
      less sensitive to the presence of outliers.}.
  \item
    The remaining packages handled numeric underflow in a more convincing
    manner as they guarantee to return the MLE estimate. The \pkg{flexmix}, \pkg{GMKMcharlie} and \pkg{EMCluster}
    packages use the same log-rescaling tip detailed in (\protect\hyperlink{application-of-the-em-algorithm-to-gmms}{Application of the EM algorithm to GMMs}). The \pkg{mixtools} and
    \pkg{mclust} packages use a variant of this trick, taking profit of the factorisation
    by the greatest element (Equation \eqref{eq:trick-underflow}, Equation 3 p.5 Benaglia et al. (2009)), but without exploiting the tip of Taylor's development over \(\log(1+x)\):
  \end{itemize}
\end{enumerate}

\begin{equation}
\eta_{i} (j) = \frac{p_{j} \, \varphi_{\zeta_j} (x)}{\sum_{j=1}^k p_{j} \, \varphi_{\zeta_j} (x)}=\frac{\frac{p_{j} \, \varphi_{\zeta_j} (x)}{p_{j_{\min}} \, \varphi_{\zeta_{j_{\min}}} (x)}}{1+ \frac{\sum_{j\neq j_{\min}} p_{j} \, \varphi_{\zeta_j} (x)}{p_{j_{\min}} \, \varphi_{\zeta_{j_{\min}}} (x)}}
    \label{eq:trick-underflow}
\end{equation}

In both cases, the computation of the smallest posterior probability, the most proned to be assigned a null value, is avoided, avoiding inconsistent ratios of type \(0/0\).

\begin{itemize}
\tightlist
\item
  The previous two items deal with specific numeric limitations, but do not directly address one of the main theoretical limitation of the EM algorithm, namely the risk of falling into a suboptimal maximum, plateau or getting trapped on the boundary space (occurs when the proportion of one of the component converges to zero). Some packages specifically handle the case of a vanishing component during the EM optimization: the \pkg{mixtools} package performs random re-initialisations in case one of the computed variance goes below a user-defined threshold (default \(10^{-8}\)). \pkg{flexmix} and \pkg{GMKMcharlie} deal explicitly with the removal of a component, by updating the corresponding MLE parameters. \pkg{flexmix} removes any component whose associated weight is by default below \(0.05\) (such a stringent limitation tends to an underestimation of the true number of components in highly unbalanced mixtures)\footnote{Indeed, at least one of the component was removed in \(80\%\) of our estimations in the unbalanced and overlapping case (scenario U9 in \ref{tab:parameter-configuration-univariate}) and in \(20\%\) of the simulations in the unbalanced and well-separated case (scenario U3 in \ref{tab:parameter-configuration-univariate}).}, while GMKMcharlie both implements a lower limit on the proportions of the components and an upper threshold over the ratio of the maximum and minimal eigenvalue resulting from the factorisation of the covariance matrix (Equation \eqref{eq:covariance-decomposition})\footnote{These options are set respectively to 0 and \(+\infty\) by default, thus they did not impact our simulations}.
\end{itemize}

We enumerate below some additional features supplied by the packages:

\begin{itemize}
\item
  In addition to log rescaling, \pkg{GMKMcharlie} includes an additional argument, \emph{embedNoise}, to avoid degenerate GMMs by adding a small constant to any diagonal term (by default \(10^{-6}\)). Besides, instead of controlling whether there was a relative change of the log-likelihood, the EM implementation of \pkg{GMKMcharlie} controls instead that there was no significant relative difference in the estimated parameters in the ten previous optimisations\footnote{In our simulation, the behaviour of the \pkg{GMKMcharlie} did not differ significantly from the remaining packages of the second class. However, the use of an Euclidean distance criterion may be problematic when parameters are not on the same order of magnitude, requiring their prior normalisation}. Finally, since \pkg{GMKMcharlie} has implemented a parallelised version of the algorithm, it ensures using a a time limit that the algorithm indeed terminates (by default, set to one hour).
\item
  \pkg{flexmix} performs an unbiased estimate of the covariance matrix, instead of the corresponding ML covariance estimate (divides by a factor \(n-1\) instead of the number of observations \(n\)). Such a choice does not affect the results in our simulations, but may have a stronger impact when fitting models to a small number of observations.
\end{itemize}

\color{blue}

\begin{itemize}
\tightlist
\item
  Similarly to flexmix, the \pkg{HDclassif} package implements some constraints to preserve numerical stability. The \texttt{min.individuals} attribute, like the \texttt{minprior} attribute of \texttt{flexmix} function, discards any cluster having fewer observations\footnote{by default, set to two, i.e.~the minimum number of replications to derive an unbiased estimate of the empirical variance of a sample}. However, unlike \pkg{flexmix}, the algorithm stops instead of reparametrising the mixture problem with a smaller number of components. Coupled with the \emph{Cattell's scree-test}, the \texttt{noise.ctrl} attribute is the minimum threshold of a feature's contribution to the overall variance, computed as the corresponding normalised eigenvalue, in order to be included in the mixture of factor analysers. This additional constraint ensures a parsimonious dimension selection process, so that the number of selected intrinsic dimensions cannot be greater than or equal to the order of the discarded eigenvalues.
  \color{black}
\end{itemize}

\newpage

\hypertarget{supplementary-figures-and-tables-in-the-univariate-simulation}{%
\subsection{Supplementary Figures and Tables in the univariate simulation}\label{supplementary-figures-and-tables-in-the-univariate-simulation}}

Table below (\ref{tab:parameter-configuration-univariate}) lists the complete set of parameters used to simulate the univariate Gaussian mixture distribution in our benchmark:

\begin{table}[!h]

\caption{\label{tab:parameter-configuration-univariate}The 9 parameter configurations tested to generate the samples of the univariate experiment, with $k=4$ components.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{cccccc}
\toprule
\textbf{ID} & \textbf{Entropy} & \textbf{OVL} & \textbf{Proportions} & \textbf{Means} & \textbf{Correlations}\\
\midrule
U1 & 1.00 & 3.3e-05 & 0.25 / 0.25 / 0.25 / 0.25 & 0 / 4 / 8 / 12 & 0.3 / 0.3 / 0.3 / 0.3\\
\midrule
U2 & 1.00 & 5.7e-03 & 0.25 / 0.25 / 0.25 / 0.25 & 0 / 4 / 8 / 12 & 1 / 1 / 1 / 1\\
\midrule
U3 & 1.00 & 2.0e-02 & 0.25 / 0.25 / 0.25 / 0.25 & 0 / 4 / 8 / 12 & 2 / 2 / 2 / 2\\
\midrule
U4 & 0.96 & 3.3e-05 & 0.2 / 0.4 / 0.2 / 0.2 & 0 / 4 / 8 / 12 & 0.3 / 0.3 / 0.3 / 0.3\\
\midrule
U5 & 0.96 & 5.8e-03 & 0.2 / 0.4 / 0.2 / 0.2 & 0 / 4 / 8 / 12 & 1 / 1 / 1 / 1\\
\midrule
\addlinespace
U6 & 0.96 & 2.0e-02 & 0.2 / 0.4 / 0.2 / 0.2 & 0 / 4 / 8 / 12 & 2 / 2 / 2 / 2\\
\midrule
U7 & 0.68 & 2.7e-05 & 0.1 / 0.7 / 0.1 / 0.1 & 0 / 4 / 8 / 12 & 0.3 / 0.3 / 0.3 / 0.3\\
\midrule
U8 & 0.68 & 4.4e-03 & 0.1 / 0.7 / 0.1 / 0.1 & 0 / 4 / 8 / 12 & 1 / 1 / 1 / 1\\
\midrule
U9 & 0.68 & 1.5e-02 & 0.1 / 0.7 / 0.1 / 0.1 & 0 / 4 / 8 / 12 & 2 / 2 / 2 / 2\\
\midrule
\bottomrule
\end{tabular}}
\end{table}

Figure \ref{fig:four-component-balanced-separated}-Figure
\ref{fig:four-components-midbalanced} each summarise the benchmarking
results associated with one of the scenarios listed in Table
\ref{tab:parameter-configuration-univariate}.

Summary tables \ref{tab:balanced-well-separated-table-univariate-pdf}-
\ref{tab:unbalanced-overlapping-table-univariate-pdf}
display the average performance for each package of the benchmark with
each initialisation method. The best performing pair (lowest bias or
MSE) is highlighted in green, and the worst performing in red. The MSE and bias columns were derived by summing respectively the estimated proportions, means and standard deviations associated with the
individual components.

\begin{table}[!h]

\caption{\label{tab:balanced-well-separated-table-univariate-pdf}MSE and Bias associated to scenario U1, in Table \ref{tab:parameter-configuration-univariate} (balanced and well-separated components)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & quantiles & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0061} & \textcolor{red}{1.90000} & \textcolor{red}{0.19000} & \textcolor{red}{0.0290} & \textcolor{red}{0.5300} & \textcolor{red}{0.1100}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{1-8}
 & hc & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & quantiles & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.0064} & \textcolor{black}{1.80000} & \textcolor{red}{0.19000} & \textcolor{red}{0.0290} & \textcolor{red}{0.5300} & \textcolor{red}{0.1100}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{1-8}
 & hc & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & quantiles & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0062} & \textcolor{black}{1.80000} & \textcolor{red}{0.19000} & \textcolor{red}{0.0290} & \textcolor{red}{0.5300} & \textcolor{red}{0.1100}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{1-8}
 & hc & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & quantiles & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.0064} & \textcolor{red}{1.90000} & \textcolor{red}{0.19000} & \textcolor{red}{0.0290} & \textcolor{red}{0.5300} & \textcolor{red}{0.1100}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{1-8}
 & hc & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & quantiles & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.0064} & \textcolor{red}{1.90000} & \textcolor{red}{0.19000} & \textcolor{red}{0.0290} & \textcolor{red}{0.5300} & \textcolor{red}{0.1100}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{green}{0.0004} & \textcolor{green}{0.00077} & \textcolor{green}{0.00037} & \textcolor{green}{0.0025} & \textcolor{green}{0.0068} & \textcolor{green}{0.0028}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/univariate/univariate_balanced_separated} 

}

\caption{Benchmark summary plots of scenario U1 in Table \ref{tab:parameter-configuration-univariate} (balanced and well-separated components), organised as such:
The panel A displays the distribution of the global mixture distribution $f_{\theta}(X)$
(pink solid line) and of each of its constitutive components scaled by
their respective proportions (dotted lines).
Running times are displayed in Panel B with the \textit{k}-means initialisation. The number of observations
(x-axis) and the running time (y-axis) is in $\log(10)$ scale, implying
that any linear relationship between the running time and the number of
observations is represented by a slope of 1. The points represent median
running time. The coloured bands represent the 5th and 95th percentiles
of the running time.
In panel C are represented the boxplots associated with the distribution of the estimates, with one box per pair of package and initialisation method. The median is displayed with bold
black line, the mean with a yellow cross and the 0.25 and 0.95 quantiles
match the edges of the rectangular band. Solid black lines extending
past the box boundaries represent the $1.5$ IQR, estimates above these
limits considered as outliers and omitted from the plot. Finally, the
true value of the parameter is represented as a dashed red line. The bold black writing in the upper right-hand corner refers to the parameter whose distribution is shown in the corresponding facet. The first, second and third rows are the distributions of the ratios, means and variances of each component, identified by the column index.}\label{fig:four-component-balanced-separated}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:unbalanced-well-separated-table-univariate-pdf}MSE and Bias associated to scenario U7, in Table \ref{tab:parameter-configuration-univariate} (unbalanced and well-separated components)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{black}{0.02900} & \textcolor{black}{2.8000} & \textcolor{black}{0.45000} & \textcolor{black}{0.1000} & \textcolor{black}{0.840} & \textcolor{black}{0.250}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00730} & \textcolor{black}{0.7900} & \textcolor{black}{0.13000} & \textcolor{black}{0.0260} & \textcolor{black}{0.240} & \textcolor{black}{0.075}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.16000} & \textcolor{red}{19.0000} & \textcolor{red}{3.20000} & \textcolor{red}{0.6400} & \textcolor{red}{6.100} & \textcolor{black}{1.800}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.17000} & \textcolor{black}{10.5000} & \textcolor{black}{1.40000} & \textcolor{black}{0.3600} & \textcolor{black}{3.100} & \textcolor{black}{0.780}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{green}{0.00027} & \textcolor{green}{0.0015} & \textcolor{black}{0.00077} & \textcolor{green}{0.0025} & \textcolor{green}{0.014} & \textcolor{black}{0.014}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.05500} & \textcolor{black}{2.8000} & \textcolor{black}{0.45000} & \textcolor{black}{0.1100} & \textcolor{black}{0.850} & \textcolor{black}{0.250}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00760} & \textcolor{black}{0.7800} & \textcolor{black}{0.13000} & \textcolor{black}{0.0260} & \textcolor{black}{0.240} & \textcolor{black}{0.075}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.11000} & \textcolor{red}{19.0000} & \textcolor{red}{3.20000} & \textcolor{black}{0.5400} & \textcolor{black}{6.000} & \textcolor{red}{1.900}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.15000} & \textcolor{black}{8.4000} & \textcolor{black}{1.00000} & \textcolor{black}{0.3000} & \textcolor{black}{2.500} & \textcolor{black}{0.580}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{green}{0.00027} & \textcolor{green}{0.0015} & \textcolor{green}{0.00076} & \textcolor{green}{0.0025} & \textcolor{green}{0.014} & \textcolor{green}{0.011}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.03200} & \textcolor{black}{2.8000} & \textcolor{black}{0.45000} & \textcolor{black}{0.1000} & \textcolor{black}{0.850} & \textcolor{black}{0.250}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00740} & \textcolor{black}{0.7800} & \textcolor{black}{0.13000} & \textcolor{black}{0.0260} & \textcolor{black}{0.240} & \textcolor{black}{0.075}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.14000} & \textcolor{red}{19.0000} & \textcolor{red}{3.20000} & \textcolor{black}{0.6000} & \textcolor{black}{6.000} & \textcolor{red}{1.900}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.18000} & \textcolor{black}{10.4000} & \textcolor{black}{1.40000} & \textcolor{black}{0.3600} & \textcolor{black}{3.100} & \textcolor{black}{0.800}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{green}{0.00027} & \textcolor{green}{0.0015} & \textcolor{black}{0.00077} & \textcolor{green}{0.0025} & \textcolor{green}{0.014} & \textcolor{black}{0.014}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.03200} & \textcolor{black}{2.8000} & \textcolor{black}{0.45000} & \textcolor{black}{0.1000} & \textcolor{black}{0.850} & \textcolor{black}{0.250}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00620} & \textcolor{black}{0.7600} & \textcolor{black}{0.13000} & \textcolor{black}{0.0170} & \textcolor{black}{0.230} & \textcolor{black}{0.079}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.15000} & \textcolor{red}{19.0000} & \textcolor{red}{3.20000} & \textcolor{black}{0.5800} & \textcolor{black}{6.000} & \textcolor{black}{1.800}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.18000} & \textcolor{black}{10.3000} & \textcolor{black}{1.40000} & \textcolor{black}{0.3600} & \textcolor{black}{3.100} & \textcolor{black}{0.800}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{green}{0.00027} & \textcolor{green}{0.0015} & \textcolor{black}{0.00077} & \textcolor{green}{0.0025} & \textcolor{green}{0.014} & \textcolor{black}{0.014}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.02900} & \textcolor{black}{2.8000} & \textcolor{black}{0.45000} & \textcolor{black}{0.1000} & \textcolor{black}{0.850} & \textcolor{black}{0.250}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00540} & \textcolor{black}{0.7700} & \textcolor{black}{0.13000} & \textcolor{black}{0.0190} & \textcolor{black}{0.230} & \textcolor{black}{0.078}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.14000} & \textcolor{red}{19.0000} & \textcolor{red}{3.20000} & \textcolor{black}{0.5900} & \textcolor{black}{6.000} & \textcolor{black}{1.800}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.17000} & \textcolor{black}{10.4000} & \textcolor{black}{1.40000} & \textcolor{black}{0.3600} & \textcolor{black}{3.100} & \textcolor{black}{0.800}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{green}{0.00027} & \textcolor{green}{0.0015} & \textcolor{black}{0.00077} & \textcolor{green}{0.0025} & \textcolor{green}{0.014} & \textcolor{black}{0.014}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/univariate/univariate_unbalanced_separated} 

}

\caption{Benchmark summary plots of scenario U7 in Table \ref{tab:parameter-configuration-univariate} (unbalanced and well-separated components), with same layout as in Figure \ref{fig:four-component-balanced-separated}.}\label{fig:four-component-unbalanced-separated}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:balanced-overlapping-table-univariate-pdf}MSE and Bias associated to scenario U3, in Table \ref{tab:parameter-configuration-univariate} (balanced and overlapping components)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{black}{0.0170} & \textcolor{black}{1.60} & \textcolor{black}{0.45} & \textcolor{black}{0.1950} & \textcolor{black}{1.320} & \textcolor{black}{0.93}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0054} & \textcolor{black}{0.81} & \textcolor{black}{0.18} & \textcolor{black}{0.0125} & \textcolor{green}{0.023} & \textcolor{black}{0.32}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.0070} & \textcolor{black}{0.67} & \textcolor{black}{0.30} & \textcolor{black}{0.0930} & \textcolor{black}{0.590} & \textcolor{black}{0.56}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0440} & \textcolor{black}{8.40} & \textcolor{black}{1.00} & \textcolor{black}{0.0710} & \textcolor{black}{0.330} & \textcolor{black}{0.63}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{black}{0.0990} & \textcolor{black}{11.00} & \textcolor{black}{1.60} & \textcolor{black}{0.2000} & \textcolor{black}{1.600} & \textcolor{black}{0.78}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.0260} & \textcolor{black}{2.60} & \textcolor{black}{0.94} & \textcolor{black}{0.1120} & \textcolor{black}{1.160} & \textcolor{black}{1.22}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.0044} & \textcolor{black}{0.67} & \textcolor{green}{0.14} & \textcolor{green}{0.0036} & \textcolor{black}{0.091} & \textcolor{black}{0.27}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.0054} & \textcolor{green}{0.57} & \textcolor{black}{0.27} & \textcolor{black}{0.0850} & \textcolor{black}{0.670} & \textcolor{black}{0.55}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0420} & \textcolor{black}{8.20} & \textcolor{black}{1.10} & \textcolor{black}{0.0450} & \textcolor{black}{0.450} & \textcolor{black}{0.68}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{black}{0.1210} & \textcolor{red}{14.30} & \textcolor{black}{2.70} & \textcolor{black}{0.2700} & \textcolor{red}{2.700} & \textcolor{black}{1.17}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.0110} & \textcolor{black}{2.50} & \textcolor{black}{0.84} & \textcolor{black}{0.0330} & \textcolor{black}{1.160} & \textcolor{black}{1.10}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0068} & \textcolor{black}{0.86} & \textcolor{black}{0.24} & \textcolor{black}{0.0294} & \textcolor{black}{0.114} & \textcolor{black}{0.36}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.0075} & \textcolor{black}{0.70} & \textcolor{black}{0.32} & \textcolor{black}{0.1110} & \textcolor{black}{0.720} & \textcolor{black}{0.63}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0490} & \textcolor{black}{9.10} & \textcolor{black}{1.20} & \textcolor{black}{0.0800} & \textcolor{black}{0.320} & \textcolor{black}{0.68}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{red}{0.1410} & \textcolor{black}{10.90} & \textcolor{red}{2.90} & \textcolor{red}{0.2900} & \textcolor{black}{1.800} & \textcolor{red}{1.47}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.0320} & \textcolor{black}{2.40} & \textcolor{black}{0.80} & \textcolor{black}{0.0670} & \textcolor{black}{0.360} & \textcolor{green}{0.25}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0415} & \textcolor{black}{2.51} & \textcolor{black}{1.11} & \textcolor{black}{0.1000} & \textcolor{black}{0.664} & \textcolor{black}{0.74}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.0383} & \textcolor{black}{2.40} & \textcolor{black}{1.00} & \textcolor{black}{0.1170} & \textcolor{black}{0.770} & \textcolor{black}{0.78}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0660} & \textcolor{black}{9.40} & \textcolor{black}{1.80} & \textcolor{black}{0.0130} & \textcolor{black}{0.340} & \textcolor{black}{0.48}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{black}{0.1090} & \textcolor{black}{9.60} & \textcolor{black}{2.50} & \textcolor{black}{0.2600} & \textcolor{black}{1.800} & \textcolor{black}{1.33}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.0220} & \textcolor{black}{2.00} & \textcolor{black}{0.67} & \textcolor{black}{0.0490} & \textcolor{black}{0.370} & \textcolor{green}{0.25}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0318} & \textcolor{black}{2.31} & \textcolor{black}{0.85} & \textcolor{black}{0.0952} & \textcolor{black}{0.602} & \textcolor{black}{0.67}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.0297} & \textcolor{black}{2.19} & \textcolor{black}{0.80} & \textcolor{black}{0.1210} & \textcolor{black}{0.770} & \textcolor{black}{0.76}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0620} & \textcolor{black}{9.40} & \textcolor{black}{1.70} & \textcolor{black}{0.0160} & \textcolor{black}{0.310} & \textcolor{black}{0.50}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.1140} & \textcolor{black}{10.30} & \textcolor{black}{2.60} & \textcolor{black}{0.2600} & \textcolor{black}{1.900} & \textcolor{black}{1.31}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/univariate/univariate_balanced_overlapping} 

}

\caption{Benchmark summary plots of scenario U3 in Table \ref{tab:parameter-configuration-univariate} (balanced and overlapping components), with same layout as in Figure \ref{fig:four-component-balanced-separated}.}\label{fig:four-component-balanced-overlapping}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:unbalanced-overlapping-table-univariate-pdf}MSE and Bias associated to scenario U9, in Table \ref{tab:parameter-configuration-univariate} (unbalanced and overlapping components)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{black}{0.230} & \textcolor{black}{9.3} & \textcolor{black}{0.94} & \textcolor{black}{0.78} & \textcolor{black}{4.9} & \textcolor{black}{1.33}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.094} & \textcolor{green}{5.1} & \textcolor{green}{0.57} & \textcolor{black}{0.50} & \textcolor{black}{3.4} & \textcolor{black}{0.89}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.230} & \textcolor{black}{9.9} & \textcolor{black}{1.00} & \textcolor{black}{0.80} & \textcolor{black}{5.1} & \textcolor{black}{1.34}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.270} & \textcolor{black}{11.5} & \textcolor{black}{0.90} & \textcolor{black}{0.63} & \textcolor{black}{2.8} & \textcolor{black}{0.85}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{black}{0.330} & \textcolor{black}{20.0} & \textcolor{black}{2.20} & \textcolor{black}{0.53} & \textcolor{black}{5.3} & \textcolor{black}{0.84}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.170} & \textcolor{black}{10.5} & \textcolor{black}{0.88} & \textcolor{black}{0.64} & \textcolor{black}{5.2} & \textcolor{black}{1.14}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.051} & \textcolor{black}{5.6} & \textcolor{black}{0.61} & \textcolor{black}{0.34} & \textcolor{black}{3.6} & \textcolor{black}{0.94}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.210} & \textcolor{black}{11.3} & \textcolor{black}{1.20} & \textcolor{black}{0.75} & \textcolor{red}{5.6} & \textcolor{black}{1.53}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.180} & \textcolor{black}{9.5} & \textcolor{black}{0.77} & \textcolor{black}{0.43} & \textcolor{black}{2.7} & \textcolor{black}{0.86}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{black}{0.110} & \textcolor{black}{10.0} & \textcolor{black}{1.70} & \textcolor{green}{0.15} & \textcolor{black}{2.3} & \textcolor{black}{1.48}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.230} & \textcolor{black}{10.2} & \textcolor{black}{0.84} & \textcolor{black}{0.79} & \textcolor{black}{5.1} & \textcolor{black}{1.20}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.107} & \textcolor{black}{5.5} & \textcolor{black}{0.62} & \textcolor{black}{0.53} & \textcolor{black}{3.6} & \textcolor{black}{0.96}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.270} & \textcolor{black}{11.4} & \textcolor{black}{1.20} & \textcolor{red}{0.87} & \textcolor{red}{5.6} & \textcolor{red}{1.59}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.300} & \textcolor{black}{12.2} & \textcolor{black}{1.06} & \textcolor{black}{0.66} & \textcolor{black}{2.9} & \textcolor{black}{0.84}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{black}{0.270} & \textcolor{black}{21.0} & \textcolor{black}{2.50} & \textcolor{black}{0.46} & \textcolor{black}{5.2} & \textcolor{black}{1.13}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.200} & \textcolor{black}{9.7} & \textcolor{black}{1.19} & \textcolor{black}{0.64} & \textcolor{black}{3.4} & \textcolor{black}{0.69}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.135} & \textcolor{black}{7.7} & \textcolor{black}{1.16} & \textcolor{black}{0.46} & \textcolor{green}{2.1} & \textcolor{black}{0.48}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.280} & \textcolor{black}{11.2} & \textcolor{black}{1.60} & \textcolor{black}{0.74} & \textcolor{black}{4.2} & \textcolor{black}{0.72}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.350} & \textcolor{black}{15.7} & \textcolor{black}{1.62} & \textcolor{black}{0.65} & \textcolor{green}{2.1} & \textcolor{black}{0.64}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{black}{0.240} & \textcolor{red}{22.0} & \textcolor{red}{2.70} & \textcolor{black}{0.47} & \textcolor{black}{5.1} & \textcolor{black}{1.18}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.210} & \textcolor{black}{9.5} & \textcolor{black}{1.07} & \textcolor{black}{0.69} & \textcolor{black}{3.8} & \textcolor{black}{0.79}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.113} & \textcolor{black}{6.5} & \textcolor{black}{0.90} & \textcolor{black}{0.46} & \textcolor{black}{2.4} & \textcolor{green}{0.43}\\
\cmidrule{2-8}
 & quantiles & \textcolor{black}{0.240} & \textcolor{black}{10.1} & \textcolor{black}{1.30} & \textcolor{black}{0.74} & \textcolor{black}{4.2} & \textcolor{black}{0.81}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.320} & \textcolor{black}{14.6} & \textcolor{black}{1.45} & \textcolor{black}{0.61} & \textcolor{green}{2.1} & \textcolor{black}{0.58}\\
\cmidrule{2-8}
\multirow{-5}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.250} & \textcolor{red}{22.0} & \textcolor{red}{2.70} & \textcolor{black}{0.49} & \textcolor{black}{5.2} & \textcolor{black}{1.18}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/univariate/univariate_unbalanced_overlapping} 

}

\caption{Benchmark summary plots of scenario U9 in Table \ref{tab:parameter-configuration-univariate} (unbalanced and overlapping components), with same layout as in Figure \ref{fig:four-component-balanced-separated}.}\label{fig:four-component-unbalanced-overlapping}
\end{figure}

\newpage
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/univariate/univariate_midbalanced} 

}

\caption{Benchmark summary plots of scenarios U4 and U6 in Table \ref{tab:parameter-configuration-univariate} (small unbalance, with additional overlap in scenario U6). Panel A and B display the univariate GMM distributions of respectively scenarios U4 and U6, and Panel C and D the benchmarked distributions of respectively scenarios U4 and U6, built as Panel C of Figure \ref{fig:four-component-balanced-separated}.}\label{fig:four-components-midbalanced}
\end{figure}

\newpage
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/univariate/heatmap_univariate} 

}

\caption{Correlation heatmaps of the estimated parameters extended to the four initialisation methods benchmarked, using the same configuration described in Figure \ref{fig:dichotomy-package-conclusion}, in the bivariate setting.}\label{fig:heatmap-all-correlation-plots-univariate}
\end{figure}

\newpage
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/univariate_initialisation_time_computations} 

}

\caption{Distribution of the running times taken by each initialisation algorithm enumerated in Table \ref{tab:general-parameter-description-pdf}, across all scenarios listed in Table \ref{tab:parameter-configuration-univariate}, sorted by increasing ID number in the lexicographical order.}\label{fig:univariate-initialisation-time-computations}
\end{figure}

The panels indexed by the B letter, from Figure
\ref{fig:four-component-balanced-separated} to Figure
\ref{fig:four-components-midbalanced}, display the 0.05, 0.5 and 0.95 quantiles of the distribution of the operating times
taken for parameter estimation, for the scenarios listed in Table \ref{tab:parameter-configuration-univariate}.

First, we note that the execution time grows asymptotically
linearly with the number of observations, confirming empirically the
expected linear complexity of the EM algorithm. The most important
factor playing on the differences observed is related to the complexity
of the distribution, and especially the degree of overlap between the
components:

\begin{itemize}
\item
  On the one hand hand, when components are well-separated (scenarios 1 and 3 in Table \ref{tab:parameter-configuration-univariate}), the estimation of
  the parameters is simple, leading to a reduced number of iterations
  required to reach the convergence and shorter running times.
\item
  On the other hand, the time taken by the slowest package for the estimation of the parameters increases by a hundred factor with the most
  complex scenario (see scenario U9, \ref{tab:parameter-configuration-univariate}, illustrated in Figure
  \ref{fig:four-component-unbalanced-overlapping}), compared to the simplest scenario (see U1, \ref{tab:parameter-configuration-univariate}, shown in Figure \ref{fig:four-component-balanced-separated}). Indeed, the average running time for a complete run of the EM algorithm increases from 0.215 seconds to 10.8 seconds.
\end{itemize}

To better understand the running times' differences observed between the
packages for a given scenario, we perform a three-way anova, taking into
account the choice of initialisation method, the programming language
and the class of packages\footnote{To compare
  whether differences between mean running times or estimation
  performances differ across packages, we used the between-subjects Anova test \code{rstatix::anova\_test()} to generate
  the \emph{p}-values and \code{rstatix::partial\_eta\_squared()} to compute the corresponding effect sizes.}:

\begin{itemize}
\item
  With well-separated components (Scenarios U1
  and U7 in Table \ref{tab:parameter-configuration-univariate}), the class
  of packages (namely the choice of the convergence criterion) has a negligible impact compared to the choice of initialisation algorithm or the programming language. The effect sizes associated to the programming language and the initialisation method are respectively \(1.688 \times 10^{-2}\) (\emph{p}-value of \(3 \times 10^{-60}\)) and \(13 \times 10^{-5}\) (\emph{p}-value of
  \(3 \times 10^{-60}\)), while the choice of the termination criteria did
  not significantly impact the execution time, with an effect size of
  \(8.119 \times 10^{-4}\)( \emph{p}-value of \(0.35\)). Faster running times with
  packages natively encoded in Fortran or C compared to those encoded in R
  only were expected, as R is a high-level programming language known to
  be slower. Indeed, the \pkg{flexmix} package is the slowest, preceded by
  our baseline R implementation. Additionally,
  \pkg{mclust}, followed by \pkg{mixtools}, \pkg{Rmixmod} and \pkg{bgmm}
  are the fastest.
\item
  On the other hand, with overlapping components (Scenarios U3 and U9 in Table \ref{tab:parameter-configuration-univariate}), the package class and the programming language have a statistically significant impact on the average running times (the effect sizes associated to the
  choice of the termination criteria and the programming language are
  respectively \(0.111\) (numerical null \emph{p}-value) and \(0.0852\) (\emph{p}-value of
  \(8 \times 10^{-307}\))) while the initialisation method has no substantial impact (effect size of
  \(2.967 \times 10^{-4}\) and \emph{p}-value of \(0.32\)). In the context of highly overlapping mixture, the fastest ones are \pkg{mclust} and \pkg{GMKMcharlie}, benefiting from both using
  relative ratios and a fast programming language, while our
  baseline implementation \texttt{emnmix}, preceded by \pkg{Rmixmod} and \pkg{mixtools}, are on average a hundred times slower.
\end{itemize}

\newpage

\hypertarget{supplementary-figures-and-tables-in-the-bivariate-simulation}{%
\subsection{Supplementary Figures and Tables in the bivariate simulation}\label{supplementary-figures-and-tables-in-the-bivariate-simulation}}

Table below (\ref{tab:parameter-configuration-bivariate}) lists the complete set of parameters used to simulate the multivariate Gaussian mixture distribution in our benchmark:

\begin{table}[!h]

\caption{\label{tab:parameter-configuration-bivariate}The 20 parameter configurations tested to generate the samples of the bivariate experiment.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{cccccc}
\toprule
\textbf{ID} & \textbf{Entropy} & \textbf{OVL} & \textbf{Proportions} & \textbf{Means} & \textbf{Correlations}\\
\midrule
B1 & 1.00 & 0.15000 & 0.5 / 0.5 & (0,2);(2,0) & -0.8 / -0.8\\
\midrule
B2 & 1.00 & 0.07300 & 0.5 / 0.5 & (0,2);(2,0) & -0.8 / 0.8\\
\midrule
B3 & 1.00 & 0.07300 & 0.5 / 0.5 & (0,2);(2,0) & 0.8 / -0.8\\
\midrule
B4 & 1.00 & 0.00078 & 0.5 / 0.5 & (0,2);(2,0) & 0.8 / 0.8\\
\midrule
B5 & 1.00 & 0.07900 & 0.5 / 0.5 & (0,2);(2,0) & 0 / 0\\
\midrule
\addlinespace
B6 & 1.00 & 0.00000 & 0.5 / 0.5 & (0,20);(20,0) & -0.8 / -0.8\\
\midrule
B7 & 1.00 & 0.00000 & 0.5 / 0.5 & (0,20);(20,0) & -0.8 / 0.8\\
\midrule
B8 & 1.00 & 0.00000 & 0.5 / 0.5 & (0,20);(20,0) & 0.8 / -0.8\\
\midrule
B9 & 1.00 & 0.00000 & 0.5 / 0.5 & (0,20);(20,0) & 0.8 / 0.8\\
\midrule
B10 & 1.00 & 0.00000 & 0.5 / 0.5 & (0,20);(20,0) & 0 / 0\\
\midrule
\addlinespace
B11 & 0.47 & 0.06600 & 0.9 / 0.1 & (0,2);(2,0) & -0.8 / -0.8\\
\midrule
B12 & 0.47 & 0.01600 & 0.9 / 0.1 & (0,2);(2,0) & -0.8 / 0.8\\
\midrule
B13 & 0.47 & 0.05000 & 0.9 / 0.1 & (0,2);(2,0) & 0.8 / -0.8\\
\midrule
B14 & 0.47 & 0.00045 & 0.9 / 0.1 & (0,2);(2,0) & 0.8 / 0.8\\
\midrule
B15 & 0.47 & 0.03900 & 0.9 / 0.1 & (0,2);(2,0) & 0 / 0\\
\midrule
\addlinespace
B16 & 0.47 & 0.00000 & 0.9 / 0.1 & (0,20);(20,0) & -0.8 / -0.8\\
\midrule
B17 & 0.47 & 0.00000 & 0.9 / 0.1 & (0,20);(20,0) & -0.8 / 0.8\\
\midrule
B18 & 0.47 & 0.00000 & 0.9 / 0.1 & (0,20);(20,0) & 0.8 / -0.8\\
\midrule
B19 & 0.47 & 0.00000 & 0.9 / 0.1 & (0,20);(20,0) & 0.8 / 0.8\\
\midrule
B20 & 0.47 & 0.00000 & 0.9 / 0.1 & (0,20);(20,0) & 0 / 0\\
\midrule
\bottomrule
\end{tabular}}
\end{table}

Figures \ref{fig:multivariate-overlapping-unbalanced-negative-correlated}-
\ref{fig:multivariate-overlapping-unbalanced-uncorrelated} are associated to scenarios B11 - B15 of Table \ref{tab:parameter-configuration-bivariate}. Summary tables
\ref{tab:multivariate-overlapping-unbalanced-negative-correlated-pdf}-\ref{tab:multivariate-overlapping-unbalanced-uncorrelated-pdf}
show the average performance for each combination of a benchmarked package and initialisation method, with the same conventions as discussed in \protect\hyperlink{supplementary-figures-and-tables-in-the-univariate-simulation}{Supplementary Figures and Tables in the univariate simulation}.

First, we can directly observe that the OVL increases as the individual variance of each component, the proximity of the centroids of the clusters and the level of imbalance is increased. We demonstrate this statement formally in section \protect\hyperlink{an-analytic-formula-of-the-overlap-for-univariate-gaussian-mixtures}{An analytic formula of the overlap for univariate Gaussian mixtures}. Nonetheless, the influence of the correlation between the \(x\) and the \(y\)-axis (the off-diagonal term of the covariance matrix) is not immediate, notably the assumption of independent features does not automatically entail a lower OVL or simpler estimation.

From our experiments, we deduce that the highest OVL is obtained when the main axis of the two respective components aligns with the line joining the two centroids. For instance, in our scenario, the lowest OVL is obtained when the correlation term is positive for both clusters (scenario 14, Table \ref{tab:parameter-configuration-bivariate} and isodensity plot in panel A, Figure \ref{fig:multivariate-overlapping-unbalanced-positive-correlated}), whereas the highest OVL is obtained with a negative correlation (scenario 11, Table \ref{tab:parameter-configuration-bivariate}and isodensity plot in panel A, Figure \ref{fig:multivariate-overlapping-unbalanced-negative-correlated}). Recall that the slope joining the two centroids of the two components in all our simulated distributions is indeed negative.

\begin{table}[!h]

\caption{\label{tab:multivariate-overlapping-unbalanced-negative-correlated-pdf}MSE and Bias associated to scenario B11, in Table \ref{tab:parameter-configuration-bivariate} (unbalanced, overlapping and negative correlated components).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{black}{0.230} & \textcolor{red}{3.90} & \textcolor{black}{1.8} & \textcolor{red}{0.550} & \textcolor{black}{2.30} & \textcolor{black}{1.200}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.136} & \textcolor{black}{2.80} & \textcolor{red}{1.9} & \textcolor{black}{0.450} & \textcolor{black}{2.30} & \textcolor{black}{2.200}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.028} & \textcolor{black}{1.27} & \textcolor{black}{1.1} & \textcolor{black}{0.084} & \textcolor{green}{0.12} & \textcolor{black}{0.140}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{black}{0.071} & \textcolor{black}{2.20} & \textcolor{black}{1.4} & \textcolor{black}{0.170} & \textcolor{black}{0.66} & \textcolor{black}{0.111}\\
\cmidrule{1-8}
 & hc & \textcolor{red}{0.260} & \textcolor{red}{3.90} & \textcolor{red}{1.9} & \textcolor{black}{0.480} & \textcolor{red}{2.40} & \textcolor{black}{1.300}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.077} & \textcolor{black}{2.80} & \textcolor{red}{1.9} & \textcolor{black}{0.270} & \textcolor{red}{2.40} & \textcolor{red}{2.300}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.028} & \textcolor{green}{0.96} & \textcolor{green}{1.0} & \textcolor{green}{0.064} & \textcolor{black}{0.77} & \textcolor{black}{0.720}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{black}{0.087} & \textcolor{black}{1.90} & \textcolor{green}{1.0} & \textcolor{black}{0.170} & \textcolor{black}{1.02} & \textcolor{black}{0.468}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.230} & \textcolor{red}{3.90} & \textcolor{black}{1.8} & \textcolor{red}{0.550} & \textcolor{black}{2.30} & \textcolor{black}{1.200}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.136} & \textcolor{black}{2.80} & \textcolor{red}{1.9} & \textcolor{black}{0.450} & \textcolor{black}{2.30} & \textcolor{black}{2.200}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.028} & \textcolor{black}{1.27} & \textcolor{black}{1.1} & \textcolor{black}{0.084} & \textcolor{green}{0.12} & \textcolor{black}{0.140}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{black}{0.071} & \textcolor{black}{2.20} & \textcolor{black}{1.4} & \textcolor{black}{0.170} & \textcolor{black}{0.66} & \textcolor{black}{0.111}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.210} & \textcolor{black}{3.30} & \textcolor{black}{1.8} & \textcolor{black}{0.470} & \textcolor{black}{1.80} & \textcolor{black}{1.100}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.131} & \textcolor{black}{2.60} & \textcolor{black}{1.8} & \textcolor{black}{0.380} & \textcolor{black}{1.80} & \textcolor{black}{1.800}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.051} & \textcolor{black}{1.61} & \textcolor{black}{1.1} & \textcolor{black}{0.129} & \textcolor{black}{0.20} & \textcolor{black}{0.180}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{black}{0.093} & \textcolor{black}{2.40} & \textcolor{black}{1.4} & \textcolor{black}{0.210} & \textcolor{black}{0.60} & \textcolor{green}{0.063}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.210} & \textcolor{black}{3.30} & \textcolor{black}{1.8} & \textcolor{black}{0.470} & \textcolor{black}{1.80} & \textcolor{black}{1.100}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.131} & \textcolor{black}{2.60} & \textcolor{black}{1.8} & \textcolor{black}{0.380} & \textcolor{black}{1.80} & \textcolor{black}{1.800}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.051} & \textcolor{black}{1.61} & \textcolor{black}{1.1} & \textcolor{black}{0.129} & \textcolor{black}{0.20} & \textcolor{black}{0.180}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.093} & \textcolor{black}{2.40} & \textcolor{black}{1.4} & \textcolor{black}{0.210} & \textcolor{black}{0.60} & \textcolor{green}{0.063}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figs/bivariate/multivariate_unbalanced_overlapping_negatively_correlated} 

}

\caption{Results of scenario B11 in Table \ref{tab:parameter-configuration-bivariate} (unbalanced, overlapping and negative correlated components),  organised as such:
The panel A displays the bivariate contour maps associated to the two-components multivariate Gaussian distribution corresponding to the parametrisation described by the scenario, warmer colours corresponding to regions of higher densities. The two centroids, whose coordinates are given by the mean components' elements, are represented with distinct shaped and coloured point estimates.
In both Panels A and B, the ellipsoids correspond to the $95\%$ confidence region associated to each component's distribution. To generate them, we largely inspired from the \code{mixtools::ellipse()} and website \href{https://cookierobotics.com/007/}{How to draw ellipses}. To generate them, we retain for each individual parameter its mean (similar results with the median) over the $N=100$ sampling experiments, restrained to the random initialisation method.
The running times are displayed in Panel C with the \textit{k}-means initialisation. The number of observations (x-axis) and the running time (y-axis) is in $\log(10)$ scale. The points represent median running time. The coloured bands represent the $5^{\text{th}}$ and $95^{\text{th}}$ percentiles of the running time.
The distributions of the Hellinger distances (a closed form is only available for the Gaussian multivariate distribution, not the mixture) are computed for each component, each initialisation method and each package with respect to the true Gaussian distribution expected for each component. The more dissimilar are the distributions, the higher is the Hellinger distance, knowing it is normalised between 0 and 1. We represent them using boxplot representations in Panel D.
In panel E we represent the boxplots associated with the distribution of the estimates, with one box per pair of package and initialisation method. As the correlation is a symmetric operator, we only represent the distribution of the lower part of the lower matrix. Each column is associated to the parameters of a component. First row represents the distribution of the estimated ratios, second and third respectively the distributions of the mean vector on the x-axis and on the y-axis, third and four the distributions of the individual variances of each feature and finally the fifth row shows the distribution of the correlation between dimension 1 and 2.}\label{fig:multivariate-overlapping-unbalanced-negative-correlated}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:multivariate-overlapping-unbalanced-opposite-correlated-pdf}MSE and Bias associated to scenario B12, in Table \ref{tab:parameter-configuration-bivariate} (unbalanced, overlapping and opposite correlated components).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{black}{0.00076} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0063} & \textcolor{black}{0.056} & \textcolor{black}{0.131}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00076} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0063} & \textcolor{black}{0.056} & \textcolor{black}{0.131}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.00075} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0057} & \textcolor{black}{0.055} & \textcolor{black}{0.123}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{black}{0.00087} & \textcolor{red}{0.066} & \textcolor{red}{0.17} & \textcolor{black}{0.0070} & \textcolor{red}{0.063} & \textcolor{red}{0.190}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.00144} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0101} & \textcolor{black}{0.055} & \textcolor{black}{0.071}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00144} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0101} & \textcolor{black}{0.055} & \textcolor{black}{0.071}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.00144} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{black}{0.0099} & \textcolor{black}{0.054} & \textcolor{green}{0.067}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{red}{0.00145} & \textcolor{green}{0.048} & \textcolor{green}{0.16} & \textcolor{red}{0.0142} & \textcolor{green}{0.047} & \textcolor{black}{0.110}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.00076} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0063} & \textcolor{black}{0.056} & \textcolor{black}{0.131}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.00076} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0063} & \textcolor{black}{0.056} & \textcolor{black}{0.131}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.00075} & \textcolor{black}{0.049} & \textcolor{green}{0.16} & \textcolor{black}{0.0057} & \textcolor{black}{0.055} & \textcolor{black}{0.124}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{black}{0.00087} & \textcolor{red}{0.066} & \textcolor{red}{0.17} & \textcolor{black}{0.0070} & \textcolor{red}{0.063} & \textcolor{red}{0.190}\\
\cmidrule{1-8}
 & hc & \textcolor{green}{0.00075} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{green}{0.0049} & \textcolor{black}{0.054} & \textcolor{black}{0.112}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.00075} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{green}{0.0049} & \textcolor{black}{0.054} & \textcolor{black}{0.112}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.00075} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{green}{0.0049} & \textcolor{black}{0.054} & \textcolor{black}{0.112}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{black}{0.00086} & \textcolor{red}{0.066} & \textcolor{red}{0.17} & \textcolor{black}{0.0061} & \textcolor{black}{0.062} & \textcolor{black}{0.170}\\
\cmidrule{1-8}
 & hc & \textcolor{green}{0.00075} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{green}{0.0049} & \textcolor{black}{0.054} & \textcolor{black}{0.112}\\
\cmidrule{2-8}
 & kmeans & \textcolor{green}{0.00075} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{green}{0.0049} & \textcolor{black}{0.054} & \textcolor{black}{0.112}\\
\cmidrule{2-8}
 & random & \textcolor{green}{0.00075} & \textcolor{black}{0.050} & \textcolor{green}{0.16} & \textcolor{green}{0.0049} & \textcolor{black}{0.054} & \textcolor{black}{0.112}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.00086} & \textcolor{red}{0.066} & \textcolor{red}{0.17} & \textcolor{black}{0.0061} & \textcolor{black}{0.062} & \textcolor{black}{0.170}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/bivariate/multivariate_unbalanced_overlapping_opposite_correlated} 

}

\caption{Results of scenario B12 in Table \ref{tab:parameter-configuration-bivariate} (unbalanced, overlapping and opposite correlated components), with the same layout as Figure \ref{fig:multivariate-overlapping-unbalanced-negative-correlated}.}\label{fig:multivariate-overlapping-unbalanced-opposite-correlated}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:multivariate-overlapping-unbalanced-positive-correlated-pdf}MSE and Bias associated to scenario B14, in Table \ref{tab:parameter-configuration-bivariate}  (unbalanced, overlapping and positive correlated components).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00081} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & kmeans & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00081} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00080} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{green}{0.00040} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{red}{0.00120} & \textcolor{red}{0.047} & \textcolor{black}{0.053}\\
\cmidrule{1-8}
 & hc & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{green}{0.00072} & \textcolor{green}{0.043} & \textcolor{green}{0.035}\\
\cmidrule{2-8}
 & kmeans & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{green}{0.00072} & \textcolor{green}{0.043} & \textcolor{green}{0.035}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{green}{0.00072} & \textcolor{black}{0.044} & \textcolor{green}{0.035}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{green}{0.00040} & \textcolor{red}{0.044} & \textcolor{red}{0.14} & \textcolor{black}{0.00110} & \textcolor{red}{0.047} & \textcolor{black}{0.044}\\
\cmidrule{1-8}
 & hc & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00081} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & kmeans & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00081} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00080} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{green}{0.00040} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{red}{0.00120} & \textcolor{red}{0.047} & \textcolor{black}{0.053}\\
\cmidrule{1-8}
 & hc & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00078} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & kmeans & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00078} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00078} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{green}{0.00040} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00110} & \textcolor{red}{0.047} & \textcolor{black}{0.053}\\
\cmidrule{1-8}
 & hc & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00078} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & kmeans & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00078} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
 & random & \textcolor{red}{0.00043} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00078} & \textcolor{black}{0.044} & \textcolor{red}{0.060}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{green}{0.00040} & \textcolor{red}{0.044} & \textcolor{green}{0.13} & \textcolor{black}{0.00110} & \textcolor{red}{0.047} & \textcolor{black}{0.053}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/bivariate/multivariate_unbalanced_overlapping_positive_correlated} 

}

\caption{Results of scenario B14 in Table \ref{tab:parameter-configuration-bivariate} (unbalanced, overlapping and positive correlated components), with the same layout as Figure \ref{fig:multivariate-overlapping-unbalanced-negative-correlated}.}\label{fig:multivariate-overlapping-unbalanced-positive-correlated}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:multivariate-overlapping-unbalanced-uncorrelated-pdf}MSE and Bias associated to scenario B15, in Table \ref{tab:parameter-configuration-bivariate}   (unbalanced, overlapping and uncorrelated components).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}}\\
\midrule
 & hc & \textcolor{black}{0.1110} & \textcolor{black}{2.30} & \textcolor{black}{1.30} & \textcolor{red}{0.280} & \textcolor{black}{1.40} & \textcolor{black}{0.90}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0500} & \textcolor{black}{1.50} & \textcolor{black}{1.30} & \textcolor{black}{0.200} & \textcolor{black}{1.05} & \textcolor{black}{1.06}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0290} & \textcolor{black}{0.71} & \textcolor{green}{0.63} & \textcolor{black}{0.070} & \textcolor{black}{0.28} & \textcolor{green}{0.19}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{EMCluster / GMKMcharlie}} & rebmix & \textcolor{black}{0.0163} & \textcolor{black}{0.69} & \textcolor{black}{0.78} & \textcolor{black}{0.074} & \textcolor{black}{0.37} & \textcolor{black}{0.44}\\
\cmidrule{1-8}
 & hc & \textcolor{red}{0.1330} & \textcolor{red}{2.40} & \textcolor{red}{1.40} & \textcolor{black}{0.240} & \textcolor{red}{1.50} & \textcolor{black}{1.05}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0320} & \textcolor{black}{1.60} & \textcolor{red}{1.40} & \textcolor{black}{0.110} & \textcolor{black}{1.21} & \textcolor{red}{1.26}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0370} & \textcolor{black}{0.71} & \textcolor{black}{0.64} & \textcolor{black}{0.048} & \textcolor{black}{0.35} & \textcolor{black}{0.29}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{flexmix}} & rebmix & \textcolor{green}{0.0058} & \textcolor{black}{0.70} & \textcolor{black}{0.84} & \textcolor{green}{0.028} & \textcolor{black}{0.49} & \textcolor{black}{0.62}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.1110} & \textcolor{black}{2.30} & \textcolor{black}{1.30} & \textcolor{red}{0.280} & \textcolor{black}{1.40} & \textcolor{black}{0.90}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0500} & \textcolor{black}{1.50} & \textcolor{black}{1.30} & \textcolor{black}{0.200} & \textcolor{black}{1.05} & \textcolor{black}{1.06}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0290} & \textcolor{black}{0.71} & \textcolor{green}{0.63} & \textcolor{black}{0.070} & \textcolor{black}{0.28} & \textcolor{green}{0.19}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mclust / bgmm}} & rebmix & \textcolor{black}{0.0163} & \textcolor{black}{0.69} & \textcolor{black}{0.78} & \textcolor{black}{0.074} & \textcolor{black}{0.37} & \textcolor{black}{0.44}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.0860} & \textcolor{black}{1.90} & \textcolor{black}{1.20} & \textcolor{black}{0.220} & \textcolor{black}{1.10} & \textcolor{black}{0.75}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0470} & \textcolor{black}{1.30} & \textcolor{black}{1.10} & \textcolor{black}{0.170} & \textcolor{black}{0.79} & \textcolor{black}{0.78}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0230} & \textcolor{green}{0.67} & \textcolor{black}{0.66} & \textcolor{black}{0.065} & \textcolor{green}{0.24} & \textcolor{green}{0.19}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{mixtools}} & rebmix & \textcolor{black}{0.0158} & \textcolor{black}{0.69} & \textcolor{black}{0.77} & \textcolor{black}{0.068} & \textcolor{black}{0.30} & \textcolor{black}{0.37}\\
\cmidrule{1-8}
 & hc & \textcolor{black}{0.0860} & \textcolor{black}{1.90} & \textcolor{black}{1.20} & \textcolor{black}{0.220} & \textcolor{black}{1.10} & \textcolor{black}{0.75}\\
\cmidrule{2-8}
 & kmeans & \textcolor{black}{0.0470} & \textcolor{black}{1.30} & \textcolor{black}{1.10} & \textcolor{black}{0.170} & \textcolor{black}{0.79} & \textcolor{black}{0.78}\\
\cmidrule{2-8}
 & random & \textcolor{black}{0.0230} & \textcolor{green}{0.67} & \textcolor{black}{0.66} & \textcolor{black}{0.065} & \textcolor{green}{0.24} & \textcolor{green}{0.19}\\
\cmidrule{2-8}
\multirow{-4}{*}{\raggedright\arraybackslash \textbf{Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.0158} & \textcolor{black}{0.69} & \textcolor{black}{0.77} & \textcolor{black}{0.068} & \textcolor{black}{0.30} & \textcolor{black}{0.37}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/bivariate/multivariate_unbalanced_uncorrelated} 

}

\caption{Results of scenario B15 in Table \ref{tab:parameter-configuration-bivariate} (unbalanced, overlapping and uncorrelated components), with the same layout as Figure \ref{fig:multivariate-overlapping-unbalanced-negative-correlated}.}\label{fig:multivariate-overlapping-unbalanced-uncorrelated}
\end{figure}

In contrast to the univariate setting (\protect\hyperlink{supplementary-figures-and-tables-in-the-univariate-simulation}{Supplementary Figures and Tables in the univariate simulation}), the fastest packages are \pkg{bgmm}, \pkg{EMCluster}, \pkg{flexmix}, and \pkg{Rmixmod}, and the slowest ones \pkg{mclust}, \pkg{GMKMcharlie} and \pkg{mixtools}, independently from the difficulty of the simulation.

Finally, Figures \ref{fig:general-balanced-overlapping-bivariate}, \ref{fig:general-balanced-well-separated-bivariate} and \ref{fig:general-unbalanced-well-separated-bivariate} represent in a synthetic way less interesting scenarios benchmarked with to the left, the contour maps and to the right the corresponding Hellinger boxplots, with one scenario being illustrated per row.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/bivariate/multivariate_balanced_overlapping} 

}

\caption{Benchmark summary plots of respectively scenarios B1, B2 and B5 in Table \ref{tab:parameter-configuration-bivariate} featuring balanced and overlapping clusters. Summary plots of B1, B2 and B5 are represented in this order on each row, with the left column displaying the $95\%$ confidence ellipsoidal regions associated to the mean estimated parameters across each package and the right column the distribution of the Hellinger distances.}\label{fig:general-balanced-overlapping-bivariate}
\end{figure}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/bivariate/multivariate_balanced_well_separated} 

}

\caption{Benchmark summary plots of respectively scenarios B6, B7 and B10 in Table \ref{tab:parameter-configuration-bivariate} featuring balanced and well-separated clusters, with the same layout as Figure \ref{fig:general-balanced-overlapping-bivariate}.}\label{fig:general-balanced-well-separated-bivariate}
\end{figure}

\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figs/bivariate/multivariate_unbalanced_well_separated} 

}

\caption{Benchmark summary plots of respectively scenarios B16, B17 and B20 in Table \ref{tab:parameter-configuration-bivariate} featuring unbalanced and well-separated clusters, with the same layout as Figure \ref{fig:general-balanced-overlapping-bivariate}.}\label{fig:general-unbalanced-well-separated-bivariate}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figs/bivariate/heatmap_bivariate} 

}

\caption{Correlation heatmaps of the estimated parameters in the bivariate setting extended to the four initialisation methods benchmarked, with the most discriminating scenario B11, using the same process described in Figure \ref{fig:dichotomy-package-conclusion}.}\label{fig:heatmap-all-correlation-plots-bivariate}
\end{figure}

\hypertarget{supplementary-figures-and-tables-in-the-hd-simulation}{%
\subsection{Supplementary Figures and Tables in the HD simulation}\label{supplementary-figures-and-tables-in-the-hd-simulation}}

Table below (\ref{tab:parameter-configuration-HD}) lists the complete set of parameters used to simulate Gaussian distributions in the high dimensional benchmark:

\begin{table}[!h]

\caption{\label{tab:parameter-configuration-HD}The 16 parameter configurations tested to generate the samples in a high dimensional context. The first digit of each ID index refers
      to an unique parameter configuration (identified by its level of overlap, entropy and topological structure, either circular or ellipsoidal,
      of the covariance matrix, while the lowercase letter depicts the number of observations, a) with $n=200$ and b) with $n=2000$.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{ccccc}
\toprule
\textbf{ID} & \textbf{OVL} & \textbf{\makecell[r]{Number of \\observations}} & \textbf{Proportions} & \textbf{Spherical}\\
\midrule
HD1a & 1e-04 & 200 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD1b & 1e-04 & 2000 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD2a & 1e-04 & 200 & 0.19 / 0.81 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD2b & 1e-04 & 2000 & 0.19 / 0.81 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD3a & 1e-04 & 200 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
\addlinespace
HD3b & 1e-04 & 2000 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
HD4a & 1e-04 & 200 & 0.21 / 0.79 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
HD4b & 1e-04 & 2000 & 0.21 / 0.79 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
HD5a & 2e-01 & 200 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD5b & 2e-01 & 2000 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
\addlinespace
HD6a & 2e-01 & 200 & 0.15 / 0.85 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD6b & 2e-01 & 2000 & 0.15 / 0.85 & \includegraphics[scale=0.05]{figs/green_tick.png}\\
\midrule
HD7a & 2e-01 & 200 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
HD7b & 2e-01 & 2000 & 0.5 / 0.5 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
HD8a & 2e-01 & 200 & 0.69 / 0.31 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
\addlinespace
HD8b & 2e-01 & 2000 & 0.69 / 0.31 & \includegraphics[scale=0.05]{figs/red_cross.png}\\
\midrule
\bottomrule
\end{tabular}}
\end{table}

\newpage

\begin{table}[!h]

\caption{\label{tab:HD-separated-unbalanced-ellipsoidal-pdf}MSE and Bias associated to scenario HD4a, in Table
      \ref{tab:parameter-configuration-HD} (unbalanced, separated and ellipsoidal components).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}} & \multicolumn{1}{c}{\textbf{$\%$ Success}}\\
\midrule
 & hc & \textcolor{green}{0.0333} & \textcolor{green}{0.0212} & \textcolor{green}{0.0106} & \textcolor{black}{0.0020} & \textcolor{green}{0.056} & \textcolor{black}{0.097} & \textcolor{green}{100}\\

 & kmeans & \textcolor{green}{0.0333} & \textcolor{green}{0.0212} & \textcolor{green}{0.0106} & \textcolor{black}{0.0020} & \textcolor{green}{0.056} & \textcolor{black}{0.097} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mixtools / Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.3244} & \textcolor{black}{0.1980} & \textcolor{black}{0.0845} & \textcolor{black}{0.0720} & \textcolor{black}{0.395} & \textcolor{black}{0.535} & \textcolor{black}{98}\\
\cmidrule{1-9}
 & hc & \textcolor{green}{0.0333} & \textcolor{green}{0.0212} & \textcolor{green}{0.0106} & \textcolor{black}{0.0020} & \textcolor{green}{0.056} & \textcolor{black}{0.097} & \textcolor{green}{100}\\

 & kmeans & \textcolor{green}{0.0333} & \textcolor{green}{0.0212} & \textcolor{green}{0.0106} & \textcolor{black}{0.0020} & \textcolor{green}{0.056} & \textcolor{black}{0.097} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mclust / flexmix / GMKMcharlie}} & rebmix & \textcolor{black}{0.2553} & \textcolor{black}{0.1444} & \textcolor{black}{0.0924} & \textcolor{black}{0.0470} & \textcolor{black}{0.364} & \textcolor{black}{0.596} & \textcolor{black}{85}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{0.0337} & \textcolor{black}{0.0214} & \textcolor{black}{0.0107} & \textcolor{black}{0.0070} & \textcolor{black}{0.064} & \textcolor{green}{0.096} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{0.0338} & \textcolor{black}{0.0216} & \textcolor{green}{0.0106} & \textcolor{black}{0.0074} & \textcolor{black}{0.064} & \textcolor{green}{0.096} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{bgmm}} & rebmix & \textcolor{black}{0.4818} & \textcolor{black}{0.1152} & \textcolor{black}{0.3442} & \textcolor{black}{0.0320} & \textcolor{black}{0.223} & \textcolor{black}{2.329} & \textcolor{black}{94}\\
\cmidrule{1-9}
 & hc & \textcolor{green}{0.0333} & \textcolor{green}{0.0212} & \textcolor{black}{0.0107} & \textcolor{black}{0.0023} & \textcolor{green}{0.056} & \textcolor{green}{0.096} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{0.0334} & \textcolor{black}{0.0213} & \textcolor{green}{0.0106} & \textcolor{green}{0.0018} & \textcolor{green}{0.056} & \textcolor{green}{0.096} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMCluster}} & rebmix & \textcolor{black}{1.5983} & \textcolor{black}{1.0992} & \textcolor{red}{0.3794} & \textcolor{black}{0.3100} & \textcolor{black}{2.018} & \textcolor{black}{2.575} & \textcolor{black}{84}\\
\midrule
\cmidrule{1-9}
 & hc & \textcolor{red}{8.4062} & \textcolor{red}{8.3936} & \textcolor{black}{0.0111} & \textcolor{black}{0.0020} & \textcolor{red}{10.426} & \textcolor{black}{0.149} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{7.9407} & \textcolor{black}{7.9282} & \textcolor{black}{0.0111} & \textcolor{black}{0.0019} & \textcolor{black}{10.081} & \textcolor{black}{0.149} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{HDclassif}} & rebmix & \textcolor{black}{7.9803} & \textcolor{black}{7.9514} & \textcolor{black}{0.0273} & \textcolor{black}{0.0044} & \textcolor{black}{10.128} & \textcolor{black}{0.262} & \textcolor{black}{84}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{4.0605} & \textcolor{black}{3.3317} & \textcolor{black}{0.3357} & \textcolor{red}{0.6500} & \textcolor{black}{5.757} & \textcolor{black}{2.772} & \textcolor{black}{95}\\

 & kmeans & \textcolor{black}{3.8790} & \textcolor{black}{3.2175} & \textcolor{black}{0.3372} & \textcolor{black}{0.5400} & \textcolor{black}{5.781} & \textcolor{red}{2.777} & \textcolor{black}{96}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMMIXmfa}} & rebmix & \textcolor{black}{4.0127} & \textcolor{black}{3.2715} & \textcolor{black}{0.3337} & \textcolor{black}{0.5700} & \textcolor{black}{5.680} & \textcolor{black}{2.757} & \textcolor{red}{80}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figs/HD/HD-separated-unbalanced-ellipsoidal} 

}

\caption{Results of scenario HD4a) in Table \ref{tab:parameter-configuration-HD} (unbalanced, overlapping and negative correlated components), organised as such:
The panel A displays the bivariate factorial projection of a random sample drawn from the 10-dimensional multivariate Gaussian distribution parametrised by Table \ref{tab:parameter-configuration-HD}. Each component is associated to a specific color, a centroid whose coordinates are given by the mean components' elements in the bivariate projected space and a $95\%$ confidence ellipse. Arrows represent the correlation circle of the dimensional variables.
 Both panels were displayed respectively using functions \code{factoextra::fviz\_eig} and \code{factoextra::fviz\_pca\_biplot} while the underlying computations proceed from the principal component analysis performed by \code{ade4::dudi.pca} preceded by standard scaling of the sampling dataset.
The panel B pictures the \textit{parallel distribution plots} from a random sampling of $n=100$ observations, generated using \code{GGally::ggparcoord}, and representing the coordinates of each simulated data point in 10 dimensions. 
The running times are displayed in Panel C with the \textit{k}-means initialisation. The number of observations (x-axis) and the running time (y-axis) is in $\log(10)$ scale.
The distributions of the Hellinger distances are computed for each component in Panel D, each initialisation method and each package with respect to the true Gaussian distribution expected for each component. 
In panel E we represent the boxplots associated with the distribution of some of the estimates. Since it was impractical to represent all of the $k + kD + k\frac{D \times (D+1)}{2}$ with $k=2$ and $D=10$ parameters, we only represent the first component's mean, two first components' variances and their covariance term.}\label{fig:HD-separated-unbalanced-ellipsoidal-plot}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:HD-overlapping-balanced-ellipsoidal-pdf}MSE and Bias associated to scenario HD7a, in Table
      \ref{tab:parameter-configuration-HD} (balanced, overlapping and ellipsoidal components).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}r>{}r>{}r>{}r>{}r>{}r>{}r}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}} & \multicolumn{1}{c}{\textbf{$\%$ Success}}\\
\midrule
 & hc & \textcolor{black}{5.8544} & \textcolor{black}{2.2153} & \textcolor{black}{3.5586} & \textcolor{black}{0.0450} & \textcolor{black}{2.084} & \textcolor{black}{6.704} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{5.4773} & \textcolor{black}{1.9490} & \textcolor{black}{3.4819} & \textcolor{green}{0.0086} & \textcolor{black}{2.323} & \textcolor{black}{6.861} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mixtools / Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{6.9243} & \textcolor{black}{2.5185} & \textcolor{black}{4.2898} & \textcolor{black}{0.0620} & \textcolor{black}{2.670} & \textcolor{black}{7.626} & \textcolor{black}{97}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{6.0584} & \textcolor{black}{2.4737} & \textcolor{black}{3.5198} & \textcolor{black}{0.0180} & \textcolor{black}{2.565} & \textcolor{black}{7.624} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{5.6388} & \textcolor{black}{2.1597} & \textcolor{black}{3.4549} & \textcolor{black}{0.0140} & \textcolor{black}{2.744} & \textcolor{black}{8.266} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mclust / flexmix / GMKMcharlie}} & rebmix & \textcolor{black}{6.5397} & \textcolor{black}{2.4738} & \textcolor{black}{3.9661} & \textcolor{black}{0.0700} & \textcolor{black}{2.774} & \textcolor{black}{7.764} & \textcolor{black}{93}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{9.5015} & \textcolor{black}{5.1348} & \textcolor{black}{4.1086} & \textcolor{black}{0.1000} & \textcolor{black}{3.720} & \textcolor{black}{10.310} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{8.7930} & \textcolor{black}{4.7119} & \textcolor{black}{3.8693} & \textcolor{black}{0.1500} & \textcolor{black}{3.932} & \textcolor{black}{10.108} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{bgmm}} & rebmix & \textcolor{black}{10.3630} & \textcolor{black}{5.6474} & \textcolor{black}{4.4026} & \textcolor{red}{0.2700} & \textcolor{black}{3.798} & \textcolor{black}{10.049} & \textcolor{black}{97}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{6.4022} & \textcolor{black}{2.8255} & \textcolor{black}{3.5124} & \textcolor{black}{0.0120} & \textcolor{black}{3.141} & \textcolor{black}{9.086} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{6.4333} & \textcolor{black}{2.8740} & \textcolor{black}{3.5523} & \textcolor{black}{0.0110} & \textcolor{black}{4.210} & \textcolor{red}{11.007} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMCluster}} & rebmix & \textcolor{black}{6.5527} & \textcolor{black}{2.9643} & \textcolor{black}{3.4862} & \textcolor{black}{0.0580} & \textcolor{black}{3.051} & \textcolor{black}{9.253} & \textcolor{black}{93}\\
\midrule
\cmidrule{1-9}
 & hc & \textcolor{black}{15.9010} & \textcolor{red}{11.5382} & \textcolor{black}{4.2950} & \textcolor{black}{0.1400} & \textcolor{black}{10.846} & \textcolor{black}{10.100} & \textcolor{green}{100}\\

 & kmeans & \textcolor{black}{15.3377} & \textcolor{black}{10.9441} & \textcolor{black}{4.3716} & \textcolor{black}{0.0087} & \textcolor{red}{10.990} & \textcolor{black}{10.771} & \textcolor{green}{100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{HDclassif}} & rebmix & \textcolor{red}{16.1231} & \textcolor{black}{11.1103} & \textcolor{red}{4.9113} & \textcolor{black}{0.1600} & \textcolor{black}{10.761} & \textcolor{black}{10.513} & \textcolor{black}{93}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{4.8606} & \textcolor{black}{1.6546} & \textcolor{black}{3.1856} & \textcolor{black}{0.0160} & \textcolor{black}{2.030} & \textcolor{black}{7.395} & \textcolor{red}{15}\\

 & kmeans & \textcolor{green}{4.4039} & \textcolor{green}{1.4129} & \textcolor{green}{2.9701} & \textcolor{black}{0.0260} & \textcolor{green}{1.734} & \textcolor{green}{6.236} & \textcolor{black}{21}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMMIXmfa}} & rebmix & \textcolor{black}{5.0984} & \textcolor{black}{2.0057} & \textcolor{black}{3.0689} & \textcolor{black}{0.0470} & \textcolor{black}{2.314} & \textcolor{black}{7.613} & \textcolor{black}{16}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figs/HD/HD-overlapping-balanced-ellipsoidal} 

}

\caption{Results of scenario HD7a) in Table \ref{tab:parameter-configuration-HD} (balanced and overlapping components, with full covariance structure), with the same layout as Figure \ref{fig:HD-separated-unbalanced-ellipsoidal-plot}.}\label{fig:HD-overlapping-balanced-ellipsoidal-plot}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:HD-overlapping-spherical-pdf}MSE and Bias associated to scenarios HD5a) and HD6a), in Table
\ref{tab:parameter-configuration-HD} (overlapping and spherical-distributed components). We delimite each scenario by doubled backslashes with respectively balanced and unbalanced clusters.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}l>{}l>{}l>{}l>{}l>{}l>{}l}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}} & \multicolumn{1}{c}{\textbf{$\%$ Success}}\\
\midrule
 & hc & \textcolor{black}{4.2772 // 19.5172} & \textcolor{black}{0.9198 // 1.9835} & \textcolor{black}{3.3027 // 17.4943} & \textcolor{black}{0.017 // 0.069} & \textcolor{black}{0.995 // 0.6} & \textcolor{black}{3.571 // 4.381} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{3.9776 // 17.2212} & \textcolor{black}{0.8279 // 1.6336} & \textcolor{black}{3.1111 // 15.5684} & \textcolor{black}{0.072 // 0.069} & \textcolor{black}{0.841 // 0.82} & \textcolor{black}{3.023 // 5.034} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mixtools / Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{9.3136 // 25.8028} & \textcolor{black}{2.7793 // 4.2893} & \textcolor{black}{6.4009 // 21.3519} & \textcolor{black}{0.15 // 0.22} & \textcolor{black}{3.619 // 2.507} & \textcolor{black}{9.061 // 11.826} & \textcolor{black}{96 // 80}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{2.9743 // 18.1175} & \textcolor{black}{0.5862 // 1.7729} & \textcolor{black}{2.3612 // 16.3168} & \textcolor{green}{0.024 // 0.057} & \textcolor{green}{0.449 // 0.514} & \textcolor{green}{2.127 // 4.412} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{2.5629 // 15.2959} & \textcolor{green}{0.4642 // 1.5608} & \textcolor{black}{2.0855 // 13.7206} & \textcolor{black}{0.085 // 0.086} & \textcolor{black}{0.671 // 1.047} & \textcolor{black}{1.67 // 5.801} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mclust / flexmix / GMKMcharlie}} & rebmix & \textcolor{black}{8.2907 // 23.7588} & \textcolor{black}{2.6468 // 4.1629} & \textcolor{black}{5.5421 // 19.4579} & \textcolor{black}{0.12 // 0.22} & \textcolor{black}{3.438 // 2.543} & \textcolor{black}{8.792 // 11.94} & \textcolor{black}{96 // 69}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{2.4088 // 33.8392} & \textcolor{black}{0.7261 // 9.0609} & \textcolor{black}{1.6153 // 24.6796} & \textcolor{black}{0.12 // 0.038} & \textcolor{black}{0.652 // 1.986} & \textcolor{black}{1.98 // 10.77} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{2.0912 // 28.5103} & \textcolor{black}{0.5899 // 7.5426} & \textcolor{black}{1.4577 // 20.8989} & \textcolor{black}{0.091 // 0.025} & \textcolor{black}{0.566 // 1.45} & \textcolor{black}{1.738 // 9.783} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{bgmm}} & rebmix & \textcolor{black}{4.6278 // 35.9294} & \textcolor{black}{1.9526 // 11.0184} & \textcolor{black}{2.5372 // 24.6276} & \textcolor{black}{0.048 // 0.22} & \textcolor{black}{0.632 // 2.023} & \textcolor{black}{2.96 // 12.729} & \textcolor{black}{98 // 86}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{2.5152 // 17.7053} & \textcolor{black}{0.5087 // 2.1191} & \textcolor{black}{1.9849 // 15.5379} & \textcolor{black}{0.024 // 0.12} & \textcolor{black}{0.321 // 0.929} & \textcolor{black}{1.512 // 5.611} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{green}{1.793 // 12.8799} & \textcolor{black}{0.3527 // 1.6839} & \textcolor{green}{1.4344 // 11.155} & \textcolor{black}{0.062 // 0.24} & \textcolor{black}{0.593 // 2.177} & \textcolor{black}{2.547 // 9.595} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMCluster}} & rebmix & \textcolor{black}{6.9275 // 23.0817} & \textcolor{black}{2.7461 // 5.4713} & \textcolor{black}{4.0985 // 17.4511} & \textcolor{black}{0.044 // 0.32} & \textcolor{black}{3.177 // 3.836} & \textcolor{black}{8.535 // 15.437} & \textcolor{black}{96 // 70}\\
\midrule
\cmidrule{1-9}
 & hc & \textcolor{black}{11.4938 // 49.4328} & \textcolor{black}{9.1746 // 12.2155} & \textcolor{black}{2.2913 // 36.5886} & \textcolor{black}{0.027 // 0.91} & \textcolor{red}{8.899 // 9.56} & \textcolor{black}{1.98 // 19.55} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{11.1438 // 40.4749} & \textcolor{black}{9.0384 // 11.9946} & \textcolor{black}{2.0912 // 28.0385} & \textcolor{black}{0.096 // 0.7} & \textcolor{black}{9.059 // 9.024} & \textcolor{black}{1.682 // 16.35} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{HDclassif}} & rebmix & \textcolor{red}{14.6998 // 47.2364} & \textcolor{red}{8.7649 // 12.6715} & \textcolor{red}{5.8029 // 33.929} & \textcolor{red}{0.22 // 0.92} & \textcolor{black}{8.135 // 9.145} & \textcolor{red}{8.018 // 21.824} & \textcolor{black}{96 // 70}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{5.6809 // 21.1181} & \textcolor{black}{3.7272 // 6.1206} & \textcolor{black}{1.7452 // 14.9126} & \textcolor{black}{0.41 // 0.019} & \textcolor{black}{5.772 // 3.645} & \textcolor{black}{4.299 // 12.812} & \textcolor{black}{96 // 45}\\

 & kmeans & \textcolor{black}{5.7063 // 21.3775} & \textcolor{black}{3.6759 // 6.589} & \textcolor{black}{1.79 // 14.5681} & \textcolor{black}{0.39 // 0.17} & \textcolor{black}{5.788 // 4.08} & \textcolor{black}{4.357 // 13.352} & \textcolor{black}{96 // 40}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMMIXmfa}} & rebmix & \textcolor{black}{5.8175 // 19.9703} & \textcolor{black}{3.8142 // 6.3202} & \textcolor{black}{1.7592 // 13.5389} & \textcolor{black}{0.35 // 0.033} & \textcolor{black}{5.819 // 4.402} & \textcolor{black}{4.349 // 13.812} & \textcolor{red}{93 // 34}\\
\bottomrule
\end{tabu}}
\end{table}

\begin{table}[!h]

\caption{\label{tab:HD-overlapping-spherical-pdf-offterms}Minimal example setting apart MSE and Bias whether it proceeds from
  diagonal or offset terms of the covariance matrix, for scenarios HD5a) and HD6a), in Table
\ref{tab:parameter-configuration-HD} (overlapping and spherical-distributed components).
      We delimite each scenario by doubled backslashes with respectively balanced and unbalanced clusters.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}l>{}l>{}l>{}l}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global\\MSE $\text{diag} (\boldsymbol{\Sigma})$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\text{upper.tri} (\boldsymbol{\Sigma})$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\text{diag} (\boldsymbol{\Sigma})$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global\\Bias $\text{upper.tri} (\boldsymbol{\Sigma})$}}}\\
\midrule
 & hc & \textcolor{black}{1.1 // 5.9} & \textcolor{black}{2.2 // 12} & \textcolor{black}{0.9194 // 2.3003} & \textcolor{black}{2.651 // 2.081}\\

\multirow{-2}{*}{\raggedright\arraybackslash \textbf{mixtools / Rmixmod / RGMMBench}} & kmeans & \textcolor{black}{0.99 // 5.6} & \textcolor{black}{2.1 // 10} & \textcolor{black}{0.8929 // 2.7422} & \textcolor{black}{2.13 // 2.292}\\
\cmidrule{1-6}
 & hc & \textcolor{black}{0.76 // 5.5} & \textcolor{black}{1.6 // 11} & \textcolor{green}{0.5698 // 2.418} & \textcolor{black}{1.557 // 1.994}\\

\multirow{-2}{*}{\raggedright\arraybackslash \textbf{mclust / flexmix / GMKMcharlie}} & kmeans & \textcolor{green}{0.67 // 5.2} & \textcolor{black}{1.4 // 8.5} & \textcolor{black}{0.6909 // 3.4316} & \textcolor{black}{0.979 // 2.37}\\
\cmidrule{1-6}
 & hc & \textcolor{black}{0.67 // 11} & \textcolor{red}{0.94 // 14} & \textcolor{black}{0.7755 // 6.9204} & \textcolor{red}{1.205 // 3.849}\\

\multirow{-2}{*}{\raggedright\arraybackslash \textbf{bgmm}} & kmeans & \textcolor{black}{0.58 // 9.1} & \textcolor{black}{0.88 // 12} & \textcolor{black}{0.6004 // 6.124} & \textcolor{black}{1.138 // 3.659}\\
\cmidrule{1-6}
 & hc & \textcolor{black}{0.62 // 6.1} & \textcolor{black}{1.4 // 9.5} & \textcolor{black}{0.4985 // 3.4685} & \textcolor{black}{1.013 // 2.143}\\

\multirow{-2}{*}{\raggedright\arraybackslash \textbf{EMCluster}} & kmeans & \textcolor{black}{0.48 // 7.5} & \textcolor{black}{0.95 // 3.6} & \textcolor{black}{0.9269 // 7.6352} & \textcolor{black}{1.621 // 1.96}\\
\midrule
\cmidrule{1-6}
 & hc & \textcolor{red}{0.72 // 26} & \textcolor{black}{1.6 // 11} & \textcolor{red}{0.5383 // 17.2225} & \textcolor{black}{1.441 // 2.328}\\

\multirow{-2}{*}{\raggedright\arraybackslash \textbf{HDclassif}} & kmeans & \textcolor{black}{0.68 // 20} & \textcolor{black}{1.4 // 8.5} & \textcolor{black}{0.7156 // 13.7249} & \textcolor{black}{0.966 // 2.626}\\
\cmidrule{1-6}
 & hc & \textcolor{black}{1.6 // 10} & \textcolor{black}{0.13 // 4.6} & \textcolor{black}{3.7632 // 10.0798} & \textcolor{black}{0.536 // 2.733}\\

\multirow{-2}{*}{\raggedright\arraybackslash \textbf{EMMIXmfa}} & kmeans & \textcolor{black}{1.6 // 11} & \textcolor{green}{0.17 // 3.9} & \textcolor{black}{3.7621 // 10.8057} & \textcolor{green}{0.594 // 2.546}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figs/HD/HD-overlapping-spherical} 

}

\caption{We gathered on the same plot two multivariate benchmark scenarios, in which we consider a strictly spherical structure of the covariance matrix:
We represent in Panel A and B, respectively the bivariate projection and parallel distribution plot, associated to scenario HD5a) in Table \ref{tab:parameter-configuration-HD}  (balanced and overlapping components, with spherical covariance structure).
In Panel C, we display the boxplots associated to scenario HD5a), computing them similarly as in Panel E of Figure \ref{fig:HD-separated-unbalanced-ellipsoidal-plot}.
In Panel D, we display the boxplots associated to scenario HD6a) (unbalanced and overlapping components, with spherical covariance structure).}\label{fig:HD-overlapping-spherical-plot}
\end{figure}

\newpage

\begin{table}[!h]

\caption{\label{tab:HD-impact-num-observations-pdf-tab1}MSE and Bias associated to scenarios HD1a) and HD1b), in Table
      \ref{tab:parameter-configuration-HD} (well-separated and spherical-distributed components).
      We delimite by doubled backslashes for each entry of the summary metrics table respectively the scores with $n=200$ and $n=2000$ observations.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}l>{}l>{}l>{}l>{}l>{}l>{}l}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}} & \multicolumn{1}{c}{\textbf{$\%$ Success}}\\
\midrule
 & hc & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0097 // 0.00071} & \textcolor{green}{0.053 // 0.018} & \textcolor{green}{0.139 // 0.04} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0097 // 0.00071} & \textcolor{green}{0.053 // 0.018} & \textcolor{green}{0.139 // 0.04} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mixtools / Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{0.5611 // 0.0058} & \textcolor{black}{0.2364 // 0.0028} & \textcolor{black}{0.3035 // 0.0026} & \textcolor{black}{0.019 // 0.00071} & \textcolor{black}{0.372 // 0.018} & \textcolor{black}{0.915 // 0.04} & \textcolor{black}{98 // 100}\\
\cmidrule{1-9}
 & hc & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0095 // 0.00071} & \textcolor{green}{0.053 // 0.018} & \textcolor{black}{0.14 // 0.04} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0095 // 0.00071} & \textcolor{green}{0.053 // 0.018} & \textcolor{black}{0.14 // 0.04} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mclust / flexmix / GMKMcharlie}} & rebmix & \textcolor{black}{0.3134 // 0.0058} & \textcolor{black}{0.1305 // 0.0029} & \textcolor{black}{0.1729 // 0.0026} & \textcolor{black}{0.0039 // 0.0022} & \textcolor{black}{0.2 // 0.02} & \textcolor{black}{0.537 // 0.044} & \textcolor{black}{88 // 81}\\
\cmidrule{1-9}
 & hc & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0098 // 0.00071} & \textcolor{green}{0.053 // 0.018} & \textcolor{black}{0.139 // 0.041} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0097 // 0.00071} & \textcolor{green}{0.053 // 0.018} & \textcolor{green}{0.139 // 0.04} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{bgmm}} & rebmix & \textcolor{black}{0.7437 // 0.1977} & \textcolor{black}{0.3409 // 0.0028} & \textcolor{black}{0.3895 // 0.1946} & \textcolor{black}{0.02 // 0.00034} & \textcolor{black}{0.308 // 0.017} & \textcolor{black}{1.602 // 0.926} & \textcolor{black}{97 // 99}\\
\cmidrule{1-9}
 & hc & \textcolor{green}{0.0577 // 0.0058} & \textcolor{black}{0.0289 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0093 // 0.00061} & \textcolor{black}{0.054 // 0.018} & \textcolor{black}{0.139 // 0.041} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{green}{0.0577 // 0.0058} & \textcolor{green}{0.0288 // 0.0028} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0092 // 0.00039} & \textcolor{black}{0.054 // 0.017} & \textcolor{green}{0.139 // 0.04} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMCluster}} & rebmix & \textcolor{black}{0.6887 // 0.3391} & \textcolor{black}{0.2466 // 0.1276} & \textcolor{black}{0.4201 // 0.1969} & \textcolor{black}{0.019 // 0.048} & \textcolor{black}{0.519 // 0.289} & \textcolor{black}{1.721 // 0.875} & \textcolor{red}{87 // 81}\\
\midrule
\cmidrule{1-9}
 & hc & \textcolor{black}{11.3787 // 11.3322} & \textcolor{black}{11.3499 // 11.3293} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0094 // 0.00071} & \textcolor{black}{11.166 // 11.179} & \textcolor{green}{0.139 // 0.04} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{11.3831 // 11.3301} & \textcolor{black}{11.3543 // 11.3271} & \textcolor{green}{0.0264 // 0.0026} & \textcolor{black}{0.0094 // 0.00068} & \textcolor{black}{11.162 // 11.181} & \textcolor{green}{0.139 // 0.04} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{HDclassif}} & rebmix & \textcolor{red}{11.5949 // 11.6085} & \textcolor{red}{11.5167 // 11.6055} & \textcolor{black}{0.072 // 0.0026} & \textcolor{green}{0.0024 // 0.0022} & \textcolor{red}{11.227 // 11.369} & \textcolor{black}{0.27 // 0.044} & \textcolor{red}{87 // 81}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{5.9739 // 5.9149} & \textcolor{black}{4.0397 // 3.9727} & \textcolor{black}{1.8288 // 1.8228} & \textcolor{black}{0.32 // 0.47} & \textcolor{black}{6.999 // 7.042} & \textcolor{black}{4.25 // 4.296} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{5.972 // 5.8863} & \textcolor{black}{4.0431 // 3.9596} & \textcolor{red}{1.8283 // 1.8259} & \textcolor{black}{0.33 // 0.43} & \textcolor{black}{6.997 // 7.051} & \textcolor{red}{4.255 // 4.34} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMMIXmfa}} & rebmix & \textcolor{black}{5.9835 // 5.9078} & \textcolor{black}{4.0477 // 3.9671} & \textcolor{black}{1.8257 // 1.8244} & \textcolor{red}{0.37 // 0.46} & \textcolor{black}{6.994 // 7.045} & \textcolor{black}{4.232 // 4.301} & \textcolor{red}{87 // 81}\\
\bottomrule
\end{tabu}}
\end{table}

\begin{table}[!h]

\caption{\label{tab:HD-impact-num-observations-pdf-tab2}MSE and Bias associated to scenarios HD8a) and HD8b), in Table
      \ref{tab:parameter-configuration-HD} (overlapping components with full covariance structure).
      We delimite by doubled backslashes for each entry of the summary metrics table respectively the scores with $n=200$ and
              $n=2000$ observations.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabu} to \linewidth {>{}l>{}l>{}l>{}l>{}l>{}l>{}l>{}l>{}l}
\toprule
\multicolumn{1}{c}{\textbf{Package}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Initialisation\\Method}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ MSE $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\MSE $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\MSE $\sigma$}}} & \multicolumn{1}{c}{\textbf{\makecell[r]{Global \\ Bias $p$}}} & \multicolumn{1}{c}{\textbf{\makecell[l]{Global\\Bias $\mu$}}} & \multicolumn{1}{c}{\textbf{\makecell[c]{Global\\Bias $\sigma$}}} & \multicolumn{1}{c}{\textbf{$\%$ Success}}\\
\midrule
 & hc & \textcolor{black}{18.6085 // 0.6735} & \textcolor{black}{3.566 // 0.0536} & \textcolor{black}{14.9495 // 0.6193} & \textcolor{black}{0.23 // 0.0017} & \textcolor{black}{3.327 // 0.107} & \textcolor{black}{14.475 // 0.649} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{green}{16.7452 // 0.6735} & \textcolor{green}{2.9065 // 0.0536} & \textcolor{green}{13.7662 // 0.6193} & \textcolor{green}{0.2 // 0.0016} & \textcolor{green}{2.819 // 0.107} & \textcolor{green}{12.552 // 0.649} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mixtools / Rmixmod / RGMMBench}} & rebmix & \textcolor{black}{22.2986 // 0.6738} & \textcolor{black}{4.3127 // 0.0536} & \textcolor{black}{17.8418 // 0.6196} & \textcolor{black}{0.25 // 0.0021} & \textcolor{black}{3.768 // 0.108} & \textcolor{black}{16.249 // 0.648} & \textcolor{black}{95 // 100}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{20.6916 // 0.728} & \textcolor{black}{4.5672 // 0.0696} & \textcolor{black}{16.0328 // 0.6557} & \textcolor{black}{0.28 // 0.064} & \textcolor{black}{4.381 // 0.459} & \textcolor{black}{18.656 // 2.07} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{17.9622 // 0.7169} & \textcolor{black}{3.7547 // 0.0671} & \textcolor{black}{14.1405 // 0.6474} & \textcolor{black}{0.27 // 0.062} & \textcolor{black}{3.88 // 0.465} & \textcolor{black}{16.802 // 2.049} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{mclust / flexmix / GMKMcharlie}} & rebmix & \textcolor{black}{22.4636 // 0.7553} & \textcolor{black}{4.7502 // 0.0678} & \textcolor{black}{17.5784 // 0.6853} & \textcolor{black}{0.26 // 0.0054} & \textcolor{black}{4.165 // 0.158} & \textcolor{black}{17.735 // 0.725} & \textcolor{black}{94 // 98}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{35.6085 // 13.8411} & \textcolor{black}{12.8826 // 3.6502} & \textcolor{black}{22.3428 // 10.0718} & \textcolor{black}{0.29 // 0.46} & \textcolor{black}{6.212 // 5.661} & \textcolor{black}{26.812 // 23.753} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{33.8007 // 12.5545} & \textcolor{black}{11.7236 // 3.1419} & \textcolor{black}{21.7292 // 9.2934} & \textcolor{black}{0.28 // 0.47} & \textcolor{black}{6.348 // 5.654} & \textcolor{black}{26.546 // 23.141} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{bgmm}} & rebmix & \textcolor{black}{35.3167 // 13.106} & \textcolor{black}{12.2374 // 3.3747} & \textcolor{black}{22.6615 // 9.6213} & \textcolor{black}{0.37 // 0.42} & \textcolor{black}{6.007 // 5.273} & \textcolor{black}{26.287 // 23.02} & \textcolor{black}{96 // 100}\\
\cmidrule{1-9}
 & hc & \textcolor{black}{23.4472 // 16.4192} & \textcolor{black}{6.2451 // 4.9191} & \textcolor{black}{17.0777 // 11.3124} & \textcolor{red}{0.35 // 0.51} & \textcolor{black}{5.469 // 6.279} & \textcolor{black}{23.503 // 22.821} & \textcolor{black}{99 // 100}\\

 & kmeans & \textcolor{black}{21.0058 // 14.6852} & \textcolor{black}{5.9951 // 4.1684} & \textcolor{black}{14.9329 // 10.4293} & \textcolor{black}{0.38 // 0.42} & \textcolor{black}{5.604 // 6.592} & \textcolor{black}{24.628 // 24.706} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{EMCluster}} & rebmix & \textcolor{black}{23.0408 // 19.7372} & \textcolor{black}{6.4923 // 7} & \textcolor{black}{16.3824 // 12.5099} & \textcolor{black}{0.36 // 0.35} & \textcolor{black}{5.272 // 5.454} & \textcolor{black}{23.419 // 23.9} & \textcolor{red}{93 // 98}\\
\midrule
\cmidrule{1-9}
 & hc & \textcolor{red}{36.5924 // 33.4077} & \textcolor{red}{16.108 // 14.4007} & \textcolor{red}{20.3809 // 18.8638} & \textcolor{black}{0.3 // 0.44} & \textcolor{red}{12.706 // 13.085} & \textcolor{red}{25.393 // 29.363} & \textcolor{green}{100 // 100}\\

 & kmeans & \textcolor{black}{34.7935 // 30.1935} & \textcolor{black}{15.4329 // 13.78} & \textcolor{black}{19.2529 // 16.2816} & \textcolor{black}{0.4 // 0.41} & \textcolor{black}{12.766 // 12.756} & \textcolor{black}{24.988 // 25.151} & \textcolor{green}{100 // 100}\\

\multirow{-3}{*}{\raggedright\arraybackslash \textbf{HDclassif}} & rebmix & \textcolor{black}{38.9707 // 24.0327} & \textcolor{black}{16.134 // 12.9266} & \textcolor{black}{22.6961 // 11.0138} & \textcolor{black}{0.25 // 0.21} & \textcolor{black}{12.811 // 12.275} & \textcolor{black}{25.79 // 15.996} & \textcolor{black}{95 // 98}\\
\bottomrule
\end{tabu}}
\end{table}

\newpage

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{figs/HD/HD-impact-nobservations} 

}

\caption{Overview of scenarios HD1 a) and b) and HD8 a) and b) in Table \ref{tab:parameter-configuration-HD} comparing the performance of the algorithms in the most complex scenario (highly overlapping and unbalanced clusters, with a full covariance structure). The left-hand column shows box plots of the estimated parameters from simulations with $n=200$ observations on the left and $n=2000$ observations on the right.}\label{fig:HD-impact-num-observations}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./figs/HD/heatmap_global_HD} 

}

\caption{Correlation heatmaps of the estimated parameters in the high dimensional (HD) setting extended to the three initialisation methods benchmarked (respectively hc, \textit{k}-means and rebmix) in the most discriminating scenario HD8a), using the same process described in Figure \ref{fig:dichotomy-package-conclusion}.}\label{fig:heatmap-all-correlation-plots-HD}
\end{figure}

\newpage

\hypertarget{additional-files}{%
\section{Additional files}\label{additional-files}}

\begin{itemize}
\item
  Additional files related to the univariate setting

  \begin{itemize}
  \tightlist
  \item
    \textbf{S1.} Bootstrap distributions of the estimated parameters for each
    scenario described in \ref{tab:parameter-configuration-univariate}.
  \item
    \textbf{S2.} Mean, standard deviation, bias and MSE for each individually
    estimated parameter in scenarios listed in \ref{tab:parameter-configuration-univariate}.
  \item
    \textbf{S3.} Distribution of the running times taken for the EM estimation of the parameters of the GMM, across all nine scenarios described in \ref{tab:parameter-configuration-univariate}, for each benchmarked package. We selected the \emph{k}-means algorithm to initialise the EM algorithm, as being the least variable for a given package and scenario.
  \item
    \textbf{S4.} Distribution of the time computations taken by the six initialisation methods listed in Table \ref{tab:general-parameter-description-pdf}.
  \end{itemize}
\item
  Additional files related to the outliers setting:

  \begin{itemize}
  \tightlist
  \item
    \textbf{S5.} Bootstrap distributions of the estimated parameters used to
    generate Figure \ref{fig:outliers}. We additionally include the \pkg{otrimle} package, dedicated to these extreme distributions. Two scenarios were tested, introducing \(2 \%\) and \(4 \%\) of outliers drawn from an improper uniform distribution.
  \item
    \textbf{S6.} Mean, standard deviation, bias and MSE for each individually
    estimated parameter in both scenarios visualised on Figure
    \ref{fig:outliers}, for each combination of package and
    initialisation method.
  \end{itemize}
\item
  Additional files related to the bivariate benchmark:

  \begin{itemize}
  \item
    \textbf{S7.} Bootstrap distributions of the estimated parameters for each
    scenario described in \ref{tab:parameter-configuration-bivariate}.
  \item
    \textbf{S8.} Mean, standard deviation, bias and MSE for each individually
    estimated parameter in scenarios listed in \ref{tab:parameter-configuration-bivariate}.
  \item
    \textbf{S9.} Distribution of the running times taken for the EM estimation of the parameters of the GMM, across all twenty scenarios described in \ref{tab:parameter-configuration-bivariate}, for each benchmarked package. We selected the \emph{k}-means algorithm to initialise the EM algorithm, as being the least variable for a given package and scenario.
  \end{itemize}
\item
  Additional files related to the high-dimensional benchmark:

  \begin{itemize}
  \item
    \textbf{S10.} Bootstrap distributions of the estimated parameters for each
    scenario described in \ref{tab:parameter-configuration-HD}.
  \item
    \textbf{S11.} Mean, standard deviation, bias and MSE for each individually
    estimated parameter in scenarios listed in \ref{tab:parameter-configuration-HD}.
  \item
    \textbf{S12.} Distribution of the running times taken for the EM estimation of the parameters of the GMM, across all twenty scenarios described in \ref{tab:parameter-configuration-HD}, for each benchmarked package. We selected the \emph{k}-means algorithm to initialise the EM algorithm, as being the least variable for a given package and scenario.
  \end{itemize}
\end{itemize}

\hypertarget{references}{%
\section{References}\label{references}}

\bibliography{chassagnol-becht-nuel-benchmark-of-Gaussian-mixtures.bib}

\address{%
Bastien Chassagnol\\
Laboratoire de Probabilités, Statistiques et Modélisation (LPSM), UMR CNRS 8001\\%
4 Place Jussieu Sorbonne Université\\ 75005, Paris, France\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-8955-2391}{0000-0002-8955-2391}}\\%
\href{mailto:bastien_chassagnol@laposte.net}{\nolinkurl{bastien\_chassagnol@laposte.net}}%
}

\address{%
Antoine Bichat\\
Les Laboratoires Servier\\%
50 Rue Carnot\\ 92150, Suresnes, France\\
%
\url{https://rdrr.io/github/abichat/abutils/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0001-6599-7081}{0000-0001-6599-7081}}\\%
\href{mailto:antoine.bichat@servier.com}{\nolinkurl{antoine.bichat@servier.com}}%
}

\address{%
Cheïma Boudjeniba\\
Systems Biology Group, Dept. of Computational Biology, Institut Pasteur\\%
25 Rue du Dr Roux\\ 75015 Paris\\
%
%
%
\href{mailto:cheima.boudjeniba@servier.com}{\nolinkurl{cheima.boudjeniba@servier.com}}%
}

\address{%
Pierre-Henri Wuillemin\\
Laboratoire d'Informatique de Paris 6 (LIP6), UMR 7606\\%
4 Place Jussieu Sorbonne Université\\ 75005, Paris, France\\
%
\url{http://www-desir.lip6.fr/~phw/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0003-3691-4886}{0000-0003-3691-4886}}\\%
\href{mailto:pierre-henri.wuillemin@lip6.fr}{\nolinkurl{pierre-henri.wuillemin@lip6.fr}}%
}

\address{%
Mickaël Guedj\\
Les Laboratoires Servier\\%
50 Rue Carnot\\ 92150, Suresnes, France\\
%
\url{https://michaelguedj.github.io/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0001-6694-0554}{0000-0001-6694-0554}}\\%
\href{mailto:mickael.guedj@gmail.com}{\nolinkurl{mickael.guedj@gmail.com}}%
}

\address{%
Gregory Nuel\\
Laboratoire de Probabilités, Statistiques et Modélisation (LPSM), UMR CNRS 8001\\%
4 Place Jussieu Sorbonne Université\\ 75005, Paris, France\\
%
\url{http://nuel.perso.math.cnrs.fr/}\\%
\textit{ORCiD: \href{https://orcid.org/0000-0001-9910-2354}{0000-0001-9910-2354}}\\%
\href{mailto:Gregory.Nuel@math.cnrs.fr}{\nolinkurl{Gregory.Nuel@math.cnrs.fr}}%
}

\address{%
Etienne Becht\\
Les Laboratoires Servier\\%
50 Rue Carnot\\ 92150, Suresnes, France\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0003-1859-9202}{0000-0003-1859-9202}}\\%
\href{mailto:etienne.becht@servier.com}{\nolinkurl{etienne.becht@servier.com}}%
}
